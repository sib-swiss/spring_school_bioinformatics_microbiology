{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Spring School Bioinformatics and computational approaches in Microbiology Overview This Spring School is co-organized by the NCCR AntiResist , NCCR Microbiomes and the SIB PhD Training Network . The goal of this joint School is to provide PhD students and postdocs with theoretical and mostly hands-on knowledge on selected topics about microbiome and antibiotics resistance. In particular, the participants will be split into four groups to work on mini-projects, which will address the following topics: Project 1: Modeling antibiotic resistance evolution. Project 2: Computational image analysis: transforming images into insights. Project 3: Profiling and modeling the colorectal cancer microbiome. Project 4: Mathematical modeling of bacterial metabolism. Audience This course is addressed to life scientists (mostly PhD students and Postdocs) dealing with microbiology topics such as microbiomes and resistance to antibiotics. Learning outcomes At the end of the course, the participants are expected to: have a good understanding of the most common methods used during the event; repeat the same type of analysis achieved during the mini-project; present the mini-project and more globally disseminate their experience to the members of their group. Hotel Hotel and Conference Center Sempachersee Guido A. Z\u00e4ch Strasse 2 6207 Nottwil LU T +41 41 939 23 23 info -at- hotelsempachersee.ch How to get there","title":"Home"},{"location":"#spring-school-bioinformatics-and-computational-approaches-in-microbiology","text":"","title":"Spring School Bioinformatics and computational approaches in Microbiology"},{"location":"#overview","text":"This Spring School is co-organized by the NCCR AntiResist , NCCR Microbiomes and the SIB PhD Training Network . The goal of this joint School is to provide PhD students and postdocs with theoretical and mostly hands-on knowledge on selected topics about microbiome and antibiotics resistance. In particular, the participants will be split into four groups to work on mini-projects, which will address the following topics: Project 1: Modeling antibiotic resistance evolution. Project 2: Computational image analysis: transforming images into insights. Project 3: Profiling and modeling the colorectal cancer microbiome. Project 4: Mathematical modeling of bacterial metabolism.","title":"Overview"},{"location":"#audience","text":"This course is addressed to life scientists (mostly PhD students and Postdocs) dealing with microbiology topics such as microbiomes and resistance to antibiotics.","title":"Audience"},{"location":"#learning-outcomes","text":"At the end of the course, the participants are expected to: have a good understanding of the most common methods used during the event; repeat the same type of analysis achieved during the mini-project; present the mini-project and more globally disseminate their experience to the members of their group.","title":"Learning outcomes"},{"location":"#hotel","text":"Hotel and Conference Center Sempachersee Guido A. Z\u00e4ch Strasse 2 6207 Nottwil LU T +41 41 939 23 23 info -at- hotelsempachersee.ch How to get there","title":"Hotel"},{"location":"course_schedule/","text":"Schedule ONGOING WORK Sunday 8 May (~18h00) start end topic ~18:00 Arrival & check-in 19:00 Dinner Monday 9 May start end topic 09:00 09:30 Introduction by the 3 organizing partners about the event 09:30 10:30 Lecture Topic 1 - Modeling antibiotic resistance evolution 10:30 11:00 Coffee Break 11:00 12:00 Lecture Topic 2 - Computational image analysis: transforming images into insights 12:15 13:45 Lunch 13:45 14:45 Lecture Topic 3 - Profiling and modeling the colorectal cancer microbiome 14:45 15:45 Lecture Topic 4 - Mathematical modeling of bacterial metabolism: diauxic growth of a population and syntrophic interactions in a consortium 15:45 16:15 Coffee break 16:15 17:00 Preparation for the mini-projects Tuesday 10 May start end topic 09:00 10:30 Mini-projects (work in groups) 10:30 11:00 Coffee Break 11:00 13:00 Mini-projects (work in groups) 13:00 14:00 Lunch 14:00 15:30 Mini-projects (work in groups) 15:30 16:00 Coffee Break 16:00 17:00 Mini-projects (work in groups) 19:00 Dinner Wednesday 11 May start end topic 09:00 10:30 Mini-projects (work in groups) 10:30 11:00 Coffee Break 11:00 12:30 Mini-projects (work in groups) 12:30 13:45 Lunch 14:00 17:30 Recreational activity 19:00 Dinner Thursday 12 May start end topic 09:00 10:30 Mini-projects (work in groups) 10:30 11:00 Coffee Break 11:00 12:45 Keynote lecture by Dr. Joao Xavier from the Memorial Sloan Kettering Institute 12:45 14:00 Lunch 14:00 17:00 Mini-projects : prepare your presentations 19:00 Dinner Friday 13 May start end topic 09:00 09:45 Presentation of Mini-projects topic 1 09:45 10:30 Presentation of Mini-projects topic 2 10:30 11:00 Coffee Break + check out 11:00 11:45 Presentation of Mini-projects topic 3 11:45 12:30 Presentation of Mini-projects topic 4 12:30 13:30 Lunch 13:30 ~14:00 Conclusion & Wrap up","title":"Course schedule"},{"location":"course_schedule/#schedule","text":"ONGOING WORK Sunday 8 May (~18h00) start end topic ~18:00 Arrival & check-in 19:00 Dinner Monday 9 May start end topic 09:00 09:30 Introduction by the 3 organizing partners about the event 09:30 10:30 Lecture Topic 1 - Modeling antibiotic resistance evolution 10:30 11:00 Coffee Break 11:00 12:00 Lecture Topic 2 - Computational image analysis: transforming images into insights 12:15 13:45 Lunch 13:45 14:45 Lecture Topic 3 - Profiling and modeling the colorectal cancer microbiome 14:45 15:45 Lecture Topic 4 - Mathematical modeling of bacterial metabolism: diauxic growth of a population and syntrophic interactions in a consortium 15:45 16:15 Coffee break 16:15 17:00 Preparation for the mini-projects Tuesday 10 May start end topic 09:00 10:30 Mini-projects (work in groups) 10:30 11:00 Coffee Break 11:00 13:00 Mini-projects (work in groups) 13:00 14:00 Lunch 14:00 15:30 Mini-projects (work in groups) 15:30 16:00 Coffee Break 16:00 17:00 Mini-projects (work in groups) 19:00 Dinner Wednesday 11 May start end topic 09:00 10:30 Mini-projects (work in groups) 10:30 11:00 Coffee Break 11:00 12:30 Mini-projects (work in groups) 12:30 13:45 Lunch 14:00 17:30 Recreational activity 19:00 Dinner Thursday 12 May start end topic 09:00 10:30 Mini-projects (work in groups) 10:30 11:00 Coffee Break 11:00 12:45 Keynote lecture by Dr. Joao Xavier from the Memorial Sloan Kettering Institute 12:45 14:00 Lunch 14:00 17:00 Mini-projects : prepare your presentations 19:00 Dinner Friday 13 May start end topic 09:00 09:45 Presentation of Mini-projects topic 1 09:45 10:30 Presentation of Mini-projects topic 2 10:30 11:00 Coffee Break + check out 11:00 11:45 Presentation of Mini-projects topic 3 11:45 12:30 Presentation of Mini-projects topic 4 12:30 13:30 Lunch 13:30 ~14:00 Conclusion & Wrap up","title":"Schedule"},{"location":"precourse/","text":"See directly in your project","title":"Precourse preparations"},{"location":"project1/","text":"Project 1: Modeling antibiotic resistance evolution. Trainers: Fernanda Pinheiro , Group Leader in the Computational Biology Research Centre, Human Technopole , Milan; Leon Seeger , PhD student enrolled in the lab of Fernanda Pinheiro and Michael L\u00e4ssig - University of Cologne. Description: Bacterial growth under antibiotic challenge results from a complex interplay between antibiotic chemistry, nutrient conditions, bacterial physiology, and evolution. While antibiotics disrupt essential cellular processes of the wild-type susceptible background, they are less harmful to resistant bacteria. Resistant bacteria have multiple ways to counteract antibiotic action, including upregulation of its targets, alteration of membrane permeability, overexpression of efflux pumps, and enzymatic modification of antibiotics. How do different resistance mechanisms shape dosage-dependent bacterial growth? And what determines prevalent mechanisms of resistance evolution as a function of environmental conditions? In this course, we will build fitness models to characterize the dosage-dependent growth of antibiotic-resistant mutants. We will integrate the biophysics of drug action and antibiotic resistance mechanisms into coarse-grained models of cell metabolism to establish a mechanistic description of bacterial growth under challenge. We will then fit these models to existing data of dosage-response curves and map the conditions for the onset of different resistance mechanisms from the inferred biophysical parameters. Can we predict mechanisms of resistance evolution as a function of conditions? The lectures will expose the participants to theoretical developments (systems biology models of bacterial growth, metabolic fitness landscapes, evolutionary models), and numerical work will be done in MatLab or Python.","title":"Description"},{"location":"project2/","text":"Project 2: Computational image analysis: transforming images into insights. Trainers: Simon van Vliet , Project Leader, Biozentrum, University of Basel & NCCR AntiResist member; Alma Dal Co , Assistant Professor, Department of Computational Biology, University of Lausanne and NCCR Microbiomes member. Description: In natural environments, bacteria are often exposed to environments that change in time or space. Moreover, even in well-mixed clonal populations, bacteria can show dramatic variation in their growth, gene expression, and physiology. A main goal in current research is to study how these effects affect the growth and activity of microbial populations. To address these questions, many research groups are using in-vitro systems, such as agar pads and microfluidic devices, together with time-lapse microscopy to follow cell growth and gene expression in microbial populations at the single cell level. Although these experiments yield rich datasets, extracting biological insight from them can be challenging. In this course, participants will learn how to obtain biological insight from single-cell resolution, time-lapse microscopy data. We will cover all aspects of the image and data analysis workflow: from preprocessing, to segmenting and tracking cells, to feature extraction and statistical analysis. The participant will be introduced to a number of commonly used and state-of-the art tools, including imageJ, Ilastik, SciPy, and Deep Learning based workflows. The projects will focus on gaining hands-on experience: participants will work through a full image analysis workflow. In addition, we will give short lectures to introduce participants to alternative tools and other useful resources. At the end of the course, participants should be able to successfully analyze their own data using a variety of existing analysis tools. Requirements: Specific knowledge requirements for this project (besides basic knowledge of UNIX and R): Basic programming knowledge in Python / Matlab / R or similar Basic knowledge of statistical methods Some familiarity with Python is highly beneficial but not necessary. Participants without prior experience with Python are strongly recommended to follow a short online tutorial before the start of the course (instruction will be provided 3-4 weeks before start of course).","title":"Description"},{"location":"project3/","text":"Project 3: Profiling and modeling the colorectal cancer microbiome. Trainers: Alessio Milanese , Sunagawa Lab - Microbiome Research, ETH Zurich and SIB . NCCR Microbiomes member; Lukas Malfertheiner , Von Mering Lab - System Biology, University of Zurich and SIB . NCCR Microbiomes member; Description: Explore different cross-sectional metagenomic studies related to colorectal cancer (CRC), starting from taxonomic profiling to modeling by machine learning. Students will learn to work with metagenomic sequencing data and how to create taxonomic profiles from metagenomic samples using mOTUs (Milanese et al. 2019) . They will use R to explore properties typical for microbiome data (e.g., sparsity and sample size differences), use dimensionality reduction techniques and quantify within- and between-sample diversity. To explore associations between microbes and disease status, students will learn to use the SIAMCAT tool box (Wirbel et al. 2021) and develop a model for CRC classification based on taxonomic profiles. The project requires a basic understanding of the Unix command line and basic programming knowledge in R.","title":"Description"},{"location":"project4/","text":"Project 4: Mathematical modeling of bacterial metabolism: diauxic growth of a population and syntrophic interactions in a consortium Trainers: Hidde de Jong , Senior Research Scientist, INRIA Grenoble . Maaike Sangster , INRIA Grenoble . Description: We will step-by-step build simple ODE models of a metabolic network integrating regulation on both the metabolic and gene expression level, and investigate the effect of these layers of regulation on the network dynamics. The course will be structured around two examples: (i) carbon catabolite repression and diauxic growth in bacteria and (ii) synthrophic interactions in a consortium of two bacterial strains. All simulations will be carried out by means of Python.","title":"Description"},{"location":"project1/software/","text":"Numerical analyses and fits to dosage-response curves will be carried on in Python (or MatLab if necessary). We will use Jupyter notebook and the following packages: pandas, numpy, matplotlib, seaborn, scipy, lmfit, mpl_toolkits.","title":"Software"},{"location":"project1/timeline/","text":"Project schedule: Monday: preparation for the projects to check Python software requirements we will fit Hill models to dosage-response curves of E. coli under streptomycin challenge. Tuesday: crash course on bacterial growth crash course on bacterial growth data we will fit Hill functions to dosage-response curves of E. coli mutants selected under streptomycin challenge and study correlations in the data. How far can we go without a mechanistic model of dosage-response? mechanistic models of growth response for ribosome-targeting drugs I we will build a mechanistic model of growth inhibition by ribosome-targeting drugs and study its properties Wednesday: mechanistic models of growth response for ribosome-targeting drugs II we will fit the mechanistic model of growth inhibition by ribosome-targeting drugs to the dosage-response curves of E. coli mutants selected under streptomycin challenge and study correlations in the data we will build fitness models for different resistance mechanisms and we will predict the dosage-response curves of families of mutants with common mechanisms Thursday: phase diagram for resistance mechanisms we will compare the growth of mutants with different resistance mechanisms, and we will study the growth conditions favoring different mechanisms preparation for the presentation References: Scott, M., Gunderson, C. W., Mateescu, E. M., Zhang, Z., & Hwa, T. (2010). Interdependence of cell growth and gene expression: origins and consequences. Science, 330(6007), 1099-1102. Greulich, P., Scott, M., Evans, M. R., & Allen, R. J. (2015). Growth\u2010dependent bacterial susceptibility to ribosome\u2010targeting antibiotics. Molecular systems biology, 11(3), 796. Pinheiro, F., Warsi, O., Andersson, D. I., & L\u00e4ssig, M. (2021). Metabolic fitness landscapes predict the evolution of antibiotic resistance. Nature Ecology & Evolution, 5(5), 677-687.","title":"Overview"},{"location":"project1/timeline/#project-schedule","text":"","title":"Project schedule:"},{"location":"project1/timeline/#monday","text":"preparation for the projects to check Python software requirements we will fit Hill models to dosage-response curves of E. coli under streptomycin challenge.","title":"Monday:"},{"location":"project1/timeline/#tuesday","text":"crash course on bacterial growth crash course on bacterial growth data we will fit Hill functions to dosage-response curves of E. coli mutants selected under streptomycin challenge and study correlations in the data. How far can we go without a mechanistic model of dosage-response? mechanistic models of growth response for ribosome-targeting drugs I we will build a mechanistic model of growth inhibition by ribosome-targeting drugs and study its properties","title":"Tuesday:"},{"location":"project1/timeline/#wednesday","text":"mechanistic models of growth response for ribosome-targeting drugs II we will fit the mechanistic model of growth inhibition by ribosome-targeting drugs to the dosage-response curves of E. coli mutants selected under streptomycin challenge and study correlations in the data we will build fitness models for different resistance mechanisms and we will predict the dosage-response curves of families of mutants with common mechanisms","title":"Wednesday:"},{"location":"project1/timeline/#thursday","text":"phase diagram for resistance mechanisms we will compare the growth of mutants with different resistance mechanisms, and we will study the growth conditions favoring different mechanisms preparation for the presentation","title":"Thursday:"},{"location":"project1/timeline/#references","text":"Scott, M., Gunderson, C. W., Mateescu, E. M., Zhang, Z., & Hwa, T. (2010). Interdependence of cell growth and gene expression: origins and consequences. Science, 330(6007), 1099-1102. Greulich, P., Scott, M., Evans, M. R., & Allen, R. J. (2015). Growth\u2010dependent bacterial susceptibility to ribosome\u2010targeting antibiotics. Molecular systems biology, 11(3), 796. Pinheiro, F., Warsi, O., Andersson, D. I., & L\u00e4ssig, M. (2021). Metabolic fitness landscapes predict the evolution of antibiotic resistance. Nature Ecology & Evolution, 5(5), 677-687.","title":"References:"},{"location":"project2/Home/","text":"Computational image analysis: transforming images into insights This wiki contains all information for the \u201cComputational image analysis: transforming images into insights\u201d part of the \u201cSpring School Bioinformatics and computational approaches in Microbiology\u201d organized by the NCCR AntiResist, NCCR Microbiomes, and the SIB PhD training network. Before the start of the course please follow all instructions on this page","title":"Computational image analysis: transforming images into insights"},{"location":"project2/Home/#computational-image-analysis-transforming-images-into-insights","text":"This wiki contains all information for the \u201cComputational image analysis: transforming images into insights\u201d part of the \u201cSpring School Bioinformatics and computational approaches in Microbiology\u201d organized by the NCCR AntiResist, NCCR Microbiomes, and the SIB PhD training network. Before the start of the course please follow all instructions on this page","title":"Computational image analysis: transforming images into insights"},{"location":"project2/Project-2A/","text":"Project 2A: Semi-automated processing using Fiji, Ilastik, and Python Here we will use a semi-automated pipeline to segment time-lapse data and quantify cell properties over time. We will not track cells. We will use a set of tools that are very frequently used in image analysis: Fiji, aka: ImageJ on steroids , which is a generalist user-friendly, GUI based, image analysis program offering many tools and plugins for manual and semi-automated analysis. Ilastik , which is a user-friendly, GUI based tool, offering several supervised machine learning pipelines. python , a generalist programming language which has become the language of choice for the image analysis community General note: this guide has been written assuming you use a Mac or Linux Command Line. For windows Powershell see here , for command prompt see here . Note on data The dataset we will work with here consists of a time-lapse data of a synthetic microbial cross-feeding community consisting of two auxotrophic strains of E. coli that can only grow by exchanging Amino-Acids with each other. The two cell-types are fluorescently labeled with two different colors and grow together in a mono-layer within a 2D microfluidic growth chamber (a so-called family-machine). Create project folders We first create a folder on your private computer for this course. Here we will assume it is ~/I2ICourse/ but you can use any folder (just change path variables accordingly). Open the command line and navigate to your home folder, then create a new folder called I2ICourse for the course: cd ~ mkdir I2ICourse Next create a folder for Project-2A ~/I2ICourse/Project2A/ : cd ~/I2ICourse/ mkdir Project2A And also make a folder for the processed data (output from all steps below): cd Project2A mkdir ProcessedData This should have created the following folder: ~/I2ICourse/Project2A/ProcessedData/ : Note: two useful terminal commands are: ls : show current folder content (on Windows use dir ) pwd : show path of current folder Download Data Here we will download the data via the command line. Navigate to the data folder: cd ~/I2ICourse/Project2A/ Download the data set and unzip it using: curl -o RawData.zip https://drive.switch.ch/index.php/s/VsWWiuaIctITWQl/download unzip RawData.zip You should now have a folder ~/I2ICourse/Project2A/RawData containing 3 tif files (hint: use ls to check!) We can delete the zip-file using: rm RawData.zip Alternative Download via browser If you have trouble downloading via command line, you can also use your browser: Open this link Click Download Unzip the compressed file inside your data folder. The data should now be located in ~/I2ICourse/Project2A/RawData Please rename the folder if needed Data preprocessing with Fiji We will first prepare the data in Fiji Merging & splitting color channels Before pre-processing we need to merge the color channels. Open the individual color images ( pos0-[c].tif , where c={r,g,p}) Image -> Color -> Merge Channels Make sure Create composite is selected Under C1 (red) select the [r] image Under C2 (green) select the [g] image Under C4 (grey) select the [p] image Save on disk as pos0-merged.tif use your processed data folder, e.g.: ~/I2ICourse/Project2A/ProcessedData/ Crop Image Make a rectangular selection around the area of interest Make sure that you make the area big enough to accommodate any remaining jitter. Go to Image -> Crop Save image as pos0-preproc-merged.tif use your processed data folder, e.g.: ~/I2ICourse/Project2A/ProcessedData/ Export data for segmentation First we split the image into separate color channels Image -> Color -> Split Channels Save the separate image channels under the name pos0-preproc-[c].tif In addition we need a combined image with the red and green channels. Use Merge Channels to combine the red and green channels (do not include phase!). Save on disk as pos0-preproc-rg.tif use your processed data folder, e.g.: ~/I2ICourse/Project2A/ProcessedData/ Aside: Other preprocessing steps During preprocessing you would often also do some other steps, for example: Registration: i.e. aligning images between frames to compensate for stage and sample drift Deconvolution: i.e. correcting for diffraction artifacts to make segmentation easier. Unfortunately we do not have time to go into this now, but please ask us during the breaks for more information! Segment with Ilastik We will give a brief live-demo of how to use Ilastik, please let us know when you are at this step, so that we can get the entire group together. You can find detailed instructions (and a movie) here . Most important steps (see also the pdf in the Project2A repository folder): Open Ilastik Select Pixel Classification workflow Save it in processed data folder as \u2018proj2A-ilastik\u2019 Go to Input Data Load the data of the red-green channel. Important: when adding the input data, you might have to change the axis order: double click on the axis order (e.g. zcyx ) and change to tcyx . Go to Feature Selection Select all features Go to Training Make two labels: Cells and BG Add sparse training points to indicate which pixels belong to cells and which to background Important: save your project frequently! Ilastik can crash! Use Live Update to visualize training Evaluate the result by checking Segmentation (Layer 1) You can use the Uncertainty to see where more training points need to be added Focus attention on pixels in between cells Once the segmentation looks good, check a few other frames and update training as needed, until it looks good for all frames. Go to Prediction Export See details here In Export Settings select Probabilities in the source field. Open Choose Export Image Setting and select hdf5 format. Then click Export All Select as output folder ~/I2ICourse/Project2A/ProcessedData/ The output should be stored under the name [input_file_name]_Probabilities.h5 , i.e. pos0_preproc-rg_Probabilities.h5 . Post-process with Python - Personal Computer Follow these instruction if you run the python code locally (see below for instructions on how to run it on the cloud computer) Launch Jupyter Labs Navigate to the project folder, activate the conda environment, and launch Jupyter Labs: cd ~/I2ICourse/ conda activate i2i_env jupyter lab In Jupyter Labs, navigate to spring_school_bioinformatics_microbiology/projects/project2/Project2A/ Then open the 0_post_process_segementation.ipynb notebook Now run the notebook, see here for instructions on Jupyter Labs and follow the instructions in the notebook. Post-process with Python - Cloud Computer Follow these instructions if you are using the cloud computer to run Python. Export Data on local computer For the next steps we will switch to the cloud computers, but before that we need to transfer the data-file we just created: On your local computer, compress the ~/I2ICourse/Project2A/ProcessedData folder into a zip-file Upload this zip file to a cloud drive Create a public share link and copy the address Setup project folders on cloud computer Login to the cloud computer (your tutor will provide info in how to do this) Launch the command window and create a Project2A folder within the ~\\workdir\\ folder: cd ~/workdir/ mkdir Project2A Important Note: on the cloud computer we need to store all data in the ~/workdir/ folder or sub-folders of this, to make sure that files remain available after restarting the instance. Also add a ProcessedData subfolder to the Project2A folder: cd Project2A mkdir ProcessedData Download data on cloud computer Now we download the data file we just uploaded to the cloud: cd ~/workdir/Project2A/ProcessedData curl -o data.zip public_link_to_your_zip_file unzip -j data.zip Check that the data transferred successfully (use ls ) If so, then you can remove the zip file use rm data.zip Download project code on cloud computer Navigate to the workdir folder and use the git clone command to download the course code: cd ~/workdir/ git clone https://github.com/sib-swiss/spring_school_bioinformatics_microbiology.git This will create the folder ~/workdir/spring_school_bioinformatics_microbiology/ which contains all the Jupyter notebooks as well as the other course files Launch Jupyter Labs Next navigate to the project folder, activate the conda environment, and launch Jupyter Labs: cd ~/workdir/ conda activate project2 jupyter lab In Jupyter Labs, navigate to spring_school_bioinformatics_microbiology/projects/project2/Project2A/ Then open the 0_post_process_segementation.ipynb notebook Now run the notebook, see here for instructions on Jupyter Labs and follow the instructions in the notebook.","title":"Project 2A"},{"location":"project2/Project-2A/#project-2a-semi-automated-processing-using-fiji-ilastik-and-python","text":"Here we will use a semi-automated pipeline to segment time-lapse data and quantify cell properties over time. We will not track cells. We will use a set of tools that are very frequently used in image analysis: Fiji, aka: ImageJ on steroids , which is a generalist user-friendly, GUI based, image analysis program offering many tools and plugins for manual and semi-automated analysis. Ilastik , which is a user-friendly, GUI based tool, offering several supervised machine learning pipelines. python , a generalist programming language which has become the language of choice for the image analysis community General note: this guide has been written assuming you use a Mac or Linux Command Line. For windows Powershell see here , for command prompt see here .","title":"Project 2A: Semi-automated processing using Fiji, Ilastik, and Python"},{"location":"project2/Project-2A/#note-on-data","text":"The dataset we will work with here consists of a time-lapse data of a synthetic microbial cross-feeding community consisting of two auxotrophic strains of E. coli that can only grow by exchanging Amino-Acids with each other. The two cell-types are fluorescently labeled with two different colors and grow together in a mono-layer within a 2D microfluidic growth chamber (a so-called family-machine).","title":"Note on data"},{"location":"project2/Project-2A/#create-project-folders","text":"We first create a folder on your private computer for this course. Here we will assume it is ~/I2ICourse/ but you can use any folder (just change path variables accordingly). Open the command line and navigate to your home folder, then create a new folder called I2ICourse for the course: cd ~ mkdir I2ICourse Next create a folder for Project-2A ~/I2ICourse/Project2A/ : cd ~/I2ICourse/ mkdir Project2A And also make a folder for the processed data (output from all steps below): cd Project2A mkdir ProcessedData This should have created the following folder: ~/I2ICourse/Project2A/ProcessedData/ : Note: two useful terminal commands are: ls : show current folder content (on Windows use dir ) pwd : show path of current folder","title":"Create project folders"},{"location":"project2/Project-2A/#download-data","text":"Here we will download the data via the command line. Navigate to the data folder: cd ~/I2ICourse/Project2A/ Download the data set and unzip it using: curl -o RawData.zip https://drive.switch.ch/index.php/s/VsWWiuaIctITWQl/download unzip RawData.zip You should now have a folder ~/I2ICourse/Project2A/RawData containing 3 tif files (hint: use ls to check!) We can delete the zip-file using: rm RawData.zip","title":"Download Data"},{"location":"project2/Project-2A/#alternative-download-via-browser","text":"If you have trouble downloading via command line, you can also use your browser: Open this link Click Download Unzip the compressed file inside your data folder. The data should now be located in ~/I2ICourse/Project2A/RawData Please rename the folder if needed","title":"Alternative Download via browser"},{"location":"project2/Project-2A/#data-preprocessing-with-fiji","text":"We will first prepare the data in Fiji","title":"Data preprocessing with Fiji"},{"location":"project2/Project-2A/#merging-splitting-color-channels","text":"Before pre-processing we need to merge the color channels. Open the individual color images ( pos0-[c].tif , where c={r,g,p}) Image -> Color -> Merge Channels Make sure Create composite is selected Under C1 (red) select the [r] image Under C2 (green) select the [g] image Under C4 (grey) select the [p] image Save on disk as pos0-merged.tif use your processed data folder, e.g.: ~/I2ICourse/Project2A/ProcessedData/","title":"Merging &amp; splitting color channels"},{"location":"project2/Project-2A/#crop-image","text":"Make a rectangular selection around the area of interest Make sure that you make the area big enough to accommodate any remaining jitter. Go to Image -> Crop Save image as pos0-preproc-merged.tif use your processed data folder, e.g.: ~/I2ICourse/Project2A/ProcessedData/","title":"Crop Image"},{"location":"project2/Project-2A/#export-data-for-segmentation","text":"First we split the image into separate color channels Image -> Color -> Split Channels Save the separate image channels under the name pos0-preproc-[c].tif In addition we need a combined image with the red and green channels. Use Merge Channels to combine the red and green channels (do not include phase!). Save on disk as pos0-preproc-rg.tif use your processed data folder, e.g.: ~/I2ICourse/Project2A/ProcessedData/","title":"Export data for segmentation"},{"location":"project2/Project-2A/#aside-other-preprocessing-steps","text":"During preprocessing you would often also do some other steps, for example: Registration: i.e. aligning images between frames to compensate for stage and sample drift Deconvolution: i.e. correcting for diffraction artifacts to make segmentation easier. Unfortunately we do not have time to go into this now, but please ask us during the breaks for more information!","title":"Aside: Other preprocessing steps"},{"location":"project2/Project-2A/#segment-with-ilastik","text":"We will give a brief live-demo of how to use Ilastik, please let us know when you are at this step, so that we can get the entire group together. You can find detailed instructions (and a movie) here . Most important steps (see also the pdf in the Project2A repository folder): Open Ilastik Select Pixel Classification workflow Save it in processed data folder as \u2018proj2A-ilastik\u2019 Go to Input Data Load the data of the red-green channel. Important: when adding the input data, you might have to change the axis order: double click on the axis order (e.g. zcyx ) and change to tcyx . Go to Feature Selection Select all features Go to Training Make two labels: Cells and BG Add sparse training points to indicate which pixels belong to cells and which to background Important: save your project frequently! Ilastik can crash! Use Live Update to visualize training Evaluate the result by checking Segmentation (Layer 1) You can use the Uncertainty to see where more training points need to be added Focus attention on pixels in between cells Once the segmentation looks good, check a few other frames and update training as needed, until it looks good for all frames. Go to Prediction Export See details here In Export Settings select Probabilities in the source field. Open Choose Export Image Setting and select hdf5 format. Then click Export All Select as output folder ~/I2ICourse/Project2A/ProcessedData/ The output should be stored under the name [input_file_name]_Probabilities.h5 , i.e. pos0_preproc-rg_Probabilities.h5 .","title":"Segment with Ilastik"},{"location":"project2/Project-2A/#post-process-with-python-personal-computer","text":"Follow these instruction if you run the python code locally (see below for instructions on how to run it on the cloud computer)","title":"Post-process with Python - Personal Computer"},{"location":"project2/Project-2A/#launch-jupyter-labs","text":"Navigate to the project folder, activate the conda environment, and launch Jupyter Labs: cd ~/I2ICourse/ conda activate i2i_env jupyter lab In Jupyter Labs, navigate to spring_school_bioinformatics_microbiology/projects/project2/Project2A/ Then open the 0_post_process_segementation.ipynb notebook Now run the notebook, see here for instructions on Jupyter Labs and follow the instructions in the notebook.","title":"Launch Jupyter Labs"},{"location":"project2/Project-2A/#post-process-with-python-cloud-computer","text":"Follow these instructions if you are using the cloud computer to run Python.","title":"Post-process with Python - Cloud Computer"},{"location":"project2/Project-2A/#export-data-on-local-computer","text":"For the next steps we will switch to the cloud computers, but before that we need to transfer the data-file we just created: On your local computer, compress the ~/I2ICourse/Project2A/ProcessedData folder into a zip-file Upload this zip file to a cloud drive Create a public share link and copy the address","title":"Export Data on local computer"},{"location":"project2/Project-2A/#setup-project-folders-on-cloud-computer","text":"Login to the cloud computer (your tutor will provide info in how to do this) Launch the command window and create a Project2A folder within the ~\\workdir\\ folder: cd ~/workdir/ mkdir Project2A Important Note: on the cloud computer we need to store all data in the ~/workdir/ folder or sub-folders of this, to make sure that files remain available after restarting the instance. Also add a ProcessedData subfolder to the Project2A folder: cd Project2A mkdir ProcessedData","title":"Setup project folders on cloud computer"},{"location":"project2/Project-2A/#download-data-on-cloud-computer","text":"Now we download the data file we just uploaded to the cloud: cd ~/workdir/Project2A/ProcessedData curl -o data.zip public_link_to_your_zip_file unzip -j data.zip Check that the data transferred successfully (use ls ) If so, then you can remove the zip file use rm data.zip","title":"Download data on cloud computer"},{"location":"project2/Project-2A/#download-project-code-on-cloud-computer","text":"Navigate to the workdir folder and use the git clone command to download the course code: cd ~/workdir/ git clone https://github.com/sib-swiss/spring_school_bioinformatics_microbiology.git This will create the folder ~/workdir/spring_school_bioinformatics_microbiology/ which contains all the Jupyter notebooks as well as the other course files","title":"Download project code on cloud computer"},{"location":"project2/Project-2A/#launch-jupyter-labs_1","text":"Next navigate to the project folder, activate the conda environment, and launch Jupyter Labs: cd ~/workdir/ conda activate project2 jupyter lab In Jupyter Labs, navigate to spring_school_bioinformatics_microbiology/projects/project2/Project2A/ Then open the 0_post_process_segementation.ipynb notebook Now run the notebook, see here for instructions on Jupyter Labs and follow the instructions in the notebook.","title":"Launch Jupyter Labs"},{"location":"project2/Project-2B/","text":"Segmentation and Tracking 2D Data with Delta2.0 Delta 2.0 is a Deep Learning based workflow that can segment and track 1D (mother machine) and 2D (family machine and agar pads) data. It uses two consecutive U-Net networks to first segment and then track cells. The pipeline is described in this publication . Detailed instruction can be found here Note on data The dataset we will work with here consists of a time-lapse data of a micro-colony of E. coli cells growing on LB agar pads. We have two channels: phase contrast and GFP. The GFP signal comes from a transcriptional reporter for Colicin Ib, a bacteriocin that is regulated by SOS-stress response. Images were taken every 5min. More info on the data can be found here . Install Delta (Windows only) Note for windows users For Mac/Linux users Delta was installed when we created the i2i_env conda environment. However, Delta is not available from Conda for Windows. We have to install in manually, following these steps: Download Delta (via git) cd ~/I2ICourse/ git clone https://gitlab.com/dunloplab/delta.git Download Delta (manually) Download code here . Unzip file in ~/I2ICourse/ dir. Add Delta path to each Notebook Add the start of every notebook where we import delta, add the following lines at the top of the cell where we import delta: import sys sys . path . append ( \u201c path / to / delta \u201d ) Getting started - Local Computer On your computer, navigate to the I2ICourse folder and create a Project2B subfolder: cd ~/I2ICourse/ mkdir Project2B Then activate the conda environment, and launch Jupyter Labs: cd ~/I2ICourse/ conda activate i2i_env jupyter lab In Jupyter Labs, navigate to spring_school_bioinformatics_microbiology/projects/project2/Project2B/ Then follow the steps below. Getting started - Cloud Computer We will work on the cloud computers for the full project Important: on the cloud computer we need to store all data in the ~/workdir/ folder or sub-folders of this, to make sure that files remain available after restarting the instance. On the cloud computer, navigate to the workdir folder and create a Project2B subfolder: cd ~/workdir/ mkdir Project2B Then activate the conda environment, and launch Jupyter Labs: cd ~/workdir/ conda activate project2 jupyter lab In Jupyter Labs, navigate to spring_school_bioinformatics_microbiology/projects/project2/Project2B/ Then follow the steps below. Download test data Run the 0_download_model_delta notebook to download the pre-trained network and data (note this may take some time, best to run this over a break!) Run the pipeline Work trough the 1_run_pipeline_delta notebook to see how the Delta2 pipeline works. Note: processing speeds on CPU are very slow so use a CUDA compatible GPU based computer whenever possible. Note: we also provided some instructions on how to use Delta on the Scicore cluster of University Basel [see scicore.md ], to use Delta on clusters of other institutions, please contact your local cluster managers. You can also try-out Delta on Google CoLabs . Analyze the results Work trough the 2_post_processing_delta notebook. Analyze the data Once you have the data analyzed, try to extract biological insight from it. Discuss with your tutor what question you could address. For this step on we encourage people to team-up in pairs/small-groups and work together. You can use the 3_explore_data_delta notebook as a starting point. Note: use hardware acceleration on Apple Silicon Create a Conda environment conda create env -f environment_appleM1.yml This was tested with MiniConda for Apple M1 installed from here , if you run into problems try uninstalling your current conda version and install the version above. The .env can be found in the project2 repository. Git clone Delta cd ~choose/a/path/for/delta ( below I use ~/miniconda/delta ) git clone https://gitlab.com/dunloplab/delta.git Add Delta to search path (permanently) add the following line to ~/.zshrc or ~/.bashrc file (depending on if you have a bash or zsh shell): export PYTHONPATH = \" ${ PYTHONPATH } :~/miniconda/delta\" Change ~/miniconda/delta in the line above to the path you used in step 2a After that you can run the notebooks without further changes. You can check if you have access to GPU by running: import tensorflow as tf tf . config . list_physical_devices ()","title":"Project 2B"},{"location":"project2/Project-2B/#segmentation-and-tracking-2d-data-with-delta20","text":"Delta 2.0 is a Deep Learning based workflow that can segment and track 1D (mother machine) and 2D (family machine and agar pads) data. It uses two consecutive U-Net networks to first segment and then track cells. The pipeline is described in this publication . Detailed instruction can be found here","title":"Segmentation and Tracking 2D Data with Delta2.0"},{"location":"project2/Project-2B/#note-on-data","text":"The dataset we will work with here consists of a time-lapse data of a micro-colony of E. coli cells growing on LB agar pads. We have two channels: phase contrast and GFP. The GFP signal comes from a transcriptional reporter for Colicin Ib, a bacteriocin that is regulated by SOS-stress response. Images were taken every 5min. More info on the data can be found here .","title":"Note on data"},{"location":"project2/Project-2B/#install-delta-windows-only-note-for-windows-users","text":"For Mac/Linux users Delta was installed when we created the i2i_env conda environment. However, Delta is not available from Conda for Windows. We have to install in manually, following these steps:","title":"Install Delta (Windows only) Note for windows users"},{"location":"project2/Project-2B/#download-delta-via-git","text":"cd ~/I2ICourse/ git clone https://gitlab.com/dunloplab/delta.git","title":"Download Delta (via git)"},{"location":"project2/Project-2B/#download-delta-manually","text":"Download code here . Unzip file in ~/I2ICourse/ dir.","title":"Download Delta (manually)"},{"location":"project2/Project-2B/#add-delta-path-to-each-notebook","text":"Add the start of every notebook where we import delta, add the following lines at the top of the cell where we import delta: import sys sys . path . append ( \u201c path / to / delta \u201d )","title":"Add Delta path to each Notebook"},{"location":"project2/Project-2B/#getting-started-local-computer","text":"On your computer, navigate to the I2ICourse folder and create a Project2B subfolder: cd ~/I2ICourse/ mkdir Project2B Then activate the conda environment, and launch Jupyter Labs: cd ~/I2ICourse/ conda activate i2i_env jupyter lab In Jupyter Labs, navigate to spring_school_bioinformatics_microbiology/projects/project2/Project2B/ Then follow the steps below.","title":"Getting started - Local Computer"},{"location":"project2/Project-2B/#getting-started-cloud-computer","text":"We will work on the cloud computers for the full project Important: on the cloud computer we need to store all data in the ~/workdir/ folder or sub-folders of this, to make sure that files remain available after restarting the instance. On the cloud computer, navigate to the workdir folder and create a Project2B subfolder: cd ~/workdir/ mkdir Project2B Then activate the conda environment, and launch Jupyter Labs: cd ~/workdir/ conda activate project2 jupyter lab In Jupyter Labs, navigate to spring_school_bioinformatics_microbiology/projects/project2/Project2B/ Then follow the steps below.","title":"Getting started - Cloud Computer"},{"location":"project2/Project-2B/#download-test-data","text":"Run the 0_download_model_delta notebook to download the pre-trained network and data (note this may take some time, best to run this over a break!)","title":"Download test data"},{"location":"project2/Project-2B/#run-the-pipeline","text":"Work trough the 1_run_pipeline_delta notebook to see how the Delta2 pipeline works. Note: processing speeds on CPU are very slow so use a CUDA compatible GPU based computer whenever possible. Note: we also provided some instructions on how to use Delta on the Scicore cluster of University Basel [see scicore.md ], to use Delta on clusters of other institutions, please contact your local cluster managers. You can also try-out Delta on Google CoLabs .","title":"Run the pipeline"},{"location":"project2/Project-2B/#analyze-the-results","text":"Work trough the 2_post_processing_delta notebook.","title":"Analyze the results"},{"location":"project2/Project-2B/#analyze-the-data","text":"Once you have the data analyzed, try to extract biological insight from it. Discuss with your tutor what question you could address. For this step on we encourage people to team-up in pairs/small-groups and work together. You can use the 3_explore_data_delta notebook as a starting point.","title":"Analyze the data"},{"location":"project2/Project-2B/#note-use-hardware-acceleration-on-apple-silicon","text":"","title":"Note: use hardware acceleration on Apple Silicon"},{"location":"project2/Project-2B/#create-a-conda-environment","text":"conda create env -f environment_appleM1.yml This was tested with MiniConda for Apple M1 installed from here , if you run into problems try uninstalling your current conda version and install the version above. The .env can be found in the project2 repository.","title":"Create a Conda environment"},{"location":"project2/Project-2B/#git-clone-delta","text":"cd ~choose/a/path/for/delta ( below I use ~/miniconda/delta ) git clone https://gitlab.com/dunloplab/delta.git","title":"Git clone Delta"},{"location":"project2/Project-2B/#add-delta-to-search-path-permanently","text":"add the following line to ~/.zshrc or ~/.bashrc file (depending on if you have a bash or zsh shell): export PYTHONPATH = \" ${ PYTHONPATH } :~/miniconda/delta\" Change ~/miniconda/delta in the line above to the path you used in step 2a After that you can run the notebooks without further changes. You can check if you have access to GPU by running: import tensorflow as tf tf . config . list_physical_devices ()","title":"Add Delta to search path (permanently)"},{"location":"project2/Project2_Overview/","text":"Overview of Project 2 The course consists of three main parts: In the first part everyone will work on the same data (Project 2A). We will use a semi-automated workflow to segment time-lapse movie of 2D microfluidic flowcells and analyze how the frequency of two interacting cell types changes over time. In this part everyone works on independently. In the second part we will split the group into two. Half the group will use Delta2.0 to segment and track micro-colonies growing on agar pads (Project 2B). The other half will use BACMMAN to segment and track cells growing in 1D mother-machine microfluidic flowcells (Project 2C). In this part everyone works on independently. In the third part we will use data we obtained in the second part to try to answer biological questions. In this part you are encouraged to team up in pairs and work together.","title":"Overview"},{"location":"project2/Project2_Overview/#overview-of-project-2","text":"The course consists of three main parts: In the first part everyone will work on the same data (Project 2A). We will use a semi-automated workflow to segment time-lapse movie of 2D microfluidic flowcells and analyze how the frequency of two interacting cell types changes over time. In this part everyone works on independently. In the second part we will split the group into two. Half the group will use Delta2.0 to segment and track micro-colonies growing on agar pads (Project 2B). The other half will use BACMMAN to segment and track cells growing in 1D mother-machine microfluidic flowcells (Project 2C). In this part everyone works on independently. In the third part we will use data we obtained in the second part to try to answer biological questions. In this part you are encouraged to team up in pairs and work together.","title":"Overview of Project 2"},{"location":"project2/Project2_Preparation/","text":"Preparation before start of course To get started quickly during the course, we ask you to prepare a few things. General note: this code has been developed for Linux / Mac and Windows users will have to make some modifications. We will try to point them out below. You can also use WSL to run Linux on a Windows machine (you can run windows and linux side-by-side, no reboot required). Optional: Python Tutorial For those with little or no Python experience: we recommend you have a look at the following two notebooks that very briefly introduced the most important concepts and syntax: General python introduction (made available by Oliver Meacock , University of Lausanne) Short intro to Pandas dataframes (made available by Google) Setup Cloud Drive We will need to transfer data between our local computers and cloud workstations, for this you need access to a cloud drive (e.g. Dropbox, Google Drive, Switch Drive, etc.) with at least 3GB of free space. For those based at a Swiss institution: you can setup a free account at SwitchDrive which gives you 100GB of space. Install Software During the course we will use a number of software packages, we ask you to install a few of these before the start of the course. In case you run into any problems please do not hesitate to contact Simon van Vliet (preferentially via Slack). We will use the following software: Ilastik -> Please install before start of course following instructions below BACMMAN (Fiji-ImageJ) -> Please install before start of course following instructions below Python (Anaconda) -> No need to install, we will use cloud-based computers to run our Python code For completeness we also include instructions on how to install Python/Anaconda on your own computers below, however you can ignore these for now. Installation Instructions Ilastik Ilastik is a flexible GUI based application that offers several machine learning based workflow for image analysis. We will use it for supervised pixel segmentation. We will use Ilastik beta version 1.4.0b21 (or newer) in the course Download it here Expand the archive and move the Ilastik application to your application folder Bacmman BACMMAN (BACteria in Mother Machine Analyzer) is a ImageJ plugin that offers a fully automated workflow to analyze mother machine data. Install Fiji Download here On Mac/Linux: copy Fiji app to application folder (or other folder of choice) Note: OSX will give a security warning, please go into settings to give permission to launch Fiji On Windows: copy Fiji app to a folder in your user space e.g. C:\\Users\\[your name]\\ImageJ.app Already have Fiji installed? Please install a fresh copy of Fiji nonetheless! You can have multiple copies of Fiji on your computer, simply rename the new copy of Fiji to e.g. Fiji_Bacmman. Update Fiji Start Fiji Update Fiji with default update sites (ImageJ / Fiji / Java 8): Go to Help -> Update In ImageJ Updater window click on Apply Changes Restart Fiji Repeat until message \u2018Your ImageJ is up to date!\u2019 message appears Install Bacmman Go to Help -> Update In ImageJ Updater window click on Manage update sites In the list select (add a tick to tick box) the following extra update sites: BACMMAN ( https://sites.imagej.net/BACMMAN/ ) BACMMAN-DL ( https://sites.imagej.net/BACMMAN-DL/ ) ImageScience ( https://sites.imagej.net/ImageScience/ ) Click Close In ImageJ Updater window click on Apply Changes Restart Fiji Optional: Install Mac OSX terminal packages Although Mac OSX has a number of Terminal packages included by default (e.g. git), other need to be installed manually. Homebrew is a convenient package manager that allows you to obtain these packages. To install, follow these instructions: install OS X command line tools using: xcode-select --install Install Homebrew package manager using: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" You will be asked for your admin password! Install wget using: brew install wget Install Conda (Python) Note: all our code has been tested with Conda version 4.11 and python version 3.9 Install Anaconda or Mini-conda or Mamba Forge following the provided instruction. Mini-conda is a light weight version of Anaconda. Mamba is a faster, but fully compatible, alternative to conda. Important : if you have an older version of Anaconda installed (Anaconda2 or below) please first remove it and re-install the latest Anaconda3 version! If you already have an Anaconda3 (or Mini Conda) installation, please update to latest version using: conda update conda conda update --all Optionally: you can update to latest python version (3.9), but this is not needed (we will install python 3.9 in a virtual environment below). To update use: conda install python = 3 .9 Get project code and setup conda environment Note: all our code has been tested with Conda version 4.11 and python version 3.9 Download code from github We need to obtain the code we need for the course by cloning the Git repository Open the command line and navigate to your home folder, then create a new folder called I2ICourse for the course: Windows users : if you don\u2019t have git, you can get it here cd ~ mkdir I2ICourse Next navigate to this new folder and use the git clone command to download the course code: cd ~/I2ICourse/ git clone https://github.com/sib-swiss/spring_school_bioinformatics_microbiology.git This will create the folder ~/I2ICourse/spring_school_bioinformatics_microbiology/ which contains all the Jupyter notebooks as well as the other course files Create Conda environment for project It is best practice to use a separate conda environment for each project, this way you avoid conflicts in package requirements. We now create the environment for the course, using the provided environment file, which you can find in ~/I2ICourse/spring_school_bioinformatics_microbiology/projects/project2/environment.yml Important: Linux/Mac users use this command: cd ~/I2ICourse/spring_school_bioinformatics_microbiology/projects/project2/ conda env create -f environment.yml Important: Windows users use this command: (The Delta2 package is not available from conda for windows users, we will install it later by hand) cd ~/I2ICourse/spring_school_bioinformatics_microbiology/projects/project2/ conda env create -f environment_windows.yml Test Conda environment First navigate to your project folder cd ~/I2ICourse/ Then activate the conda environment: conda activate i2i_env Next open jupyter-labs: jupyter lab In Jupyter labs, navigate to /I2ICourse/spring_school_bioinformatics_microbiology/projects/project2/ Then open the test_notebook.ipynb Now run the notebook (see here for instructions ) Notes Trouble-shooting In case of persistent problems, try deleting your existing Conda installation and install the latest version from link above. Note: make sure to backup essential conda environments before doing this! Alternatives You can use Jupyter Notebook instead of Jupyter Lab. Both have same functionality, but Jupyter Lab has a bit nicer interface. To install: replace jupyterlab with notebook To open: replace jupyter-lab with jupyter notebook Alternatively, Visual Studio Code (VS Code) is a cross-platform app that you can use to run Jupyter Notebooks. It has some added advantage compared to Jupyter Notebook / Jupyter Lab: it has a nice and fully customizable interface, a great build-in debugger, and offers several useful extensions such as: Jupyter (required to Jupyter notebooks) Markdown All in One (Markdown support) Python Gitlens (full Git integration) Code Spell Checker (intelligent spell checking) and many others The conda package manager can be rather slow at times. Luckily there is a newer alternative to conda, called mamba . mamba and conda work interchangeably, and use same syntax: just replace conda with mamba . One exception: activating and deactivating environments still has to be done with the conda command. Install mamba using conda install mamba -n base -c conda-forge","title":"Preparation"},{"location":"project2/Project2_Preparation/#preparation-before-start-of-course","text":"To get started quickly during the course, we ask you to prepare a few things. General note: this code has been developed for Linux / Mac and Windows users will have to make some modifications. We will try to point them out below. You can also use WSL to run Linux on a Windows machine (you can run windows and linux side-by-side, no reboot required).","title":"Preparation before start of course"},{"location":"project2/Project2_Preparation/#optional-python-tutorial","text":"For those with little or no Python experience: we recommend you have a look at the following two notebooks that very briefly introduced the most important concepts and syntax: General python introduction (made available by Oliver Meacock , University of Lausanne) Short intro to Pandas dataframes (made available by Google)","title":"Optional: Python Tutorial"},{"location":"project2/Project2_Preparation/#setup-cloud-drive","text":"We will need to transfer data between our local computers and cloud workstations, for this you need access to a cloud drive (e.g. Dropbox, Google Drive, Switch Drive, etc.) with at least 3GB of free space. For those based at a Swiss institution: you can setup a free account at SwitchDrive which gives you 100GB of space.","title":"Setup Cloud Drive"},{"location":"project2/Project2_Preparation/#install-software","text":"During the course we will use a number of software packages, we ask you to install a few of these before the start of the course. In case you run into any problems please do not hesitate to contact Simon van Vliet (preferentially via Slack). We will use the following software: Ilastik -> Please install before start of course following instructions below BACMMAN (Fiji-ImageJ) -> Please install before start of course following instructions below Python (Anaconda) -> No need to install, we will use cloud-based computers to run our Python code For completeness we also include instructions on how to install Python/Anaconda on your own computers below, however you can ignore these for now.","title":"Install Software"},{"location":"project2/Project2_Preparation/#installation-instructions","text":"","title":"Installation Instructions"},{"location":"project2/Project2_Preparation/#ilastik","text":"Ilastik is a flexible GUI based application that offers several machine learning based workflow for image analysis. We will use it for supervised pixel segmentation. We will use Ilastik beta version 1.4.0b21 (or newer) in the course Download it here Expand the archive and move the Ilastik application to your application folder","title":"Ilastik"},{"location":"project2/Project2_Preparation/#bacmman","text":"BACMMAN (BACteria in Mother Machine Analyzer) is a ImageJ plugin that offers a fully automated workflow to analyze mother machine data.","title":"Bacmman"},{"location":"project2/Project2_Preparation/#install-fiji","text":"Download here On Mac/Linux: copy Fiji app to application folder (or other folder of choice) Note: OSX will give a security warning, please go into settings to give permission to launch Fiji On Windows: copy Fiji app to a folder in your user space e.g. C:\\Users\\[your name]\\ImageJ.app","title":"Install Fiji"},{"location":"project2/Project2_Preparation/#already-have-fiji-installed","text":"Please install a fresh copy of Fiji nonetheless! You can have multiple copies of Fiji on your computer, simply rename the new copy of Fiji to e.g. Fiji_Bacmman.","title":"Already have Fiji installed?"},{"location":"project2/Project2_Preparation/#update-fiji","text":"Start Fiji Update Fiji with default update sites (ImageJ / Fiji / Java 8): Go to Help -> Update In ImageJ Updater window click on Apply Changes Restart Fiji Repeat until message \u2018Your ImageJ is up to date!\u2019 message appears","title":"Update Fiji"},{"location":"project2/Project2_Preparation/#install-bacmman","text":"Go to Help -> Update In ImageJ Updater window click on Manage update sites In the list select (add a tick to tick box) the following extra update sites: BACMMAN ( https://sites.imagej.net/BACMMAN/ ) BACMMAN-DL ( https://sites.imagej.net/BACMMAN-DL/ ) ImageScience ( https://sites.imagej.net/ImageScience/ ) Click Close In ImageJ Updater window click on Apply Changes Restart Fiji","title":"Install Bacmman"},{"location":"project2/Project2_Preparation/#optional-install-mac-osx-terminal-packages","text":"Although Mac OSX has a number of Terminal packages included by default (e.g. git), other need to be installed manually. Homebrew is a convenient package manager that allows you to obtain these packages. To install, follow these instructions: install OS X command line tools using: xcode-select --install Install Homebrew package manager using: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" You will be asked for your admin password! Install wget using: brew install wget","title":"Optional: Install Mac OSX terminal packages"},{"location":"project2/Project2_Preparation/#install-conda-python","text":"Note: all our code has been tested with Conda version 4.11 and python version 3.9 Install Anaconda or Mini-conda or Mamba Forge following the provided instruction. Mini-conda is a light weight version of Anaconda. Mamba is a faster, but fully compatible, alternative to conda. Important : if you have an older version of Anaconda installed (Anaconda2 or below) please first remove it and re-install the latest Anaconda3 version! If you already have an Anaconda3 (or Mini Conda) installation, please update to latest version using: conda update conda conda update --all Optionally: you can update to latest python version (3.9), but this is not needed (we will install python 3.9 in a virtual environment below). To update use: conda install python = 3 .9","title":"Install Conda (Python)"},{"location":"project2/Project2_Preparation/#get-project-code-and-setup-conda-environment","text":"Note: all our code has been tested with Conda version 4.11 and python version 3.9","title":"Get project code and setup conda environment"},{"location":"project2/Project2_Preparation/#download-code-from-github","text":"We need to obtain the code we need for the course by cloning the Git repository Open the command line and navigate to your home folder, then create a new folder called I2ICourse for the course: Windows users : if you don\u2019t have git, you can get it here cd ~ mkdir I2ICourse Next navigate to this new folder and use the git clone command to download the course code: cd ~/I2ICourse/ git clone https://github.com/sib-swiss/spring_school_bioinformatics_microbiology.git This will create the folder ~/I2ICourse/spring_school_bioinformatics_microbiology/ which contains all the Jupyter notebooks as well as the other course files","title":"Download code from github"},{"location":"project2/Project2_Preparation/#create-conda-environment-for-project","text":"It is best practice to use a separate conda environment for each project, this way you avoid conflicts in package requirements. We now create the environment for the course, using the provided environment file, which you can find in ~/I2ICourse/spring_school_bioinformatics_microbiology/projects/project2/environment.yml Important: Linux/Mac users use this command: cd ~/I2ICourse/spring_school_bioinformatics_microbiology/projects/project2/ conda env create -f environment.yml Important: Windows users use this command: (The Delta2 package is not available from conda for windows users, we will install it later by hand) cd ~/I2ICourse/spring_school_bioinformatics_microbiology/projects/project2/ conda env create -f environment_windows.yml","title":"Create Conda environment for project"},{"location":"project2/Project2_Preparation/#test-conda-environment","text":"First navigate to your project folder cd ~/I2ICourse/ Then activate the conda environment: conda activate i2i_env Next open jupyter-labs: jupyter lab In Jupyter labs, navigate to /I2ICourse/spring_school_bioinformatics_microbiology/projects/project2/ Then open the test_notebook.ipynb Now run the notebook (see here for instructions )","title":"Test Conda environment"},{"location":"project2/Project2_Preparation/#notes","text":"","title":"Notes"},{"location":"project2/Project2_Preparation/#trouble-shooting","text":"In case of persistent problems, try deleting your existing Conda installation and install the latest version from link above. Note: make sure to backup essential conda environments before doing this!","title":"Trouble-shooting"},{"location":"project2/Project2_Preparation/#alternatives","text":"You can use Jupyter Notebook instead of Jupyter Lab. Both have same functionality, but Jupyter Lab has a bit nicer interface. To install: replace jupyterlab with notebook To open: replace jupyter-lab with jupyter notebook Alternatively, Visual Studio Code (VS Code) is a cross-platform app that you can use to run Jupyter Notebooks. It has some added advantage compared to Jupyter Notebook / Jupyter Lab: it has a nice and fully customizable interface, a great build-in debugger, and offers several useful extensions such as: Jupyter (required to Jupyter notebooks) Markdown All in One (Markdown support) Python Gitlens (full Git integration) Code Spell Checker (intelligent spell checking) and many others The conda package manager can be rather slow at times. Luckily there is a newer alternative to conda, called mamba . mamba and conda work interchangeably, and use same syntax: just replace conda with mamba . One exception: activating and deactivating environments still has to be done with the conda command. Install mamba using conda install mamba -n base -c conda-forge","title":"Alternatives"},{"location":"project2/Project2_Resources/","text":"Image Analysis resources Introduction Talk The slides of the intro talk can be found here Useful Websites ImageSC Forum : great place for all image analysis related questions MIT DL Course : quick intro into deep learning Coursera : Andrew Ng offers several in depth courses on machine/deep learning iBiology : Introduction tutorials on imaging and image analysis Neubias : European Bio-image analysis network Intro to Bioimage analysis Useful paper Great general intro to single cell analysis. BUT: the software list is already completely out of date and misses most modern deep learning based packages) Leygeber, 2019 Great intro in reproducible workflows Miura, 2021 Useful papers for advanced feature extraction This list mostly contains papers by ourselves, for the simple reason that we are familiar with the methods used in them. There are many other good papers out there from other labs too, of course! Accurately estimate cell length: Kiviet, 2014 Estimate cell elongation rate: Dal Co, 2019 Estimate lag time: Dal Co, 2019 Estimate population growth rate using optical flow: Dal Co, 2019 Estimate interaction range: Dal Co, 2020 Correct fluorescent images: van Vliet, 2018 Estimate promoter activity: Locke, 2011","title":"Image Analysis resources"},{"location":"project2/Project2_Resources/#image-analysis-resources","text":"","title":"Image Analysis resources"},{"location":"project2/Project2_Resources/#introduction-talk","text":"The slides of the intro talk can be found here","title":"Introduction Talk"},{"location":"project2/Project2_Resources/#useful-websites","text":"ImageSC Forum : great place for all image analysis related questions MIT DL Course : quick intro into deep learning Coursera : Andrew Ng offers several in depth courses on machine/deep learning iBiology : Introduction tutorials on imaging and image analysis Neubias : European Bio-image analysis network Intro to Bioimage analysis","title":"Useful Websites"},{"location":"project2/Project2_Resources/#useful-paper","text":"Great general intro to single cell analysis. BUT: the software list is already completely out of date and misses most modern deep learning based packages) Leygeber, 2019 Great intro in reproducible workflows Miura, 2021","title":"Useful paper"},{"location":"project2/Project2_Resources/#useful-papers-for-advanced-feature-extraction","text":"This list mostly contains papers by ourselves, for the simple reason that we are familiar with the methods used in them. There are many other good papers out there from other labs too, of course! Accurately estimate cell length: Kiviet, 2014 Estimate cell elongation rate: Dal Co, 2019 Estimate lag time: Dal Co, 2019 Estimate population growth rate using optical flow: Dal Co, 2019 Estimate interaction range: Dal Co, 2020 Correct fluorescent images: van Vliet, 2018 Estimate promoter activity: Locke, 2011","title":"Useful papers for advanced feature extraction"},{"location":"project2/project-2C/","text":"Segmentation and Tracking Mother Machine Data with Bacmman & DistNet Bacmman is a ImageJ plugin for analyzing mother machine data. All interactions with the software are via a GUI making it relatively user-friendly (though with a bit of a steep learning curve). At the backend Bacmman uses a state-of-the-art deep learning network to track and segment cells. Bacmman is described extensively in this Nature Protocols article , however note that some parts are outdated. Specifically, the protocol uses the older, classical algorithm to track and segment cells. The parts that describe the GUI are still very useful though. More up-to-date info can be found on this wiki . The Distnet Deep Learning network is described in this publication . Note: parts of this tutorial have been adapted from this Wiki page of Jean Ollion, on these wiki pages you can also find additional information and tutorials. Note on analysis Analyzing imaging data is computationally intensive and can take a long time depending on the available computer power. Normally you would run Bacmman on a powerful workstation, ideally one with a CUDA enabled GPU. Unfortunately we do not have access to such a workstation during the course, therefore we will demonstrate how the pipeline works using a small sample of our dataset. The rest of the dataset we analyzed before the start of the course, and we will provide you with the results. Note on data Our dataset consist of Pseudomonas aeruginosa cells growing in a Mother Machine flow-cells. The cells were exposed to a fluctuating environnement were both the type and availability of nutrients was changed. During the first 12h of the experiments the media had a high amount of a gluconeogenic carbon source, for the next 12h the media contained no carbon source at all (starvation conditions), for the final 12h the media contained a high amount of a glycolytic carbon source. The cells cary two transcriptional reporters: the first (channel 1, RFP) reports on the expression of the glycolytic pathways, the second (channel 2, GFP) reports on the expression level of gluconeogenic pathways. Images were taken every 10 minutes. The nutrient switches happened after 12h (frame 72) and 24h (frame 144). We have data for 54 positions. The data was kindly provided by Hector Hernandez of the Urs Jenal lab of the Biozentrum of the University of Basel. Download Image and Model Data On your local machine, open the command line and enter the following command to make a project folder cd I2ICourse mkdir Project2C Then download the image data and pre-trained DL network using: cd Project2C curl -o RawData.zip https://drive.switch.ch/index.php/s/GPOQL4yrghaEmHS/download unzip RawData.zip Check the folder content (use ls ), it should contain the following items: 2021-09-24_MM.nd2 : the imaging data in Nikon .nd2 format MM_ph_config.json : configuration file (only phase contrast) MM_UJ_config.json : configuration file (including fluorescence) distnet_model_MM_UJ folder, containing the pre-trained DL model After checking, remove the zip file using rm RawData.zip Create experiment in Bacmman Open Bacmman Open Fiji Go to Plugins -> Bacmman -> Bacteria in Mother Machine Analyzer Set working directory When using Bacmman for the first time choose a working directory. Right-click on the panel below Working Directory and select Choose Local Folder Select the ~/I2ICourse/Project2C/ folder Create new data set Click on Dataset menu and select New Dataset from Template Select the template file located in ~/I2ICourse/Project2C/MM_ph_config.json When asked give the dataset the name MM_test Note: after re-opening Bacmann you can re-open the dataset by double clicking on its name in the Dataset field Add image data Go to Home tab Right click in Positions fields and select Import/re-link images In the popup window select the ~/I2ICourse/Project2C/2021-09-24_MM.nd2 file You should now see a list of positions (only 1 in this case). You can inspect the data by right clicking on an image and select Open Input images . Adapt configuration file Set time-interval Go to the Configuration tab Expand the Pre-Processing Template Go to Time Step and set to 10 (right click on value to change it). This is the time-interval, in minutes, between frames. Click on Dataset menu and select Save configuration changes to save changes Load model weights of Deep-learning network As DiSTNet is a deep-learning based method, it requires trained weights of the model. Here we use a refined model that was trained specifically for Pseudomonas . Go to the Configuration tab Expand Object Classes Expand BacteriaDL Expand Processing Pipeline: SegmentAndTrack Expand Tracker: Distnet Expand Model: TF2engine Expand Tenserflow model Right click on Model file in the popup window select the distnet_model_MM_UI folder and click Open Aside: download default model for E. coli Do not do this step now we use the model selected above. If you do not work with Pseudomonas , it is best to use the default DiSTNet model weights. You can download them directly from within Bacmman. Follow the steps above until you get to Tenserflow model file Now right click on Tenserflow model file and select Configure from library In the popup window, select DistNet_base_tf2 and select Configure Parameter Define and run Pre-processing pipeline Preprocessing steps Bacmman needs to do some pre-processing before the Distnet algorithm can segment cells. Specifically: Images should be cropped to only contain channels Images should be rotated if needed such that channels are vertically aligned Images should be flipped if needed such that channels points down Bacmman provides automated algorithms to do this, these can be adapted to fit your images. Unfortunately this does not always work, at the end of this document we give some hints on how to optimize settings. Test automated pre-processing pipeline Go to Configuration Test Tab In Step select Pre-Processing For speed lets only test a few frames: right click on Frame Range and set range from 0 to 0. Let\u2019s try the CropMicroChannelsPhase2D step that crops the microchannels. Right click on this step and select Test Transformation You can see there is a lot of space at the top. We can reduce this a bit, keeping a margin of ~50 pixel. Now try to find settings that do this. Important spend max 5min on this! Hint: click the arrows to expand the CropMicroChannelsPhase2D step to see all settings, right click on any number or text to change it. Hint: see what effect changing Upper end Y-margin has Ok, hopefully you found settings that worked well, for us Upper end Y-margin=-300 and Lower end Y-margin=160 worked well. Once you are happy with the settings you can save them via Save Configuration Changes in the Dataset menu You can also copy changes in the configuration to other positions by clicking on Copy to all position and you can add it to the template by clicking on copy to template . Important do not use these options after setting positions specific parameters (such as manual crop or flip transformations)! Run pre-processing pipeline Now we can run the pre-processing pipeline: Go to the Home tab In the Positions field select all positions In the Tasks field select Pre-Processing In menubar go to Run and select Run Selected Tasks This step can take several minutes To visualize the pre-processed images right-click on the position and choose Open Pre-Processed Images . Make sure that the cropping worked well for all frames. Run tracking and segmentation Now we can segment and track the cells: Go to the Home tab In the Positions field select all positions In the Tasks field select Segmentat and Track In menubar go to Run and select Run Selected Tasks This step can take several minutes up to even an hour depending on your computers speed, ideally you run this step during a break, but if this is impossible, please check with the Tutors on how best to spend your time while you wait. Inspect segmentation and tracking results Now we can check the results of the segmenting and tracking. Check micro-channel segmentation and tracking To visualize the result of microchannel segmentation and tracking: Go to the Data Browsing tab Right-click on the position and choose Open Hyperstack > Microchannels The pre-processed images will open as a interactive hyperstack (multi-channel & multi-frames image stack), on which microchannels can be selected. To display all segmented microchannels object use the shortcut crtl + A To display all microchannels tracks use the shortcut crtl + Q . Tracks will be displayed as colored contours, each color corresponding to one track. Note that the shortcut are available from the menu Help > Display Shortcut table and that a shortcut preset adapted for QWERTY keyboards can be chosen from the menu Help > Shortcut Presets Check bacterial segmentation and tracking To visualize the result of bacterial segmentation and tracking: Go to the Data Browsing tab Right-click on the position and choose Open Hyperstack > Bacteria The pre-processed images will open as a interactive hyperstack (multi-channel & multi-frames image stack), on which bacteria can be selected. To display all segmented bacteria object use the shortcut crtl + A To display all bacteria tracks use the shortcut crtl + Q . Tracks will be displayed as colored contours, each color corresponding to one track. Note that the shortcut are available from the menu Help > Display Shortcut table and that a shortcut preset adapted for QWERTY keyboards can be chosen from the menu Help > Shortcut Presets Another good way to visualize tracking is to use the Kymograph view: In the Segmentation & Tracking Results area, click on the arrow next to Position #0 to expand the list of micro channels. Right-click on a micro-channel and choose Open Kymograph > Bacteria The resulting image shows a concatenation of the same micro-channel for all time points To display all segmented bacteria object use the shortcut crtl + A To display all bacteria tracks use the shortcut crtl + Q . Tracks will be displayed as colored lines connecting neighboring time points. Extract cell measurements the data Now that we have segmented and tracked cells, we need to extract cell measurements. Go to the Home tab In the Objects field select BacteriaDL in the Tasks field select Measurements & Extract Measurements In menubar go to Run and select Run Selected Tasks Post-process with Python - Personal Computer Follow these instruction if you run the python code locally (see below for instructions on how to run it on the cloud computer) Download the full dataset In the second notebook, we will analyze the full dataset that we prepared before the start of the code. Here we download this dataset Navigate to the project folder and download the data using wget : cd ~/I2ICourse/Project2C wget -O cell_data_all.csv https://drive.switch.ch/index.php/s/DiXnrjTmySyXYzl/download ls You now should have the cell_data_all.csv file in your project folder. Launch Jupyter Labs Navigate to the project folder, activate the conda environment, and launch Jupyter Labs: cd ~/I2ICourse/ conda activate i2i_env jupyter lab In Jupyter Labs, navigate to spring_school_bioinformatics_microbiology/projects/project2/Project2A/ Then open the 0_postprocess_bacmman.ipynb notebook Now run the notebook, see here for instructions on Jupyter Labs and follow the instructions in the notebook. Post-process with Python - Cloud Computer Follow these instructions if you are using the cloud computer to run Python. Export Data on local computer For the next steps we will switch to the cloud computers, but before that we need to transfer the data-file we just created: On your local computer locate the .csv file created by Bacmman, you can find it in: ~/I2Icourse/MM_test/MM_test.csv Upload this file to a cloud drive Create a public share link and copy the address Download data on cloud computer Now login to the cloud computer. Reminder: on the cloud computer we need to store all data in the ~/workdir/ folder or sub-folders of this, to make sure that files remain available after restarting the instance. On the cloud computer, navigate to the workdir folder and create a Project2C subfolder: cd ~/workdir/ mkdir Project2C Now we download the data file we just uploaded to the cloud: cd Project2C curl -o MM_test_1.csv public_link_to_your_zip_file Download the full dataset In the second notebook, we will analyze the full dataset that we prepared before the start of the code. Here we download this dataset On the cloud computer navigate to the project folder and download the data using wget : cd ~/workdir/Project2C curl -o cell_data_all.csv https://drive.switch.ch/index.php/s/DiXnrjTmySyXYzl/download ls You now should have the cell_data_all.csv file in your project folder. Launch Jupyter Labs On the cloud computer navigate to the ~/workdir/ folder, activate the conda environment, and launch Jupyter Labs: cd ~/workdir/ conda activate project2 jupyter lab In Jupyter Labs, navigate to spring_school_bioinformatics_microbiology/projects/project2/Project2C/ Then open the 0_postprocess_bacmman.ipynb notebook and follow the instructions there. Additional Information - Adapting pre-processing configurations Do not do read this now, this is additional information for when you want to use Bacmman with your own data later on One of the hardest parts of setting up Bacmman is to find a pre-processing configurations that works well. Here we give some details on how to adapt the configuration to your data. Optimize pre-processing pipeline Go to Configuration Test Tab In Step select Pre-Processing For speed lets only test a few frames: right click on Frame Range and set range from 0 to 5 (you can reduce this further if needed) To test a step, simply right-click on it and select select Test Transformation This will run all transformation up-to and including the selected step and will show you the result. Check the result and change the transformation parameters if needed Note: you can right click on almost anything in Bacmmann to see and change settings. Note: sometimes you have to put in numbers that don\u2019t make sense, e.g. to make the AutoFlipY work you might have to set the micro-channel height to a value that is completely different from the actual micro-channel height It can be hard to find good settings, and some pre-processing might need to be done by hand. To remove an automated step, right click on it and select remove . You can also add new modules. In the top right list are all Available Modules. To add one, right click on Pre-Processing Pipeline , this adds a new Transformation. Select this one, and then click on the desired module in the Available Modules list. Manual pre-processing steps Sometimes finding automatic settings might be too hard, in that case you can then use manual cropping and flipping as shown in this screen shot: With the simple crop option, you need to set the crop box manually for each position. Important: for channels that point up, first add a Flip step. This should be done before the crop (see screenshot above). Note: to set the crop-box, first test the SimpleCrop with the default settings, this opens the image without cropping. Now you can draw a bounding box around the channels. Make sure to exclude the exits of the channels where there is a strong phase artifact, at the top keep a bit of space (10-20 pixels) to accommodate stage jitter (see screenshot). Also make sure that the crop works for all frames (adapt frame range to include all frames). Write down the crop-box size and location and enter the numbers in the settings.Keep the full width of the image (do not change x-settings) and only change the y-values (see screenshot) Repeat this for all positions. Pre-processing outside of Bacmman Sometime it is also just easier to do some pre-processing before importing data into Bacmman. For example, in case a certain image region (such as channel numbering) interferes with automated cropping, you can first crop the images manually to remove this area (see the Project 2A part for how to crop) and then use the automated cropping on these pre-processed images. Storing configuration You can copy changes in the configuration to other positions by clicking on Copy to all position and you can add it to the template by clicking on copy to template . Important do not use these options after setting positions specific parameters (such as manual crop or flip transformations)! Save the configuration via the Dataset menu","title":"Project 2C"},{"location":"project2/project-2C/#segmentation-and-tracking-mother-machine-data-with-bacmman-distnet","text":"Bacmman is a ImageJ plugin for analyzing mother machine data. All interactions with the software are via a GUI making it relatively user-friendly (though with a bit of a steep learning curve). At the backend Bacmman uses a state-of-the-art deep learning network to track and segment cells. Bacmman is described extensively in this Nature Protocols article , however note that some parts are outdated. Specifically, the protocol uses the older, classical algorithm to track and segment cells. The parts that describe the GUI are still very useful though. More up-to-date info can be found on this wiki . The Distnet Deep Learning network is described in this publication . Note: parts of this tutorial have been adapted from this Wiki page of Jean Ollion, on these wiki pages you can also find additional information and tutorials.","title":"Segmentation and Tracking Mother Machine Data with Bacmman &amp; DistNet"},{"location":"project2/project-2C/#note-on-analysis","text":"Analyzing imaging data is computationally intensive and can take a long time depending on the available computer power. Normally you would run Bacmman on a powerful workstation, ideally one with a CUDA enabled GPU. Unfortunately we do not have access to such a workstation during the course, therefore we will demonstrate how the pipeline works using a small sample of our dataset. The rest of the dataset we analyzed before the start of the course, and we will provide you with the results.","title":"Note on analysis"},{"location":"project2/project-2C/#note-on-data","text":"Our dataset consist of Pseudomonas aeruginosa cells growing in a Mother Machine flow-cells. The cells were exposed to a fluctuating environnement were both the type and availability of nutrients was changed. During the first 12h of the experiments the media had a high amount of a gluconeogenic carbon source, for the next 12h the media contained no carbon source at all (starvation conditions), for the final 12h the media contained a high amount of a glycolytic carbon source. The cells cary two transcriptional reporters: the first (channel 1, RFP) reports on the expression of the glycolytic pathways, the second (channel 2, GFP) reports on the expression level of gluconeogenic pathways. Images were taken every 10 minutes. The nutrient switches happened after 12h (frame 72) and 24h (frame 144). We have data for 54 positions. The data was kindly provided by Hector Hernandez of the Urs Jenal lab of the Biozentrum of the University of Basel.","title":"Note on data"},{"location":"project2/project-2C/#download-image-and-model-data","text":"On your local machine, open the command line and enter the following command to make a project folder cd I2ICourse mkdir Project2C Then download the image data and pre-trained DL network using: cd Project2C curl -o RawData.zip https://drive.switch.ch/index.php/s/GPOQL4yrghaEmHS/download unzip RawData.zip Check the folder content (use ls ), it should contain the following items: 2021-09-24_MM.nd2 : the imaging data in Nikon .nd2 format MM_ph_config.json : configuration file (only phase contrast) MM_UJ_config.json : configuration file (including fluorescence) distnet_model_MM_UJ folder, containing the pre-trained DL model After checking, remove the zip file using rm RawData.zip","title":"Download Image and Model Data"},{"location":"project2/project-2C/#create-experiment-in-bacmman","text":"","title":"Create experiment in Bacmman"},{"location":"project2/project-2C/#open-bacmman","text":"Open Fiji Go to Plugins -> Bacmman -> Bacteria in Mother Machine Analyzer","title":"Open Bacmman"},{"location":"project2/project-2C/#set-working-directory","text":"When using Bacmman for the first time choose a working directory. Right-click on the panel below Working Directory and select Choose Local Folder Select the ~/I2ICourse/Project2C/ folder","title":"Set working directory"},{"location":"project2/project-2C/#create-new-data-set","text":"Click on Dataset menu and select New Dataset from Template Select the template file located in ~/I2ICourse/Project2C/MM_ph_config.json When asked give the dataset the name MM_test Note: after re-opening Bacmann you can re-open the dataset by double clicking on its name in the Dataset field","title":"Create new data set"},{"location":"project2/project-2C/#add-image-data","text":"Go to Home tab Right click in Positions fields and select Import/re-link images In the popup window select the ~/I2ICourse/Project2C/2021-09-24_MM.nd2 file You should now see a list of positions (only 1 in this case). You can inspect the data by right clicking on an image and select Open Input images .","title":"Add image data"},{"location":"project2/project-2C/#adapt-configuration-file","text":"","title":"Adapt configuration file"},{"location":"project2/project-2C/#set-time-interval","text":"Go to the Configuration tab Expand the Pre-Processing Template Go to Time Step and set to 10 (right click on value to change it). This is the time-interval, in minutes, between frames. Click on Dataset menu and select Save configuration changes to save changes","title":"Set time-interval"},{"location":"project2/project-2C/#load-model-weights-of-deep-learning-network","text":"As DiSTNet is a deep-learning based method, it requires trained weights of the model. Here we use a refined model that was trained specifically for Pseudomonas . Go to the Configuration tab Expand Object Classes Expand BacteriaDL Expand Processing Pipeline: SegmentAndTrack Expand Tracker: Distnet Expand Model: TF2engine Expand Tenserflow model Right click on Model file in the popup window select the distnet_model_MM_UI folder and click Open","title":"Load model weights of Deep-learning network"},{"location":"project2/project-2C/#aside-download-default-model-for-e-coli","text":"Do not do this step now we use the model selected above. If you do not work with Pseudomonas , it is best to use the default DiSTNet model weights. You can download them directly from within Bacmman. Follow the steps above until you get to Tenserflow model file Now right click on Tenserflow model file and select Configure from library In the popup window, select DistNet_base_tf2 and select Configure Parameter","title":"Aside: download default model for E. coli"},{"location":"project2/project-2C/#define-and-run-pre-processing-pipeline","text":"","title":"Define and run Pre-processing pipeline"},{"location":"project2/project-2C/#preprocessing-steps","text":"Bacmman needs to do some pre-processing before the Distnet algorithm can segment cells. Specifically: Images should be cropped to only contain channels Images should be rotated if needed such that channels are vertically aligned Images should be flipped if needed such that channels points down Bacmman provides automated algorithms to do this, these can be adapted to fit your images. Unfortunately this does not always work, at the end of this document we give some hints on how to optimize settings.","title":"Preprocessing steps"},{"location":"project2/project-2C/#test-automated-pre-processing-pipeline","text":"Go to Configuration Test Tab In Step select Pre-Processing For speed lets only test a few frames: right click on Frame Range and set range from 0 to 0. Let\u2019s try the CropMicroChannelsPhase2D step that crops the microchannels. Right click on this step and select Test Transformation You can see there is a lot of space at the top. We can reduce this a bit, keeping a margin of ~50 pixel. Now try to find settings that do this. Important spend max 5min on this! Hint: click the arrows to expand the CropMicroChannelsPhase2D step to see all settings, right click on any number or text to change it. Hint: see what effect changing Upper end Y-margin has Ok, hopefully you found settings that worked well, for us Upper end Y-margin=-300 and Lower end Y-margin=160 worked well. Once you are happy with the settings you can save them via Save Configuration Changes in the Dataset menu You can also copy changes in the configuration to other positions by clicking on Copy to all position and you can add it to the template by clicking on copy to template . Important do not use these options after setting positions specific parameters (such as manual crop or flip transformations)!","title":"Test automated pre-processing pipeline"},{"location":"project2/project-2C/#run-pre-processing-pipeline","text":"Now we can run the pre-processing pipeline: Go to the Home tab In the Positions field select all positions In the Tasks field select Pre-Processing In menubar go to Run and select Run Selected Tasks This step can take several minutes To visualize the pre-processed images right-click on the position and choose Open Pre-Processed Images . Make sure that the cropping worked well for all frames.","title":"Run pre-processing pipeline"},{"location":"project2/project-2C/#run-tracking-and-segmentation","text":"Now we can segment and track the cells: Go to the Home tab In the Positions field select all positions In the Tasks field select Segmentat and Track In menubar go to Run and select Run Selected Tasks This step can take several minutes up to even an hour depending on your computers speed, ideally you run this step during a break, but if this is impossible, please check with the Tutors on how best to spend your time while you wait.","title":"Run tracking and segmentation"},{"location":"project2/project-2C/#inspect-segmentation-and-tracking-results","text":"Now we can check the results of the segmenting and tracking.","title":"Inspect segmentation and tracking results"},{"location":"project2/project-2C/#check-micro-channel-segmentation-and-tracking","text":"To visualize the result of microchannel segmentation and tracking: Go to the Data Browsing tab Right-click on the position and choose Open Hyperstack > Microchannels The pre-processed images will open as a interactive hyperstack (multi-channel & multi-frames image stack), on which microchannels can be selected. To display all segmented microchannels object use the shortcut crtl + A To display all microchannels tracks use the shortcut crtl + Q . Tracks will be displayed as colored contours, each color corresponding to one track. Note that the shortcut are available from the menu Help > Display Shortcut table and that a shortcut preset adapted for QWERTY keyboards can be chosen from the menu Help > Shortcut Presets","title":"Check micro-channel segmentation and tracking"},{"location":"project2/project-2C/#check-bacterial-segmentation-and-tracking","text":"To visualize the result of bacterial segmentation and tracking: Go to the Data Browsing tab Right-click on the position and choose Open Hyperstack > Bacteria The pre-processed images will open as a interactive hyperstack (multi-channel & multi-frames image stack), on which bacteria can be selected. To display all segmented bacteria object use the shortcut crtl + A To display all bacteria tracks use the shortcut crtl + Q . Tracks will be displayed as colored contours, each color corresponding to one track. Note that the shortcut are available from the menu Help > Display Shortcut table and that a shortcut preset adapted for QWERTY keyboards can be chosen from the menu Help > Shortcut Presets Another good way to visualize tracking is to use the Kymograph view: In the Segmentation & Tracking Results area, click on the arrow next to Position #0 to expand the list of micro channels. Right-click on a micro-channel and choose Open Kymograph > Bacteria The resulting image shows a concatenation of the same micro-channel for all time points To display all segmented bacteria object use the shortcut crtl + A To display all bacteria tracks use the shortcut crtl + Q . Tracks will be displayed as colored lines connecting neighboring time points.","title":"Check bacterial segmentation and tracking"},{"location":"project2/project-2C/#extract-cell-measurements-the-data","text":"Now that we have segmented and tracked cells, we need to extract cell measurements. Go to the Home tab In the Objects field select BacteriaDL in the Tasks field select Measurements & Extract Measurements In menubar go to Run and select Run Selected Tasks","title":"Extract cell measurements the data"},{"location":"project2/project-2C/#post-process-with-python-personal-computer","text":"Follow these instruction if you run the python code locally (see below for instructions on how to run it on the cloud computer)","title":"Post-process with Python - Personal Computer"},{"location":"project2/project-2C/#download-the-full-dataset","text":"In the second notebook, we will analyze the full dataset that we prepared before the start of the code. Here we download this dataset Navigate to the project folder and download the data using wget : cd ~/I2ICourse/Project2C wget -O cell_data_all.csv https://drive.switch.ch/index.php/s/DiXnrjTmySyXYzl/download ls You now should have the cell_data_all.csv file in your project folder.","title":"Download the full dataset"},{"location":"project2/project-2C/#launch-jupyter-labs","text":"Navigate to the project folder, activate the conda environment, and launch Jupyter Labs: cd ~/I2ICourse/ conda activate i2i_env jupyter lab In Jupyter Labs, navigate to spring_school_bioinformatics_microbiology/projects/project2/Project2A/ Then open the 0_postprocess_bacmman.ipynb notebook Now run the notebook, see here for instructions on Jupyter Labs and follow the instructions in the notebook.","title":"Launch Jupyter Labs"},{"location":"project2/project-2C/#post-process-with-python-cloud-computer","text":"Follow these instructions if you are using the cloud computer to run Python.","title":"Post-process with Python - Cloud Computer"},{"location":"project2/project-2C/#export-data-on-local-computer","text":"For the next steps we will switch to the cloud computers, but before that we need to transfer the data-file we just created: On your local computer locate the .csv file created by Bacmman, you can find it in: ~/I2Icourse/MM_test/MM_test.csv Upload this file to a cloud drive Create a public share link and copy the address","title":"Export Data on local computer"},{"location":"project2/project-2C/#download-data-on-cloud-computer","text":"Now login to the cloud computer. Reminder: on the cloud computer we need to store all data in the ~/workdir/ folder or sub-folders of this, to make sure that files remain available after restarting the instance. On the cloud computer, navigate to the workdir folder and create a Project2C subfolder: cd ~/workdir/ mkdir Project2C Now we download the data file we just uploaded to the cloud: cd Project2C curl -o MM_test_1.csv public_link_to_your_zip_file","title":"Download data on cloud computer"},{"location":"project2/project-2C/#download-the-full-dataset_1","text":"In the second notebook, we will analyze the full dataset that we prepared before the start of the code. Here we download this dataset On the cloud computer navigate to the project folder and download the data using wget : cd ~/workdir/Project2C curl -o cell_data_all.csv https://drive.switch.ch/index.php/s/DiXnrjTmySyXYzl/download ls You now should have the cell_data_all.csv file in your project folder.","title":"Download the full dataset"},{"location":"project2/project-2C/#launch-jupyter-labs_1","text":"On the cloud computer navigate to the ~/workdir/ folder, activate the conda environment, and launch Jupyter Labs: cd ~/workdir/ conda activate project2 jupyter lab In Jupyter Labs, navigate to spring_school_bioinformatics_microbiology/projects/project2/Project2C/ Then open the 0_postprocess_bacmman.ipynb notebook and follow the instructions there.","title":"Launch Jupyter Labs"},{"location":"project2/project-2C/#additional-information-adapting-pre-processing-configurations","text":"Do not do read this now, this is additional information for when you want to use Bacmman with your own data later on One of the hardest parts of setting up Bacmman is to find a pre-processing configurations that works well. Here we give some details on how to adapt the configuration to your data.","title":"Additional Information - Adapting pre-processing configurations"},{"location":"project2/project-2C/#optimize-pre-processing-pipeline","text":"Go to Configuration Test Tab In Step select Pre-Processing For speed lets only test a few frames: right click on Frame Range and set range from 0 to 5 (you can reduce this further if needed) To test a step, simply right-click on it and select select Test Transformation This will run all transformation up-to and including the selected step and will show you the result. Check the result and change the transformation parameters if needed Note: you can right click on almost anything in Bacmmann to see and change settings. Note: sometimes you have to put in numbers that don\u2019t make sense, e.g. to make the AutoFlipY work you might have to set the micro-channel height to a value that is completely different from the actual micro-channel height It can be hard to find good settings, and some pre-processing might need to be done by hand. To remove an automated step, right click on it and select remove . You can also add new modules. In the top right list are all Available Modules. To add one, right click on Pre-Processing Pipeline , this adds a new Transformation. Select this one, and then click on the desired module in the Available Modules list.","title":"Optimize pre-processing pipeline"},{"location":"project2/project-2C/#manual-pre-processing-steps","text":"Sometimes finding automatic settings might be too hard, in that case you can then use manual cropping and flipping as shown in this screen shot: With the simple crop option, you need to set the crop box manually for each position. Important: for channels that point up, first add a Flip step. This should be done before the crop (see screenshot above). Note: to set the crop-box, first test the SimpleCrop with the default settings, this opens the image without cropping. Now you can draw a bounding box around the channels. Make sure to exclude the exits of the channels where there is a strong phase artifact, at the top keep a bit of space (10-20 pixels) to accommodate stage jitter (see screenshot). Also make sure that the crop works for all frames (adapt frame range to include all frames). Write down the crop-box size and location and enter the numbers in the settings.Keep the full width of the image (do not change x-settings) and only change the y-values (see screenshot) Repeat this for all positions.","title":"Manual pre-processing steps"},{"location":"project2/project-2C/#pre-processing-outside-of-bacmman","text":"Sometime it is also just easier to do some pre-processing before importing data into Bacmman. For example, in case a certain image region (such as channel numbering) interferes with automated cropping, you can first crop the images manually to remove this area (see the Project 2A part for how to crop) and then use the automated cropping on these pre-processed images.","title":"Pre-processing outside of Bacmman"},{"location":"project2/project-2C/#storing-configuration","text":"You can copy changes in the configuration to other positions by clicking on Copy to all position and you can add it to the template by clicking on copy to template . Important do not use these options after setting positions specific parameters (such as manual crop or flip transformations)! Save the configuration via the Dataset menu","title":"Storing configuration"},{"location":"project3/Overview/","text":"Overview of the project The project is composed of two steps: Step 1: Taxonomic profiling, the process of identifying which species are present and their relative abundance. The input are metagenomic samples (reads) from the biosample, in the form of fastq files. What you get at the end is a taxonomic profile, describing the bacterial composition of the sample. Step 2: Comparative metagenomic analysis. Here we will evaluate which species are correlated to helthy or diseased patients. In this example we will focus on colorectal cancer patients. Project timeline Day Time Topic Tuesday Morning Step1: Obtain taxonomic profiles Tuesday Afternoon Step1: Explore properties of human gut taxonomic profiles Wednesday Morning Step2: Association testing Thursday Morning Step2: Machine learning Thursday Afternoon Work on your presentations","title":"Overview"},{"location":"project3/Overview/#overview-of-the-project","text":"The project is composed of two steps: Step 1: Taxonomic profiling, the process of identifying which species are present and their relative abundance. The input are metagenomic samples (reads) from the biosample, in the form of fastq files. What you get at the end is a taxonomic profile, describing the bacterial composition of the sample. Step 2: Comparative metagenomic analysis. Here we will evaluate which species are correlated to helthy or diseased patients. In this example we will focus on colorectal cancer patients.","title":"Overview of the project"},{"location":"project3/Overview/#project-timeline","text":"Day Time Topic Tuesday Morning Step1: Obtain taxonomic profiles Tuesday Afternoon Step1: Explore properties of human gut taxonomic profiles Wednesday Morning Step2: Association testing Thursday Morning Step2: Machine learning Thursday Afternoon Work on your presentations","title":"Project timeline"},{"location":"project3/Prepare-before-start-of-the-course/","text":"Install software and packages To get started quickly during the course, we ask you to prepare a few things before the start of the course. We will use the following software: Trimmomatic fastQC mOTUs MAPseq SIAMCAT Please install these following the instructions below. Install four tools using Miniconda Conda is a package management system and environment management system that allow to quickly install, run and update packages and their dependencies. The first four tools can be easily installed using conda, we suggest to use Miniconda . To install miniconda, follow the instructions for MacOS , Windows or Linux . After installing miniconda, create a file named NCCR_p3.yaml with: name: NCCR_p3 channels: - conda-forge - defaults - bioconda dependencies: - python = 3 .8 - fastqc = 0 .11.9 - motus = 3 .0.1 - trimmomatic = 0 .39 - mapseq = 1 .2.6 In the terminal you can then type: conda env create -f NCCR_p3.yaml To create an environment with the four tools that we will run in the terminal. You need to activate the environment before using it: source activate NCCR_p3 Note that mOTUs require around 7Gb of space and it will download 3.5 Gb when installing. Hence the installation can take a few minutes. Problems installing with conda If you have problem installing the conda environment, it might be due to the size required for mOTUs. We suggest to remove motus and install it with pip . First create a new yaml file (names NCCR_p3_test2.yaml ): name: NCCR_p3_test2 channels: - conda-forge - defaults - bioconda dependencies: - python = 3 .8 - fastqc = 0 .11.9 - bwa - samtools - pip - trimmomatic = 0 .39 - mapseq = 1 .2.6 Create and activate the environment: conda env create -f NCCR_p3_test2.yaml source activate NCCR_p3_test2 Install mOTUs with pip: pip install motu-profiler And download the database manually: motus downloadDB Installing R packages Within R (we suggest to use R studio), type: ##R Version 4.0.2 or above #tidyverse packages for plotting and data wrangling install.packages(\"tidyverse\") #SIAMCAT if (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\") BiocManager::install(\"SIAMCAT\")","title":"Preparation"},{"location":"project3/Prepare-before-start-of-the-course/#install-software-and-packages","text":"To get started quickly during the course, we ask you to prepare a few things before the start of the course. We will use the following software: Trimmomatic fastQC mOTUs MAPseq SIAMCAT Please install these following the instructions below.","title":"Install software and packages"},{"location":"project3/Prepare-before-start-of-the-course/#install-four-tools-using-miniconda","text":"Conda is a package management system and environment management system that allow to quickly install, run and update packages and their dependencies. The first four tools can be easily installed using conda, we suggest to use Miniconda . To install miniconda, follow the instructions for MacOS , Windows or Linux . After installing miniconda, create a file named NCCR_p3.yaml with: name: NCCR_p3 channels: - conda-forge - defaults - bioconda dependencies: - python = 3 .8 - fastqc = 0 .11.9 - motus = 3 .0.1 - trimmomatic = 0 .39 - mapseq = 1 .2.6 In the terminal you can then type: conda env create -f NCCR_p3.yaml To create an environment with the four tools that we will run in the terminal. You need to activate the environment before using it: source activate NCCR_p3 Note that mOTUs require around 7Gb of space and it will download 3.5 Gb when installing. Hence the installation can take a few minutes.","title":"Install four tools using Miniconda"},{"location":"project3/Prepare-before-start-of-the-course/#problems-installing-with-conda","text":"If you have problem installing the conda environment, it might be due to the size required for mOTUs. We suggest to remove motus and install it with pip . First create a new yaml file (names NCCR_p3_test2.yaml ): name: NCCR_p3_test2 channels: - conda-forge - defaults - bioconda dependencies: - python = 3 .8 - fastqc = 0 .11.9 - bwa - samtools - pip - trimmomatic = 0 .39 - mapseq = 1 .2.6 Create and activate the environment: conda env create -f NCCR_p3_test2.yaml source activate NCCR_p3_test2 Install mOTUs with pip: pip install motu-profiler And download the database manually: motus downloadDB","title":"Problems installing with conda"},{"location":"project3/Prepare-before-start-of-the-course/#installing-r-packages","text":"Within R (we suggest to use R studio), type: ##R Version 4.0.2 or above #tidyverse packages for plotting and data wrangling install.packages(\"tidyverse\") #SIAMCAT if (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\") BiocManager::install(\"SIAMCAT\")","title":"Installing R packages"},{"location":"project3/Remote-environment/","text":"Setting in the remote environment General structure Each student will get their own container for the duration of the course, where they can work, share and store data. This environment can be accessed with any internet browser, by going to your assigned port, which should look similar like http://1.23.45.678:10034. In the container, you can find the following shared directories: /data: read only, and shared between all running containers. This directory is used to have a single place to store data /group_work: read and write enabled for all participants, and shared between all containers. This can be used to share data/scripts between students. ~/workdir: read and write enabled, and only shared between containers assigned to the same participant. All directories other than the shared directories only exist within the container. When visiting your assigned port, you should see the following window: The features and parts you will mainly use are numbered. While going to the tabs (1) or (2) you can switch between our two interfaces: For our course, we will work mostly in the Terminal (Bash) for Part 1, and in the console (R) for Part2. On the top right (3) is a panel in which you will be see which variables, dataframes etc. exist in your current session. On the bottom right (4) you can see a visual representation of your directory structure, and you are also able to manually switch folders, create, delete and move files around. Terminal In order to use the preinstalled programms you will use during this course, it is first necessary to activate the conda environment in which they were installed. To do this, go to the terminal tab and type conda activate project3 Afterwards, you should see (project3) on the left side, indicating that you are currently working within that environment. Now, you should be able to use the programms as described in their respective guides. For example, typing the following command should now print out the help section of fastqc. fastqc -h If you need a quick refresher for the most common bash commands, for example to create directories, search for a specific word inside a file and similar tasks, check for example the following guide. Console The console will be used mostly in Part2 of the course. Here, you have a set up R environment with all packages you will need preinstalled. You can create an R Script to better store and modify your code by selecting \u201cR Script\u201d in the icon below File.","title":"Remote Environment"},{"location":"project3/Remote-environment/#setting-in-the-remote-environment","text":"","title":"Setting in the remote environment"},{"location":"project3/Remote-environment/#general-structure","text":"Each student will get their own container for the duration of the course, where they can work, share and store data. This environment can be accessed with any internet browser, by going to your assigned port, which should look similar like http://1.23.45.678:10034. In the container, you can find the following shared directories: /data: read only, and shared between all running containers. This directory is used to have a single place to store data /group_work: read and write enabled for all participants, and shared between all containers. This can be used to share data/scripts between students. ~/workdir: read and write enabled, and only shared between containers assigned to the same participant. All directories other than the shared directories only exist within the container. When visiting your assigned port, you should see the following window: The features and parts you will mainly use are numbered. While going to the tabs (1) or (2) you can switch between our two interfaces: For our course, we will work mostly in the Terminal (Bash) for Part 1, and in the console (R) for Part2. On the top right (3) is a panel in which you will be see which variables, dataframes etc. exist in your current session. On the bottom right (4) you can see a visual representation of your directory structure, and you are also able to manually switch folders, create, delete and move files around.","title":"General structure"},{"location":"project3/Remote-environment/#terminal","text":"In order to use the preinstalled programms you will use during this course, it is first necessary to activate the conda environment in which they were installed. To do this, go to the terminal tab and type conda activate project3 Afterwards, you should see (project3) on the left side, indicating that you are currently working within that environment. Now, you should be able to use the programms as described in their respective guides. For example, typing the following command should now print out the help section of fastqc. fastqc -h If you need a quick refresher for the most common bash commands, for example to create directories, search for a specific word inside a file and similar tasks, check for example the following guide.","title":"Terminal"},{"location":"project3/Remote-environment/#console","text":"The console will be used mostly in Part2 of the course. Here, you have a set up R environment with all packages you will need preinstalled. You can create an R Script to better store and modify your code by selecting \u201cR Script\u201d in the icon below File.","title":"Console"},{"location":"project3/Step-1-sol/","text":"Solutions Step 1: Taxonomic profiling of metagenomic samples with mOTUs General note: this guide has been written assuming you use a Mac or Linux Command Line. Download example sequencing data Explore the files, in particular you can check: How many reads there are per sample? Solution 1 Knowing that each read takes up four lines in the fastq file, we can simply count the number of lines with wc -l and divide the result by 4 . The following command does it all in one line. echo $( cat sampleA_1.fastq | wc -l ) /4 | bc Solution 2 We can count the number of lines with @read : grep -c \"@read\" sampleA_1.fastq What is the average length of the reads? Is there a difference between the read lengths in the forward and reverse files? Solution 1 We can first extract only the sequences: grep -A 1 \"@read\" sampleA_1.fastq | grep -v \"\\-\\-\" | grep -v \"read\" > sequences_sampleA_1 With -A 1 we select also 1 row after the match. With grep -v we remove what is not needed. We can now check the length with: cat sequences_sampleA_1 | awk '{print length}' This will print a big list, we can count how many times each length appear: cat sequences_sampleA_1 | awk '{print length}' | sort | uniq -c | sort -n | tail Which produces: 238 96 242 92 258 93 341 97 344 94 346 95 428 98 849 20 1194 99 61306 100 So the majority of the reads have length 100 (61,306 out of 67,926, 90%) Solution 2 To quickly check the average length of the reads in the terminal, do: awk 'NR%4==2{sum+=length($0)}END{print sum/(NR/4)}' sampleA_1.fastq The average read length in the reverse reads seem to be slightly lower in all the samples. Do you have the same read IDs in the forward and reverse file? Solution Since these are paired reads, the read ids should be identical and in the same order. You can check this in the terminal like so: #get list of read ids from the forward and reverse files grep '@read' sampleA_1.fastq > sampleA_ids_1.txt grep '@read' sampleA_2.fastq > sampleA_ids_2.txt #check if they are identical diff -s sampleA_ids_1.txt sampleA_ids_2.txt There are many different ways of performing the same task. If you have done something different and accomplished the same thing, awesome! Check the quality of the sequencing data Which part of the reads is of lower quality? Solution The ends of the reads are typically of lower quality. This is to be expected. The quality of calls typically degrades as the run progresses due to problems in the sequencing chemistry. - Is there any difference between the quality of the forward and reverse reads? Solution Reverse reads are usually of lower quality than forward reads, particularly at the read ends. Again this is due to the way paired end sequencing is performed with the forward orientiation is sequenced first followed by the reverse orientation. Filter and trim reads Try to run trimmomatic (you can use different parameters). Solution Here is an example command: trimmomatic PE sampleA_1.fastq sampleA_2.fastq sampleA_filtered_1P.fastq sampleA_filtered_1U.fastq sampleA_filtered_2P.fastq sampleA_filtered_2U.fastq ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36 Here is a description of the parameters used in this specific command. You may have explored other parameters too. ILLUMINACLIP : specifies the file containing the adapter sequences to trim and additional parameters on how to perform the adapter trimming. TruSeq3-PE.fa (provided by trimmomatic ) contains the Illumina adapter sequences used by HiSeq and MiSeq machines. LEADING : Remove low quality bases (lower than 3 ) from the beginning of the reads. TRAILING : Remove low quality bases (lower than 3 ) from the ends of the reads. SLIDINGWINDOW : consider a window of bases (here 4 at once) and trim once the average quality within the window falls below a threshold quality (here 15 ). MINLEN : remove reads lower than the specified min length (here 36 ) How many files did trimmomatic generate? What do they contain? Solution 4 files are produced sampleA_filtered_1P, containing the forward reads that pass the filter and have a mate (in filtered_2P); sampleA_filtered_1U, containing the forward reads that pass the filter and do not have a mate (the paired reverse read didn\u2019t pass the filter) sampleA_filtered_2P, containing the reverse reads that pass the filter and have a mate (in filtered_1P); sampleA_filtered_2U, containing the reverse reads that pass the filter and do not have a mate (the paired forward read didn\u2019t pass the filter) - How many reads have been filtered out? Solution 866 reads (1.27%) of all reads were filtered out from sampleA using the above parameters. Check the quality of the filtered reads. Did the quality improve? Solution You can check the quality of the filterd reads again with fastqc. fastqc sampleA_filtered_1P.fastq fastqc sampleA_filtered_2P.fastq The quality of reads (particularly of the reverse reads) has improved! Taxonomic profiling with mOTUs Use motus (manual: link ) to create a profile from the files created by trimmomatic. Solution Here is the mOTU command to generate a taxonomic profile using default parameters. motus profile -f sampleA_filterd_1P.fastq -r sampleA_filtered_2P.fastq -s sampleA_filtered_1U.fastq,sampleA_filtered_2U.fastq -n sampleA -o sampleA_profile.txt - How many species are detected? How many are reference species and how many are unknown species? Solution You can quickly check how many species were detected with: #this also counts unassigned so subtract 1 from the result grep -c -v '0.0000000000\\|#' sampleA_profile.txt 97 species were dectected. Around 3.4 % were unassigned . You can check how many ref-mOTUs were detected using these command: grep -v '0.0000000000\\|#' sampleA_profile.txt > sampleA_profile_detected.txt grep -c 'ref_mOTU_v3_' sampleA_profile_detected.txt 39 ref-mOTUs were detected in sampleA. Note that this number is also reported as stdout when you run motus profile Can you change some parameters in motus to profile more or less species? (Hint, look here ) Solution Precision is the number of TP out of the total number of detected species. Recall is the number of detected species out of all the species actually present in the sample. To increase precision at the cost of recall you can increase parameters -g (default: 3) and -l (default: 75). motus profile -f sampleA_filterd_1P.fastq -r sampleA_filtered_2P.fastq -s sampleA_filtered_1U.fastq,sampleA_filtered_2U.fastq -n sampleA -o sampleA_profile_high_p.txt -g 8 -l 90 We have detected just 37 species. To increase recall at the cost of having more false positives you can do: motus profile -f sampleA_filterd_1P.fastq -r sampleA_filtered_2P.fastq -s sampleA_filtered_1U.fastq,sampleA_filtered_2U.fastq -n sampleA -o sampleA_profile_high_r.txt -g 1 -l 45 We have detected 331 species. How can you merge different motus profiles into one file? Try to profile and then merge three profiles (Sample A, B and C). Solution After creating the individual tax profiles for all the samples, do: motus merge - i sampleA_profile.txt,sampleB_profile.txt,sampleC_profile.txt -o merged_profiles.txt This results in a tab-separated file containing the tax profiles. Taxonomic profiling with MAPseq Similar as with mOTUs, first create a profile for each sample (A,B, and C) and then merge them into one (Check the github page for the command). Solution In order to create a taxonomic profile using MAPseq for sampleA do: mapseq sampleA_filtered_1P.fasta > sampleA.mseq MAPseq seems to be a bit faster than mOTUs (took ~2 min to run) sampleA.mseq contains the results from mapping reads to the reference database of OTUs provided by MAPseq (alignment score, database hit, etc) and the taxnomic classifications along with associated confidences. After generating the .mseq files for all the samples, you can merge them into one OTU table like so: mapseq -otutable sampleA.mseq sampleB.mseq sampleC.mseq -ti 1 -tl 3 > mapseq_otutable_otu97.tsv This creates an OTU table containing reads mapped to 97% OTUs for sampleA, sampleB and sampleC. Note that depending on whether -ti is 0 or 1, what -tl means changes. If you have -ti 0 , then -tl indicates the taxonomic level (0 (domain), 1 (phylum), 2 (class), 3 (order), 4 (family), 5 (genus), 6 (species)) . So if -ti 0 -tl 3 means that the OTU table will report only read counts mapping to order-level NCBI taxonomies. If you have -ti 1 , then -tl indicates the OTU clustering level (1 (90% OTU), 2 (96% OTU), 3 (97% OTU), 4 (98%), 5 (99%)) . So if -ti 1 -tl 3 means that the OTU table will report only read counts mapping to 97% OTUs. To obtain reads mapping to 99% OTUs : mapseq -otutable sampleA.mseq sampleB.mseq sampleC.mseq -ti 0 -tl 5 > mapseq_otutable_otu99.tsv If we increase the clustering level to 99%, we observe fewer species detected for all the samples. This might be because at a finer resolution, we might not be able to assign taxonomies too well resulting in a smaller number of species being profiled. To obtain reads mapping to 96% OTUs : mapseq -otutable sampleA.mseq sampleB.mseq sampleC.mseq -ti 0 -tl 2 > mapseq_otutable_otu96.tsv If we decrease the clustering level to 96%, we observe more species detected for all the samples. Can you compare mOTUs and MAPseq profiles? Solution Profiler species detected in sampleA mOTUs 97 MAPseq 97% 173 MAPseq 99% 121 MAPseq 96% 179 It looks like mOTUs is a bit more conservative at detecting species. Note that since mOTUs and OTUs are defined differently, it might not be straightforward to make a direct comparison. Explore taxonomic profiles Explore the taxonomic profiles ( tax_profile ). Solution Load packages and taxonomic profile into your R environment #packages library ( tidyverse ) #tax profiles load ( url ( \"https://zenodo.org/record/6517497/files/human_microbiome_dataset.Rdata\" )) Let's have a quick peek at our data. dim ( tax_profile ) There are 246 rows and 496 columns. Let's check the content: tax_profile [ 1 : 3 , 1 : 3 ] Which results in: 700002_T0 700002_T1 700002_T2 Blautia 229 1196 1758 Bacteroides 4018 1705 1660 Agathobacter 80 1580 126 The rows are genera and the columns are samples. head ( metadata ) Which is: Subject Timepoint Sex 700002_T0 700002 0 M 700002_T1 700002 12 M 700002_T2 700002 24 M 700002_T3 700002 50 M 700004_T0 700004 0 F 700004_T1 700004 12 F The 496 samples are from 124 patients (sampled 4 times over the course of the year). `tax_profile` contains the read counts of of species (rows) across all samples (columns). For each sample, we know the corresponding subject id, timepoint of sampling and the sex of the subject. Here are some hints of what you can check: How many reads there are per sample? Solution Calculate total number of reads in every sample (also called library size of sample) sample_read_counts <- data.frame ( total_read_counts = colSums ( tax_profile )) How are the sample read counts distributed? ggplot ( data = sample_read_counts ) + geom_histogram ( mapping = aes ( x = total_read_counts ), bins = 60 ) + ylab ( 'Number of samples' ) + xlab ( 'Total number of reads' ) Which results in: Note how variable the total read counts are across all samples. This is a problem because this variation is most likely a result of the sequencing process and not any meaningful biological variation. If you want to compare different samples, is it a problem that there are different read counts? Try to divide each value within a sample by the sum of the reads in that sample to normalise the data (also called relative abundance). Solution We need to remove this technical variation caused by differing total read counts to meaningfully compare samples. We can do this by relative abundance normalization where we divide each value within a sample by the total read counts in that sample. rel_ab = prop.table ( tax_profile , 2 ) Now the abundances of each sample should sum to 1. all ( colSums ( rel_ab ) == 1 ) Which genera are the most and least prevalent? Solution Prevalence of a species refers to the the proportion of samples in which that species is detected. First we calculate prevalence for all the genera. number_of_samples = dim ( tax_profile )[ 2 ] prevalence_df <- data.frame ( Prevalence = rowSums ( rel_ab > 0 ) / number_of_samples , genus = rownames ( tax_profile )) You can plot an histogram of the prevalences: ggplot ( data = prevalence_df ) + geom_histogram ( mapping = aes ( x = Prevalence ), bins = 60 ) You can see that there are many species that apper only few times (on the left), and there are only few species that are present in all samples (on the right). We can also check which genera are the most prevalent: head ( prevalence_df [ order ( prevalence_df $ Prevalence , decreasing = T ),]) Result: Prevalence genus Blautia 1.0000000 Blautia Bacteroides 1.0000000 Bacteroides -1 1.0000000 -1 Faecalibacterium 0.9899194 Faecalibacterium Anaerostipes 0.9879032 Anaerostipes Fusicatenibacter 0.9778226 Fusicatenibacter It means that the genus Blautia and Bacteroides are present in all species. The -1 represents unassigned reads (i.e. that they do not map to any known genus). The least prevalent genera are: Prevalence genus Rosenbergiella 0.002016129 Rosenbergiella 28-4 0.002016129 28-4 Gallicola 0.002016129 Gallicola Sarcina 0.002016129 Sarcina Harryflintia 0.002016129 Harryflintia Paeniclostridium 0.002016129 Paeniclostridium Which are present in only one sample. You can see that the most prevalent species are typically genera that that should be present in all human guts. This type of quick exploration can also serve as a sanity check (is there something we should not be seeing at all?) Is the relative abundance of the different genera normally distributed? Solution If we look at the distribution of all relative abundances with a simple histogram: abundances_df = data.frame ( rel_abundances = as.vector ( rel_ab )) ggplot ( data = abundances_df ) + geom_histogram ( aes ( rel_abundances ), bins = 100 ) + xlab ( \"All relative abundances\" ) We can clearly see that the relative abundances are not normally distributed. Maybe if we log transform the data, the result improve. Note that in order to log transform the data we need to add a small value (in this case 10^-4 ) so that we don\u2019t have the problem of calculating the log of zero. Code: log_rel_ab = data.frame ( rel_abundances = log10 ( as.vector ( rel_ab ) + 10 ^ -4 )) ggplot ( data = log_rel_ab ) + geom_histogram ( aes ( rel_abundances ), bins = 100 ) + xlab ( \"All relative abundances (log10)\" ) + scale_y_log10 () As you can see even when we log transform, the high number of zero makes the distribution not normal. We can check also single genera. Here I select 3 genera: Bacteroides with prevalence of 1, Akkermansia with a prevalence of ~0.5 and Harryflintia with the lowest relative abundance. We can plot them with the following code: df_genera = data.frame ( genus = c ( rep ( \"Bacteroides\" , ncol ( rel_ab )), rep ( \"Akkermansia\" , ncol ( rel_ab )), rep ( \"Harryflintia\" , ncol ( rel_ab ))), rel_ab = c ( rel_ab [ \"Bacteroides\" ,], rel_ab [ \"Akkermansia\" ,], rel_ab [ \"Harryflintia\" ,]) ) ggplot ( data = df_genera ) + geom_histogram ( mapping = aes ( x = rel_ab ), bins = 60 ) + facet_grid ( . ~ genus ) As you can see for Harryflintia there are almost only zeros (only one sample contain this genus). On the other hand Bacteroides can almost have a normal distribution (with a long tail on the right). While Akkermansia shows a tipical microbiome distribution plot with many samples where the measure is zero and then a tail with few samples where the relative abundance is higher. How many zeros there are per sample and per genus? Solution Taxonomic profiles are typically sparse because most species occur in frequently. Do we see this in our data as well? How many 0s are present overall in the data? # we transform the relative abundance in a vector temp = as.vector ( rel_ab ) # and check the length (how many values there are) length ( temp ) # 122016 # and how many values are zero: sum ( temp == 0 ) # 94133 77% of the data are 0s (94133/122016)! If we look at the percentage of 0s per sample: head ( data.frame ( zeros_per_sample = colSums ( rel_ab == 0 ) / ( dim ( rel_ab )[ 1 ]))) Result: zeros_per_sample 700002 _ T0 0.8536585 700002 _ T1 0.7398374 700002 _ T2 0.7520325 700002 _ T3 0.7723577 700004 _ T0 0.7723577 700004 _ T1 0.7723577 We can see that the sparsity is similar across all samples. Do you think that if we had samples from a different environment (like Soil for instance), we might see something different? Percentage of 0s per genus is the same as 1 - prevalence : head ( data.frame ( zeros_per_sample = rowSums ( rel_ab == 0 ) / ( dim ( rel_ab )[ 2 ]))) Result: zeros_per_sample Blautia 0.00000000 Bacteroides 0.00000000 Agathobacter 0.04435484 Faecalibacterium 0.01008065 Bifidobacterium 0.09072581 Fusicatenibacter 0.02217742 Again, we note that some genera are more prevalent than others. How much variability there is within Subject (check the metadata table), compare to between subjects? Or from another perspective, how stable it is the human gut microbiome? Solution 1 There are different ways to explore this problem. We can try to calculate the distance between all possible samples and then compare the distances of samples that come from the same subject and the distance that come from different subjects. For example, you take sample 1 ( 700002_T0 ) and sample 2 ( 700002_T1 ), and since they are from the same subject, we will use the distance between these two samples as an example of within-subject distance. Code: rel_ab = prop.table ( tax_profile , 2 ) log_rel_ab = log10 ( rel_ab + 10 ^ -4 ) n_samples = ncol ( log_rel_ab ) # where to save the result distance = c () type = c () # we go through all the pair of samples and we calculate the distance for ( i in c ( 1 : ( n_samples -1 ) )){ for ( j in c ( ( i +1 ) : n_samples )){ # name of the samples s_i = colnames ( log_rel_ab )[ i ] s_j = colnames ( log_rel_ab )[ j ] # profiles profile_i = log_rel_ab [, i ] profile_j = log_rel_ab [, j ] # calculate the distance as sum of the absolute distance d = sum ( abs ( profile_i - profile_j )) # add the values distance = c ( distance , d ) # add correct type if ( metadata [ s_i , \"Subject\" ] == metadata [ s_j , \"Subject\" ]){ type = c ( type , \"Within distance\" ) } else { type = c ( type , \"Between distance\" ) } } } df = data.frame ( dist = distance , type = type ) ggplot ( df , aes ( type , dist )) + geom_boxplot () It is quite clear that they are different, but we can also test it: wilcox.test ( df [ df $ type == \"Within distance\" ,] $ dist , df [ df $ type == \"Between distance\" ,] $ dist ) Result: Wilcoxon rank sum test with continuity correction data : df [ df $ type == \"Within distance\" , ] $ dist and df [ df $ type == \"Between distance\" , ] $ dist W = 2844084 , p - value < 2.2e-16 alternative hypothesis : true location shift is not equal to 0 This means that samples from the same subject have a low distance, or in other words they are similar to each other. And they are much more similar compared to other subjects, even after 1 year (the last time point is 50 weeks). From this we understand two things: - First, the human microbiome is stable over time - Second, there is a great variability between subjects Solution 2 We can do a PCA plot: rel_ab = prop.table ( tax_profile , 2 ) log_rel_ab = log10 ( rel_ab + 10 ^ -4 ) # remove zero rows log_rel_ab = log_rel_ab [ rowSums ( rel_ab ) > 0 ,] pc <- prcomp ( t ( log_rel_ab ), center = TRUE , scale. = TRUE ) df = data.frame ( pc1 = pc $ x [, 1 ], pc2 = pc $ x [, 2 ], Subject = as.factor ( metadata [ rownames ( pc $ x ), \"Subject\" ]), Timepoint = metadata [ rownames ( pc $ x ), \"Timepoint\" ], Sex = metadata [ rownames ( pc $ x ), \"Sex\" ] ) And plot the result: ggplot ( df , aes ( x = pc1 , y = pc2 , col = Sex )) + geom_point () ggplot ( df , aes ( x = pc1 , y = pc2 , col = Timepoint )) + geom_point () ggplot ( df , aes ( x = pc1 , y = pc2 , col = Subject )) + geom_point () + theme ( legend.position = \"none\" ) There is no particular grouping by sex or by timepoint (as expected). It also seems that the plot based on subject (third plot where each color is a subject) is random. But, we can actually see some structure, in particular outside of the conglomerate of points at the center.","title":"Step 1 solution"},{"location":"project3/Step-1-sol/#solutions-step-1-taxonomic-profiling-of-metagenomic-samples-with-motus","text":"General note: this guide has been written assuming you use a Mac or Linux Command Line.","title":"Solutions Step 1: Taxonomic profiling of metagenomic samples with mOTUs"},{"location":"project3/Step-1-sol/#download-example-sequencing-data","text":"Explore the files, in particular you can check: How many reads there are per sample? Solution 1 Knowing that each read takes up four lines in the fastq file, we can simply count the number of lines with wc -l and divide the result by 4 . The following command does it all in one line. echo $( cat sampleA_1.fastq | wc -l ) /4 | bc Solution 2 We can count the number of lines with @read : grep -c \"@read\" sampleA_1.fastq What is the average length of the reads? Is there a difference between the read lengths in the forward and reverse files? Solution 1 We can first extract only the sequences: grep -A 1 \"@read\" sampleA_1.fastq | grep -v \"\\-\\-\" | grep -v \"read\" > sequences_sampleA_1 With -A 1 we select also 1 row after the match. With grep -v we remove what is not needed. We can now check the length with: cat sequences_sampleA_1 | awk '{print length}' This will print a big list, we can count how many times each length appear: cat sequences_sampleA_1 | awk '{print length}' | sort | uniq -c | sort -n | tail Which produces: 238 96 242 92 258 93 341 97 344 94 346 95 428 98 849 20 1194 99 61306 100 So the majority of the reads have length 100 (61,306 out of 67,926, 90%) Solution 2 To quickly check the average length of the reads in the terminal, do: awk 'NR%4==2{sum+=length($0)}END{print sum/(NR/4)}' sampleA_1.fastq The average read length in the reverse reads seem to be slightly lower in all the samples. Do you have the same read IDs in the forward and reverse file? Solution Since these are paired reads, the read ids should be identical and in the same order. You can check this in the terminal like so: #get list of read ids from the forward and reverse files grep '@read' sampleA_1.fastq > sampleA_ids_1.txt grep '@read' sampleA_2.fastq > sampleA_ids_2.txt #check if they are identical diff -s sampleA_ids_1.txt sampleA_ids_2.txt There are many different ways of performing the same task. If you have done something different and accomplished the same thing, awesome!","title":"Download example sequencing data"},{"location":"project3/Step-1-sol/#check-the-quality-of-the-sequencing-data","text":"Which part of the reads is of lower quality? Solution The ends of the reads are typically of lower quality. This is to be expected. The quality of calls typically degrades as the run progresses due to problems in the sequencing chemistry. - Is there any difference between the quality of the forward and reverse reads? Solution Reverse reads are usually of lower quality than forward reads, particularly at the read ends. Again this is due to the way paired end sequencing is performed with the forward orientiation is sequenced first followed by the reverse orientation.","title":"Check the quality of the sequencing data"},{"location":"project3/Step-1-sol/#filter-and-trim-reads","text":"Try to run trimmomatic (you can use different parameters). Solution Here is an example command: trimmomatic PE sampleA_1.fastq sampleA_2.fastq sampleA_filtered_1P.fastq sampleA_filtered_1U.fastq sampleA_filtered_2P.fastq sampleA_filtered_2U.fastq ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36 Here is a description of the parameters used in this specific command. You may have explored other parameters too. ILLUMINACLIP : specifies the file containing the adapter sequences to trim and additional parameters on how to perform the adapter trimming. TruSeq3-PE.fa (provided by trimmomatic ) contains the Illumina adapter sequences used by HiSeq and MiSeq machines. LEADING : Remove low quality bases (lower than 3 ) from the beginning of the reads. TRAILING : Remove low quality bases (lower than 3 ) from the ends of the reads. SLIDINGWINDOW : consider a window of bases (here 4 at once) and trim once the average quality within the window falls below a threshold quality (here 15 ). MINLEN : remove reads lower than the specified min length (here 36 ) How many files did trimmomatic generate? What do they contain? Solution 4 files are produced sampleA_filtered_1P, containing the forward reads that pass the filter and have a mate (in filtered_2P); sampleA_filtered_1U, containing the forward reads that pass the filter and do not have a mate (the paired reverse read didn\u2019t pass the filter) sampleA_filtered_2P, containing the reverse reads that pass the filter and have a mate (in filtered_1P); sampleA_filtered_2U, containing the reverse reads that pass the filter and do not have a mate (the paired forward read didn\u2019t pass the filter) - How many reads have been filtered out? Solution 866 reads (1.27%) of all reads were filtered out from sampleA using the above parameters. Check the quality of the filtered reads. Did the quality improve? Solution You can check the quality of the filterd reads again with fastqc. fastqc sampleA_filtered_1P.fastq fastqc sampleA_filtered_2P.fastq The quality of reads (particularly of the reverse reads) has improved!","title":"Filter and trim reads"},{"location":"project3/Step-1-sol/#taxonomic-profiling-with-motus","text":"Use motus (manual: link ) to create a profile from the files created by trimmomatic. Solution Here is the mOTU command to generate a taxonomic profile using default parameters. motus profile -f sampleA_filterd_1P.fastq -r sampleA_filtered_2P.fastq -s sampleA_filtered_1U.fastq,sampleA_filtered_2U.fastq -n sampleA -o sampleA_profile.txt - How many species are detected? How many are reference species and how many are unknown species? Solution You can quickly check how many species were detected with: #this also counts unassigned so subtract 1 from the result grep -c -v '0.0000000000\\|#' sampleA_profile.txt 97 species were dectected. Around 3.4 % were unassigned . You can check how many ref-mOTUs were detected using these command: grep -v '0.0000000000\\|#' sampleA_profile.txt > sampleA_profile_detected.txt grep -c 'ref_mOTU_v3_' sampleA_profile_detected.txt 39 ref-mOTUs were detected in sampleA. Note that this number is also reported as stdout when you run motus profile Can you change some parameters in motus to profile more or less species? (Hint, look here ) Solution Precision is the number of TP out of the total number of detected species. Recall is the number of detected species out of all the species actually present in the sample. To increase precision at the cost of recall you can increase parameters -g (default: 3) and -l (default: 75). motus profile -f sampleA_filterd_1P.fastq -r sampleA_filtered_2P.fastq -s sampleA_filtered_1U.fastq,sampleA_filtered_2U.fastq -n sampleA -o sampleA_profile_high_p.txt -g 8 -l 90 We have detected just 37 species. To increase recall at the cost of having more false positives you can do: motus profile -f sampleA_filterd_1P.fastq -r sampleA_filtered_2P.fastq -s sampleA_filtered_1U.fastq,sampleA_filtered_2U.fastq -n sampleA -o sampleA_profile_high_r.txt -g 1 -l 45 We have detected 331 species. How can you merge different motus profiles into one file? Try to profile and then merge three profiles (Sample A, B and C). Solution After creating the individual tax profiles for all the samples, do: motus merge - i sampleA_profile.txt,sampleB_profile.txt,sampleC_profile.txt -o merged_profiles.txt This results in a tab-separated file containing the tax profiles.","title":"Taxonomic profiling with mOTUs"},{"location":"project3/Step-1-sol/#taxonomic-profiling-with-mapseq","text":"Similar as with mOTUs, first create a profile for each sample (A,B, and C) and then merge them into one (Check the github page for the command). Solution In order to create a taxonomic profile using MAPseq for sampleA do: mapseq sampleA_filtered_1P.fasta > sampleA.mseq MAPseq seems to be a bit faster than mOTUs (took ~2 min to run) sampleA.mseq contains the results from mapping reads to the reference database of OTUs provided by MAPseq (alignment score, database hit, etc) and the taxnomic classifications along with associated confidences. After generating the .mseq files for all the samples, you can merge them into one OTU table like so: mapseq -otutable sampleA.mseq sampleB.mseq sampleC.mseq -ti 1 -tl 3 > mapseq_otutable_otu97.tsv This creates an OTU table containing reads mapped to 97% OTUs for sampleA, sampleB and sampleC. Note that depending on whether -ti is 0 or 1, what -tl means changes. If you have -ti 0 , then -tl indicates the taxonomic level (0 (domain), 1 (phylum), 2 (class), 3 (order), 4 (family), 5 (genus), 6 (species)) . So if -ti 0 -tl 3 means that the OTU table will report only read counts mapping to order-level NCBI taxonomies. If you have -ti 1 , then -tl indicates the OTU clustering level (1 (90% OTU), 2 (96% OTU), 3 (97% OTU), 4 (98%), 5 (99%)) . So if -ti 1 -tl 3 means that the OTU table will report only read counts mapping to 97% OTUs. To obtain reads mapping to 99% OTUs : mapseq -otutable sampleA.mseq sampleB.mseq sampleC.mseq -ti 0 -tl 5 > mapseq_otutable_otu99.tsv If we increase the clustering level to 99%, we observe fewer species detected for all the samples. This might be because at a finer resolution, we might not be able to assign taxonomies too well resulting in a smaller number of species being profiled. To obtain reads mapping to 96% OTUs : mapseq -otutable sampleA.mseq sampleB.mseq sampleC.mseq -ti 0 -tl 2 > mapseq_otutable_otu96.tsv If we decrease the clustering level to 96%, we observe more species detected for all the samples. Can you compare mOTUs and MAPseq profiles? Solution Profiler species detected in sampleA mOTUs 97 MAPseq 97% 173 MAPseq 99% 121 MAPseq 96% 179 It looks like mOTUs is a bit more conservative at detecting species. Note that since mOTUs and OTUs are defined differently, it might not be straightforward to make a direct comparison.","title":"Taxonomic profiling with MAPseq"},{"location":"project3/Step-1-sol/#explore-taxonomic-profiles","text":"Explore the taxonomic profiles ( tax_profile ). Solution Load packages and taxonomic profile into your R environment #packages library ( tidyverse ) #tax profiles load ( url ( \"https://zenodo.org/record/6517497/files/human_microbiome_dataset.Rdata\" )) Let's have a quick peek at our data. dim ( tax_profile ) There are 246 rows and 496 columns. Let's check the content: tax_profile [ 1 : 3 , 1 : 3 ] Which results in: 700002_T0 700002_T1 700002_T2 Blautia 229 1196 1758 Bacteroides 4018 1705 1660 Agathobacter 80 1580 126 The rows are genera and the columns are samples. head ( metadata ) Which is: Subject Timepoint Sex 700002_T0 700002 0 M 700002_T1 700002 12 M 700002_T2 700002 24 M 700002_T3 700002 50 M 700004_T0 700004 0 F 700004_T1 700004 12 F The 496 samples are from 124 patients (sampled 4 times over the course of the year). `tax_profile` contains the read counts of of species (rows) across all samples (columns). For each sample, we know the corresponding subject id, timepoint of sampling and the sex of the subject. Here are some hints of what you can check: How many reads there are per sample? Solution Calculate total number of reads in every sample (also called library size of sample) sample_read_counts <- data.frame ( total_read_counts = colSums ( tax_profile )) How are the sample read counts distributed? ggplot ( data = sample_read_counts ) + geom_histogram ( mapping = aes ( x = total_read_counts ), bins = 60 ) + ylab ( 'Number of samples' ) + xlab ( 'Total number of reads' ) Which results in: Note how variable the total read counts are across all samples. This is a problem because this variation is most likely a result of the sequencing process and not any meaningful biological variation. If you want to compare different samples, is it a problem that there are different read counts? Try to divide each value within a sample by the sum of the reads in that sample to normalise the data (also called relative abundance). Solution We need to remove this technical variation caused by differing total read counts to meaningfully compare samples. We can do this by relative abundance normalization where we divide each value within a sample by the total read counts in that sample. rel_ab = prop.table ( tax_profile , 2 ) Now the abundances of each sample should sum to 1. all ( colSums ( rel_ab ) == 1 ) Which genera are the most and least prevalent? Solution Prevalence of a species refers to the the proportion of samples in which that species is detected. First we calculate prevalence for all the genera. number_of_samples = dim ( tax_profile )[ 2 ] prevalence_df <- data.frame ( Prevalence = rowSums ( rel_ab > 0 ) / number_of_samples , genus = rownames ( tax_profile )) You can plot an histogram of the prevalences: ggplot ( data = prevalence_df ) + geom_histogram ( mapping = aes ( x = Prevalence ), bins = 60 ) You can see that there are many species that apper only few times (on the left), and there are only few species that are present in all samples (on the right). We can also check which genera are the most prevalent: head ( prevalence_df [ order ( prevalence_df $ Prevalence , decreasing = T ),]) Result: Prevalence genus Blautia 1.0000000 Blautia Bacteroides 1.0000000 Bacteroides -1 1.0000000 -1 Faecalibacterium 0.9899194 Faecalibacterium Anaerostipes 0.9879032 Anaerostipes Fusicatenibacter 0.9778226 Fusicatenibacter It means that the genus Blautia and Bacteroides are present in all species. The -1 represents unassigned reads (i.e. that they do not map to any known genus). The least prevalent genera are: Prevalence genus Rosenbergiella 0.002016129 Rosenbergiella 28-4 0.002016129 28-4 Gallicola 0.002016129 Gallicola Sarcina 0.002016129 Sarcina Harryflintia 0.002016129 Harryflintia Paeniclostridium 0.002016129 Paeniclostridium Which are present in only one sample. You can see that the most prevalent species are typically genera that that should be present in all human guts. This type of quick exploration can also serve as a sanity check (is there something we should not be seeing at all?) Is the relative abundance of the different genera normally distributed? Solution If we look at the distribution of all relative abundances with a simple histogram: abundances_df = data.frame ( rel_abundances = as.vector ( rel_ab )) ggplot ( data = abundances_df ) + geom_histogram ( aes ( rel_abundances ), bins = 100 ) + xlab ( \"All relative abundances\" ) We can clearly see that the relative abundances are not normally distributed. Maybe if we log transform the data, the result improve. Note that in order to log transform the data we need to add a small value (in this case 10^-4 ) so that we don\u2019t have the problem of calculating the log of zero. Code: log_rel_ab = data.frame ( rel_abundances = log10 ( as.vector ( rel_ab ) + 10 ^ -4 )) ggplot ( data = log_rel_ab ) + geom_histogram ( aes ( rel_abundances ), bins = 100 ) + xlab ( \"All relative abundances (log10)\" ) + scale_y_log10 () As you can see even when we log transform, the high number of zero makes the distribution not normal. We can check also single genera. Here I select 3 genera: Bacteroides with prevalence of 1, Akkermansia with a prevalence of ~0.5 and Harryflintia with the lowest relative abundance. We can plot them with the following code: df_genera = data.frame ( genus = c ( rep ( \"Bacteroides\" , ncol ( rel_ab )), rep ( \"Akkermansia\" , ncol ( rel_ab )), rep ( \"Harryflintia\" , ncol ( rel_ab ))), rel_ab = c ( rel_ab [ \"Bacteroides\" ,], rel_ab [ \"Akkermansia\" ,], rel_ab [ \"Harryflintia\" ,]) ) ggplot ( data = df_genera ) + geom_histogram ( mapping = aes ( x = rel_ab ), bins = 60 ) + facet_grid ( . ~ genus ) As you can see for Harryflintia there are almost only zeros (only one sample contain this genus). On the other hand Bacteroides can almost have a normal distribution (with a long tail on the right). While Akkermansia shows a tipical microbiome distribution plot with many samples where the measure is zero and then a tail with few samples where the relative abundance is higher. How many zeros there are per sample and per genus? Solution Taxonomic profiles are typically sparse because most species occur in frequently. Do we see this in our data as well? How many 0s are present overall in the data? # we transform the relative abundance in a vector temp = as.vector ( rel_ab ) # and check the length (how many values there are) length ( temp ) # 122016 # and how many values are zero: sum ( temp == 0 ) # 94133 77% of the data are 0s (94133/122016)! If we look at the percentage of 0s per sample: head ( data.frame ( zeros_per_sample = colSums ( rel_ab == 0 ) / ( dim ( rel_ab )[ 1 ]))) Result: zeros_per_sample 700002 _ T0 0.8536585 700002 _ T1 0.7398374 700002 _ T2 0.7520325 700002 _ T3 0.7723577 700004 _ T0 0.7723577 700004 _ T1 0.7723577 We can see that the sparsity is similar across all samples. Do you think that if we had samples from a different environment (like Soil for instance), we might see something different? Percentage of 0s per genus is the same as 1 - prevalence : head ( data.frame ( zeros_per_sample = rowSums ( rel_ab == 0 ) / ( dim ( rel_ab )[ 2 ]))) Result: zeros_per_sample Blautia 0.00000000 Bacteroides 0.00000000 Agathobacter 0.04435484 Faecalibacterium 0.01008065 Bifidobacterium 0.09072581 Fusicatenibacter 0.02217742 Again, we note that some genera are more prevalent than others. How much variability there is within Subject (check the metadata table), compare to between subjects? Or from another perspective, how stable it is the human gut microbiome? Solution 1 There are different ways to explore this problem. We can try to calculate the distance between all possible samples and then compare the distances of samples that come from the same subject and the distance that come from different subjects. For example, you take sample 1 ( 700002_T0 ) and sample 2 ( 700002_T1 ), and since they are from the same subject, we will use the distance between these two samples as an example of within-subject distance. Code: rel_ab = prop.table ( tax_profile , 2 ) log_rel_ab = log10 ( rel_ab + 10 ^ -4 ) n_samples = ncol ( log_rel_ab ) # where to save the result distance = c () type = c () # we go through all the pair of samples and we calculate the distance for ( i in c ( 1 : ( n_samples -1 ) )){ for ( j in c ( ( i +1 ) : n_samples )){ # name of the samples s_i = colnames ( log_rel_ab )[ i ] s_j = colnames ( log_rel_ab )[ j ] # profiles profile_i = log_rel_ab [, i ] profile_j = log_rel_ab [, j ] # calculate the distance as sum of the absolute distance d = sum ( abs ( profile_i - profile_j )) # add the values distance = c ( distance , d ) # add correct type if ( metadata [ s_i , \"Subject\" ] == metadata [ s_j , \"Subject\" ]){ type = c ( type , \"Within distance\" ) } else { type = c ( type , \"Between distance\" ) } } } df = data.frame ( dist = distance , type = type ) ggplot ( df , aes ( type , dist )) + geom_boxplot () It is quite clear that they are different, but we can also test it: wilcox.test ( df [ df $ type == \"Within distance\" ,] $ dist , df [ df $ type == \"Between distance\" ,] $ dist ) Result: Wilcoxon rank sum test with continuity correction data : df [ df $ type == \"Within distance\" , ] $ dist and df [ df $ type == \"Between distance\" , ] $ dist W = 2844084 , p - value < 2.2e-16 alternative hypothesis : true location shift is not equal to 0 This means that samples from the same subject have a low distance, or in other words they are similar to each other. And they are much more similar compared to other subjects, even after 1 year (the last time point is 50 weeks). From this we understand two things: - First, the human microbiome is stable over time - Second, there is a great variability between subjects Solution 2 We can do a PCA plot: rel_ab = prop.table ( tax_profile , 2 ) log_rel_ab = log10 ( rel_ab + 10 ^ -4 ) # remove zero rows log_rel_ab = log_rel_ab [ rowSums ( rel_ab ) > 0 ,] pc <- prcomp ( t ( log_rel_ab ), center = TRUE , scale. = TRUE ) df = data.frame ( pc1 = pc $ x [, 1 ], pc2 = pc $ x [, 2 ], Subject = as.factor ( metadata [ rownames ( pc $ x ), \"Subject\" ]), Timepoint = metadata [ rownames ( pc $ x ), \"Timepoint\" ], Sex = metadata [ rownames ( pc $ x ), \"Sex\" ] ) And plot the result: ggplot ( df , aes ( x = pc1 , y = pc2 , col = Sex )) + geom_point () ggplot ( df , aes ( x = pc1 , y = pc2 , col = Timepoint )) + geom_point () ggplot ( df , aes ( x = pc1 , y = pc2 , col = Subject )) + geom_point () + theme ( legend.position = \"none\" ) There is no particular grouping by sex or by timepoint (as expected). It also seems that the plot based on subject (third plot where each color is a subject) is random. But, we can actually see some structure, in particular outside of the conglomerate of points at the center.","title":"Explore taxonomic profiles"},{"location":"project3/Step-1/","text":"Step 1: Taxonomic profiling of metagenomic samples with mOTUs General note: this guide has been written assuming you use a Mac or Linux Command Line. The presentation can be found here . Download example sequencing data During sequencing, the nucleotide bases in a DNA sample (library) are determined by the sequencer. For each fragment in the library, a sequence is generated, also called a read, which is simply a succession of nucleotides. Sequencing data produced by a short read sequencer like Illumina HiSeq result in two fastq files: forward and reverse. You can download three example fastq files at the following links (within a terminal): wget https://zenodo.org/record/6517497/files/sampleA_1.fastq wget https://zenodo.org/record/6517497/files/sampleA_2.fastq wget https://zenodo.org/record/6517497/files/sampleB_1.fastq wget https://zenodo.org/record/6517497/files/sampleB_2.fastq wget https://zenodo.org/record/6517497/files/sampleC_1.fastq wget https://zenodo.org/record/6517497/files/sampleC_2.fastq Note that if you are using macOS, you need to use curl : curl <link> -o <file name> Example: curl https://zenodo.org/record/6517497/files/sampleA_1.fastq -o sampleA_1.fastq When we work with metagenomic data we usually have two fastq files produced by the Illumina sequencer: - a file containing the forward reads - a file containing the reverse reads Usually the prefix of the file name is the same, and we have _1_ for the file with forward reads and _2_ for the file with reverse reads, example: HYG3LBGXC_261_1_19s00-sample119s004346_1_sequence.fq.gz HYG3LBGXC_261_1_19s00-sample119s004346_2_sequence.fq.gz A fastq file contains 4 lines for each read, with the following information: A line starting with @ and the read id The DNA sequence A line starting with + and sometimes the same information as in line 1 A string of characters that represents the quality score (same number of characters as in line 2) We can have a look at the first read (4 lines) with: head -n 4 sampleA_1.fastq @read98 CATCGACGACCTGGACGACCTGGACTTCATCGAGCGGGTGAAGATCCAGCAGAAGAACTGGATCGGCCGCTCCACCGGTGCCGAGGTCACCTTCAAGGCC + BBBFFFFFFFFFFBBFFFFIIFFFIIIIIIIIFBFIIFFFFFFFBBBFFFFBBFFFFFBBFFFBBBFBBBBFBBFBFFBBFFF0<B7BBFBB<BBFBBBF Each character in the fourth line can be converted to a quality score ( Phred-33 ) from 1 to 40: Character: !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHI | | | | | Quality score: 0........10........20........30........40 And, for each quality score there is an associated probability for correctly calling a base: Quality Score Probability of incorrect base call Base call accuracy 10 1 in 10 90% 20 1 in 100 99% 30 1 in 1000 99.9% 40 1 in 10,000 99.99% Explore the files, in particular you can check: How many reads there are per sample? What is the average length of the reads? Is there a difference between the read lengths in the forward and reverse files? Do you have the same read IDs in the forward and reverse file? Check the quality of the sequencing data Modern sequencing technologies can generate a massive number of sequence reads in a single experiment. However, no sequencing technology is perfect, and each instrument will generate different types and amount of errors, such as incorrect nucleotides being called. These wrongly called bases are due to the technical limitations of each sequencing platform. Therefore, it is necessary to understand, identify and exclude error-types that may impact the interpretation of downstream analysis. Sequence quality control is therefore an essential first step in your analysis. Catching errors early saves time later on. You can evaluate the quality of fastq files with fastQC. For example run: fastqc sampleA_1.fastq Which will produce an html file. You can find more information on the different panels here: link Which part of the reads is of lower quality? Is there any difference between forward or reverse? Filter and trim reads The quality drops in the ends of the sequences we analysed. This could cause bias in downstream analyses with these potentially incorrectly called nucleotides. Sequences must be treated to reduce bias in downstream analysis. Trimming can help to increase the number of reads the aligner or assembler are able to succesfully use, reducing the number of reads that are unmapped or unassembled. In general, quality treatments include: Trimming/cutting: - from low quality score regions - beginning/end of sequence - removing adapters Filtering of sequences - with low mean quality score - too short - with too many ambiguous (N) bases To accomplish this task we will use trimmomatic , a tool that enhances sequence quality by automating adapter trimming as well as quality control. Check their website to get more information on how to run the tool. Note that if you installed trimmomatic with conda, you can run with trimmomatic PE [...] (do not need to specify java -jar trimmomatic-0.39.jar PE [...] ). Try to run trimmomatic (you can use different parameters). How many files did trimmomatic generate? What do they contain? How many reads have been filtered out? Check the quality of the filtered reads, did the quality improve? Taxonomic profiling with mOTUs The majority of microbiome studies rely on an accurate identification of the microbes and quantification their abundance in the sample under study, a process called taxonomic profiling. We would like to save the profile in a file like: Bacteroides_vulgatus 0.34 Prevotella_copri 0.16 Eubacterium_rectale 0.10 ... Where the first column contain the name of the species and the second column contain the relative abundance. And, if we pull together many samples, in a table like: sample_1 sample_2 Bacteroides_vulgatus 0.34 0.01 Prevotella_copri 0.16 0.42 Eubacterium_rectale 0.10 0.00 ... ... ... We will use mOTUs to create taxonomic profiles of metagenomic samples. More information can be found also in this protocol paper . Use motus (manual: link ) to create a profile from the files created by trimmomatic. How many species are detected? How many are reference species and how many are unknown species? Can you change some parameters in motus to profile more or less species? (Hint, look here ) How can you merge different motus profiles into one file? Try to profile and then merge three profiles (Sample A, B and C). Taxonomic profiling with MAPseq There are other taxonomic profiling tools that you can use, one that is already available in the virtual machine is MAPseq . Try to profile the three samples with MAPseq. (Note that MAPseq need a single fasta file as input for each sample, instead of fastq files. You can combinee the forward and reverse fastq files after quality filtering with cat and then convert it into a fasta file.) Files can be converted from fastq format to fasta in multiple ways. For our purpose with a small number of samples it is sufficiently fast to use awk to filter out the first and second lines of each read (4 lines in total). In order to concatenate and convert your files, use the following command within your terminal: cat sampleA_filtered_P1.fastq sampleA_filtered_P2.fastq > sampleA_filtered.fastq cat sampleA_filtered.fastq | awk '{if(NR%4==1) {printf(\">%s\\n\",substr($0,2));} else if(NR%4==2) print;}' > sampleA.fasta By default, mapseq uses a databases which contains both the NCBI Taxonomy as well as internal, hierarchichal OTU ID\u2019s. Thus, your result will contain counts mapped to both of the different taxonomies, as well as different taxonbomic levels. The output should be saved into a .mseq file, which can be investigated by using the -otucounts flag. Here you can see all different taxonomy counts and taxonomic levels printed out after one another. On the leftmost column, you will first see the database used (0 for NCBI or 1 for internal OTUs), and in the second column the taxonomic resultion ( from 1 to 6). mapseq sample.fasta > sample.mseq mapseq -otucounts sample.mseq While running mapseq, you may encounter the following error: !! Mon May 2 14:24:17 2022 [] mapseq.cpp:3614 void load_taxa(const estr&, eseqdb&): loading taxonomy, 14922 sequences not found in sequence database This is due to some chimeras that were filtered out recently, you can ignore the message. Similar as with mOTUs, first create a profile for each sample (A,B, and C) and then merge them into one (Check the githube page for the command). You have two main different parameters when creating the otutables, with which you can create the taxonomy and taxonomic resolution used in the resulting otutable: -ti indicates which taxonomony you will use, 0 is for the NCBI Taxonomy, 1 for the mapseq-OTUs. -tl tells the program which taxonomic level to use. The higher the number, the more fine scale your resolution will become. For example, to get the 97% level OTUs (Gold standard in 16S), use the parameters -ti 1 -tl 3 You can try to play around with the parameters and observe the number of mapped reads, found species etc. in different taxonomies and taxonomic levels. If you use mapseq-OTUs, you can look them up in the interactive webinterface microbeatlas , by searching for the respective identifier in the taxa tab (e.g. 97_3). (Optional): Can you compare mOTUs and MAPseq profiles? Explore human gut taxonomic profiles Metagenomics enables the study of species abundances in complex mixtures of microorganisms and has become a standard methodology for the analysis of the human microbiome. However, species abundance data is inherently noisy and contains high levels of biological and technical variability as well as an excess of zeros due to non-detected species. This makes the statistical analysis challenging. Before moving to the next step, you will examine the properties of microbiome datasets. We will switch now to R to examine 496 human gut taxonomic profiles, from 124 patients (each measure 4 times over a period of 1 year). You can load the data within R with the command: load ( url ( \"https://zenodo.org/record/6517497/files/human_microbiome_dataset.Rdata\" )) Explore the taxonomic profiles ( tax_profile ), here are some hints of what you can check: How many reads there are per sample? If you want to compare different samples, is it a problem that there are different read counts? Try to divide each value within a sample by the sum of the reads in that sample to normalise the data (also called relative abundance). Which genera is the most and least prevalent? Is the relative abundance of the different genera normally distributed? How many zeros there are per sample and per genus? How much variability there is within Subject (check the metadata table), compare to between subjects? Or from another perspective, how stable it is the human gut microbiome?","title":"Step 1"},{"location":"project3/Step-1/#step-1-taxonomic-profiling-of-metagenomic-samples-with-motus","text":"General note: this guide has been written assuming you use a Mac or Linux Command Line. The presentation can be found here .","title":"Step 1: Taxonomic profiling of metagenomic samples with mOTUs"},{"location":"project3/Step-1/#download-example-sequencing-data","text":"During sequencing, the nucleotide bases in a DNA sample (library) are determined by the sequencer. For each fragment in the library, a sequence is generated, also called a read, which is simply a succession of nucleotides. Sequencing data produced by a short read sequencer like Illumina HiSeq result in two fastq files: forward and reverse. You can download three example fastq files at the following links (within a terminal): wget https://zenodo.org/record/6517497/files/sampleA_1.fastq wget https://zenodo.org/record/6517497/files/sampleA_2.fastq wget https://zenodo.org/record/6517497/files/sampleB_1.fastq wget https://zenodo.org/record/6517497/files/sampleB_2.fastq wget https://zenodo.org/record/6517497/files/sampleC_1.fastq wget https://zenodo.org/record/6517497/files/sampleC_2.fastq Note that if you are using macOS, you need to use curl : curl <link> -o <file name> Example: curl https://zenodo.org/record/6517497/files/sampleA_1.fastq -o sampleA_1.fastq When we work with metagenomic data we usually have two fastq files produced by the Illumina sequencer: - a file containing the forward reads - a file containing the reverse reads Usually the prefix of the file name is the same, and we have _1_ for the file with forward reads and _2_ for the file with reverse reads, example: HYG3LBGXC_261_1_19s00-sample119s004346_1_sequence.fq.gz HYG3LBGXC_261_1_19s00-sample119s004346_2_sequence.fq.gz A fastq file contains 4 lines for each read, with the following information: A line starting with @ and the read id The DNA sequence A line starting with + and sometimes the same information as in line 1 A string of characters that represents the quality score (same number of characters as in line 2) We can have a look at the first read (4 lines) with: head -n 4 sampleA_1.fastq @read98 CATCGACGACCTGGACGACCTGGACTTCATCGAGCGGGTGAAGATCCAGCAGAAGAACTGGATCGGCCGCTCCACCGGTGCCGAGGTCACCTTCAAGGCC + BBBFFFFFFFFFFBBFFFFIIFFFIIIIIIIIFBFIIFFFFFFFBBBFFFFBBFFFFFBBFFFBBBFBBBBFBBFBFFBBFFF0<B7BBFBB<BBFBBBF Each character in the fourth line can be converted to a quality score ( Phred-33 ) from 1 to 40: Character: !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHI | | | | | Quality score: 0........10........20........30........40 And, for each quality score there is an associated probability for correctly calling a base: Quality Score Probability of incorrect base call Base call accuracy 10 1 in 10 90% 20 1 in 100 99% 30 1 in 1000 99.9% 40 1 in 10,000 99.99% Explore the files, in particular you can check: How many reads there are per sample? What is the average length of the reads? Is there a difference between the read lengths in the forward and reverse files? Do you have the same read IDs in the forward and reverse file?","title":"Download example sequencing data"},{"location":"project3/Step-1/#check-the-quality-of-the-sequencing-data","text":"Modern sequencing technologies can generate a massive number of sequence reads in a single experiment. However, no sequencing technology is perfect, and each instrument will generate different types and amount of errors, such as incorrect nucleotides being called. These wrongly called bases are due to the technical limitations of each sequencing platform. Therefore, it is necessary to understand, identify and exclude error-types that may impact the interpretation of downstream analysis. Sequence quality control is therefore an essential first step in your analysis. Catching errors early saves time later on. You can evaluate the quality of fastq files with fastQC. For example run: fastqc sampleA_1.fastq Which will produce an html file. You can find more information on the different panels here: link Which part of the reads is of lower quality? Is there any difference between forward or reverse?","title":"Check the quality of the sequencing data"},{"location":"project3/Step-1/#filter-and-trim-reads","text":"The quality drops in the ends of the sequences we analysed. This could cause bias in downstream analyses with these potentially incorrectly called nucleotides. Sequences must be treated to reduce bias in downstream analysis. Trimming can help to increase the number of reads the aligner or assembler are able to succesfully use, reducing the number of reads that are unmapped or unassembled. In general, quality treatments include: Trimming/cutting: - from low quality score regions - beginning/end of sequence - removing adapters Filtering of sequences - with low mean quality score - too short - with too many ambiguous (N) bases To accomplish this task we will use trimmomatic , a tool that enhances sequence quality by automating adapter trimming as well as quality control. Check their website to get more information on how to run the tool. Note that if you installed trimmomatic with conda, you can run with trimmomatic PE [...] (do not need to specify java -jar trimmomatic-0.39.jar PE [...] ). Try to run trimmomatic (you can use different parameters). How many files did trimmomatic generate? What do they contain? How many reads have been filtered out? Check the quality of the filtered reads, did the quality improve?","title":"Filter and trim reads"},{"location":"project3/Step-1/#taxonomic-profiling-with-motus","text":"The majority of microbiome studies rely on an accurate identification of the microbes and quantification their abundance in the sample under study, a process called taxonomic profiling. We would like to save the profile in a file like: Bacteroides_vulgatus 0.34 Prevotella_copri 0.16 Eubacterium_rectale 0.10 ... Where the first column contain the name of the species and the second column contain the relative abundance. And, if we pull together many samples, in a table like: sample_1 sample_2 Bacteroides_vulgatus 0.34 0.01 Prevotella_copri 0.16 0.42 Eubacterium_rectale 0.10 0.00 ... ... ... We will use mOTUs to create taxonomic profiles of metagenomic samples. More information can be found also in this protocol paper . Use motus (manual: link ) to create a profile from the files created by trimmomatic. How many species are detected? How many are reference species and how many are unknown species? Can you change some parameters in motus to profile more or less species? (Hint, look here ) How can you merge different motus profiles into one file? Try to profile and then merge three profiles (Sample A, B and C).","title":"Taxonomic profiling with mOTUs"},{"location":"project3/Step-1/#taxonomic-profiling-with-mapseq","text":"There are other taxonomic profiling tools that you can use, one that is already available in the virtual machine is MAPseq . Try to profile the three samples with MAPseq. (Note that MAPseq need a single fasta file as input for each sample, instead of fastq files. You can combinee the forward and reverse fastq files after quality filtering with cat and then convert it into a fasta file.) Files can be converted from fastq format to fasta in multiple ways. For our purpose with a small number of samples it is sufficiently fast to use awk to filter out the first and second lines of each read (4 lines in total). In order to concatenate and convert your files, use the following command within your terminal: cat sampleA_filtered_P1.fastq sampleA_filtered_P2.fastq > sampleA_filtered.fastq cat sampleA_filtered.fastq | awk '{if(NR%4==1) {printf(\">%s\\n\",substr($0,2));} else if(NR%4==2) print;}' > sampleA.fasta By default, mapseq uses a databases which contains both the NCBI Taxonomy as well as internal, hierarchichal OTU ID\u2019s. Thus, your result will contain counts mapped to both of the different taxonomies, as well as different taxonbomic levels. The output should be saved into a .mseq file, which can be investigated by using the -otucounts flag. Here you can see all different taxonomy counts and taxonomic levels printed out after one another. On the leftmost column, you will first see the database used (0 for NCBI or 1 for internal OTUs), and in the second column the taxonomic resultion ( from 1 to 6). mapseq sample.fasta > sample.mseq mapseq -otucounts sample.mseq While running mapseq, you may encounter the following error: !! Mon May 2 14:24:17 2022 [] mapseq.cpp:3614 void load_taxa(const estr&, eseqdb&): loading taxonomy, 14922 sequences not found in sequence database This is due to some chimeras that were filtered out recently, you can ignore the message. Similar as with mOTUs, first create a profile for each sample (A,B, and C) and then merge them into one (Check the githube page for the command). You have two main different parameters when creating the otutables, with which you can create the taxonomy and taxonomic resolution used in the resulting otutable: -ti indicates which taxonomony you will use, 0 is for the NCBI Taxonomy, 1 for the mapseq-OTUs. -tl tells the program which taxonomic level to use. The higher the number, the more fine scale your resolution will become. For example, to get the 97% level OTUs (Gold standard in 16S), use the parameters -ti 1 -tl 3 You can try to play around with the parameters and observe the number of mapped reads, found species etc. in different taxonomies and taxonomic levels. If you use mapseq-OTUs, you can look them up in the interactive webinterface microbeatlas , by searching for the respective identifier in the taxa tab (e.g. 97_3). (Optional): Can you compare mOTUs and MAPseq profiles?","title":"Taxonomic profiling with MAPseq"},{"location":"project3/Step-1/#explore-human-gut-taxonomic-profiles","text":"Metagenomics enables the study of species abundances in complex mixtures of microorganisms and has become a standard methodology for the analysis of the human microbiome. However, species abundance data is inherently noisy and contains high levels of biological and technical variability as well as an excess of zeros due to non-detected species. This makes the statistical analysis challenging. Before moving to the next step, you will examine the properties of microbiome datasets. We will switch now to R to examine 496 human gut taxonomic profiles, from 124 patients (each measure 4 times over a period of 1 year). You can load the data within R with the command: load ( url ( \"https://zenodo.org/record/6517497/files/human_microbiome_dataset.Rdata\" )) Explore the taxonomic profiles ( tax_profile ), here are some hints of what you can check: How many reads there are per sample? If you want to compare different samples, is it a problem that there are different read counts? Try to divide each value within a sample by the sum of the reads in that sample to normalise the data (also called relative abundance). Which genera is the most and least prevalent? Is the relative abundance of the different genera normally distributed? How many zeros there are per sample and per genus? How much variability there is within Subject (check the metadata table), compare to between subjects? Or from another perspective, how stable it is the human gut microbiome?","title":"Explore human gut taxonomic profiles"},{"location":"project3/Step-2-sol/","text":"Solution Step 2: Comparative Metagenome Analysis with SIAMCAT General note: this guide has been written assuming you use a R. Download the taxonomic profiles and metadata Look at the metadata, how many controls ( CTR ) and cases ( CRC for colorectal cancer) are there? Solution Load the data: load ( url ( \"https://zenodo.org/record/6524317/files/motus_profiles_study1.Rdata\" )) We can check what there is in the metadata with: head ( meta_study1 ) There are many columns, but the one we are interesed in is \"Group\": table ( meta_study1 $ Group ) Which results in: CRC CTR 46 60 There are 46 profiles from diseased patients (CRC) and 60 profiles from healthy individuals. We can check if there is an overall trend in the profiles looking at a PCA plot: rel_ab = prop.table ( motus_study1 ) log_rel_ab = log10 ( rel_ab + 10 ^ -4 ) # remove zero rows log_rel_ab = log_rel_ab [ rowSums ( rel_ab ) > 0 ,] pc <- prcomp ( t ( log_rel_ab ), center = TRUE , scale. = TRUE ) df = data.frame ( pc1 = pc $ x [, 1 ], pc2 = pc $ x [, 2 ], Group = as.factor ( meta_study1 [ rownames ( pc $ x ), \"Group\" ]) ) ggplot ( df , aes ( x = pc1 , y = pc2 , col = Group )) + geom_point () Overall there is not a big shift visible from the PCA. Identify which species show an association to colorectal cancer patients Try to apply a t-test or a Wilcoxon test to your data. Solution Since we observed before that the data is not normally distributed, we can use a Wilcoxon test instead of a t-test. We can test all microbial species for statistically significant differences. In order to do so, we perform a Wilcoxon test on each individual bacterial species. # use the same log transformed data as before rel_ab = prop.table ( motus_study1 , 2 ) log_rel_ab = log10 ( rel_ab + 10 ^ -4 ) # remove zero rows log_rel_ab = log_rel_ab [ rowSums ( rel_ab ) > 0 ,] # we go through each measured species p.vals <- rep_len ( 1 , nrow ( log_rel_ab )) names ( p.vals ) <- rownames ( log_rel_ab ) for ( i in rownames ( log_rel_ab )){ x <- log_rel_ab [ i ,] y <- meta_study1 [ colnames ( log_rel_ab ),] $ Group t <- wilcox.test ( x ~ y ) p.vals [ i ] <- t $ p.value } head ( sort ( p.vals )) Result: Dialister pneumosintes [ ref_mOTU_v3_03630 ] 1.277337e-07 Fusobacterium nucleatum subsp. animalis [ ref_mOTU_v3_01001 ] 1.137605e-06 Olsenella sp. Marseille - P2300 [ ref_mOTU_v3_10001 ] 2.184340e-05 Fusobacterium nucleatum subsp. vincentii [ ref_mOTU_v3_01002 ] 5.576030e-05 Anaerotignum lactatifermentans [ ref_mOTU_v3_02190 ] 8.752588e-05 Fusobacterium nucleatum subsp. nucleatum [ ref_mOTU_v3_01003 ] 1.667614e-04 The species with the most significant effect seems to be Dialister pneumosintes , so let us take a look at the distribution of this species: species <- 'Dialister pneumosintes [ref_mOTU_v3_03630]' df.plot <- data.frame ( log_rel_ab = log_rel_ab [ species ,], group = meta_study1 [ colnames ( log_rel_ab ),] $ Group ) ggplot ( df.plot , aes ( x = group , y = log_rel_ab )) + geom_boxplot ( outlier.shape = NA ) + geom_jitter ( width = 0.08 ) + xlab ( '' ) + ylab ( 'D. pneumosintes rel. ab. (log 10)' ) Try to run SIAMCAT to do association testing. Solution We can also use the SIAMCAT R package to test for differential abundance and produce standard visualizations. library ( \"SIAMCAT\" ) Within SIAMCAT, the data are stored in the SIAMCAT object which contains the feature matrix, the metadata, and information about the groups you want to compare. rel_ab = prop.table ( motus_study1 , 2 ) sc.obj <- siamcat ( feat = rel_ab , meta = meta_study1 , label = 'Group' , case = 'CRC' ) We can use SIAMCAT for feature filtering as well. Currently, the matrix of taxonomic profiles contains 33,571 different bacterial species. Of those, not all will be relevant for our question, since some are present only in a handful of samples (low prevalence) or at extremely low abundance. Therefore, it can make sense to filter your taxonomic profiles before you begin the analysis. Here, we could for example use the maximum species abundance as a filtering criterion. All species that have a relative abundance of at least 1e-03 in at least one of the samples will be kept, the rest is filtered out. sc.obj <- filter.features ( sc.obj , filter.method = 'abundance' , cutoff = 1e-03 ) Additionally we can filter based on the prevalence: sc.obj <- filter.features ( sc.obj , filter.method = 'prevalence' , cutoff = 0.05 , feature.type = 'filtered' ) And we can have a look at the object: sc.obj Result: siamcat - class object label () Label object : 60 CTR and 46 CRC samples filt_feat () Filtered features : 1167 features after abundance , prevalence filtering contains phyloseq - class experiment - level object @ phyloseq : phyloseq @ otu_table () OTU Table : [ 33571 taxa and 106 samples ] phyloseq @ sam_data () Sample Data : [ 106 samples by 12 sample variables ] We go from 33,571 taxa to 1,167 after abundance and prevalence filtering. Now, we can test the filtered feature for differential abundance with SIAMCAT: sc.obj <- check.associations ( sc.obj , detect.lim = 1e-05 ) Build machine learning models to predict colorectal cancer patients from a metagenomic sample Explore the SIAMCAT basic vignette to understand how you can train machine learning models to predict colerectal cancer from metagenomic samples. The SIAMCAT machine learning workflow consists of several steps: Normalization SIAMCAT offers a few normalization approaches that can be useful for subsequent statistical modeling in the sense that they transform features in a way that can increase the accuracy of the resulting models. Importantly, these normalization techniques do not make use of any label information (patient status), and can thus be applied up front to the whole data set (and outside of the following cross validation). sc.obj <- normalize.features ( sc.obj , norm.method = 'log.std' , norm.param = list ( log.n0 = 1e-05 , sd.min.q = 0 )) # Features normalized successfully. sc.obj # siamcat-class object # label() Label object: 60 CTR and 46 CRC samples # filt_feat() Filtered features: 1167 features after abundance, prevalence filtering # associations() Associations: Results from association testing # with 12 significant features at alpha 0.05 # norm_feat() Normalized features: 1167 features normalized using log.std # # contains phyloseq-class experiment-level object @phyloseq: # phyloseq@otu_table() OTU Table: [ 33571 taxa and 106 samples ] # phyloseq@sam_data() Sample Data: [ 106 samples by 12 sample variables ] Cross Validation Split Cross validation is a technique to assess how well an ML model would generalize to external data by partionining the dataset into training and test sets. Here, we split the dataset into 10 parts and then train a model on 9 of these parts and use the left-out part to test the model. The whole process is repeated 10 times. sc.obj <- create.data.split ( sc.obj , num.folds = 10 , num.resample = 10 ) # Features splitted for cross-validation successfully. Model Training Now, we can train a LASSO logistic regression classifier in order to distinguish CRC cases and controls. sc.obj <- train.model ( sc.obj , method = 'lasso' ) # Trained lasso models successfully. Predictions This function will automatically apply the models trained in cross validation to their respective test sets and aggregate the predictions across the whole data set. sc.obj <- make.predictions ( sc.obj ) # Made predictions successfully. Model Evaluation Calling the evaluate.predictions function will result in an assessment of precision and recall as well as in ROC analysis, both of which can be plotted: sc.obj <- evaluate.predictions ( sc.obj ) # Evaluated predictions successfully. model.evaluation.plot ( sc.obj ) ROC: Precision-recall: Model Interpretation Finally, the model.interpretation.plot function will plot characteristics of the models (i.e. model coefficients or feature importance) alongside the input data aiding in understanding how / why the model works (or not). model.interpretation.plot ( sc.obj , consens.thres = 0.7 ) Prediction on External Data Apply the trained model (from study1_species.motus ) on this new dataset and check the model performance on the external dataset. ( Tip : Check out the help for the make.prediction function in SIAMCAT ) There are few steps: Load the data We load the data load ( url ( \"https://zenodo.org/record/6524317/files/motus_profiles_study2.Rdata\" )) And we create relative abundances: rel_ab2 = prop.table ( motus_study2 , 2 ) Make predictions We can now load the second dataset into a SIAMCAT object: test.obj = siamcat ( feat = rel_ab2 , meta = meta_study2 , label = 'Group' , case = 'CRC' ) Now we can use the model that we built before (in sc.obj ) and we can apply it to the test.obj holdout dataset. First, we will make the predictions on the based on the old dataset: test.obj <- make.predictions ( siamcat = sc.obj , siamcat.holdout = test.obj ) # note that the features are normalized with the same parameters for the old dataset # evaluate the prediction test.obj <- evaluate.predictions ( test.obj ) Now, we can compare the performance of the classifier on the original and the holdout dataset by using the model.evaluation.plot function. Here, we can supply several SIAMCAT objects for which the model evaluation will be plotted in the same plot. Note that we can supply the objects as named objects in order to print the names in the legend. model.evaluation.plot ( 'Original' = sc.obj , 'Test' = test.obj , colours = c ( 'dimgrey' , 'orange' ))","title":"Step 2 solutions"},{"location":"project3/Step-2-sol/#solution-step-2-comparative-metagenome-analysis-with-siamcat","text":"General note: this guide has been written assuming you use a R.","title":"Solution Step 2: Comparative Metagenome Analysis with SIAMCAT"},{"location":"project3/Step-2-sol/#download-the-taxonomic-profiles-and-metadata","text":"Look at the metadata, how many controls ( CTR ) and cases ( CRC for colorectal cancer) are there? Solution Load the data: load ( url ( \"https://zenodo.org/record/6524317/files/motus_profiles_study1.Rdata\" )) We can check what there is in the metadata with: head ( meta_study1 ) There are many columns, but the one we are interesed in is \"Group\": table ( meta_study1 $ Group ) Which results in: CRC CTR 46 60 There are 46 profiles from diseased patients (CRC) and 60 profiles from healthy individuals. We can check if there is an overall trend in the profiles looking at a PCA plot: rel_ab = prop.table ( motus_study1 ) log_rel_ab = log10 ( rel_ab + 10 ^ -4 ) # remove zero rows log_rel_ab = log_rel_ab [ rowSums ( rel_ab ) > 0 ,] pc <- prcomp ( t ( log_rel_ab ), center = TRUE , scale. = TRUE ) df = data.frame ( pc1 = pc $ x [, 1 ], pc2 = pc $ x [, 2 ], Group = as.factor ( meta_study1 [ rownames ( pc $ x ), \"Group\" ]) ) ggplot ( df , aes ( x = pc1 , y = pc2 , col = Group )) + geom_point () Overall there is not a big shift visible from the PCA.","title":"Download the taxonomic profiles and metadata"},{"location":"project3/Step-2-sol/#identify-which-species-show-an-association-to-colorectal-cancer-patients","text":"Try to apply a t-test or a Wilcoxon test to your data. Solution Since we observed before that the data is not normally distributed, we can use a Wilcoxon test instead of a t-test. We can test all microbial species for statistically significant differences. In order to do so, we perform a Wilcoxon test on each individual bacterial species. # use the same log transformed data as before rel_ab = prop.table ( motus_study1 , 2 ) log_rel_ab = log10 ( rel_ab + 10 ^ -4 ) # remove zero rows log_rel_ab = log_rel_ab [ rowSums ( rel_ab ) > 0 ,] # we go through each measured species p.vals <- rep_len ( 1 , nrow ( log_rel_ab )) names ( p.vals ) <- rownames ( log_rel_ab ) for ( i in rownames ( log_rel_ab )){ x <- log_rel_ab [ i ,] y <- meta_study1 [ colnames ( log_rel_ab ),] $ Group t <- wilcox.test ( x ~ y ) p.vals [ i ] <- t $ p.value } head ( sort ( p.vals )) Result: Dialister pneumosintes [ ref_mOTU_v3_03630 ] 1.277337e-07 Fusobacterium nucleatum subsp. animalis [ ref_mOTU_v3_01001 ] 1.137605e-06 Olsenella sp. Marseille - P2300 [ ref_mOTU_v3_10001 ] 2.184340e-05 Fusobacterium nucleatum subsp. vincentii [ ref_mOTU_v3_01002 ] 5.576030e-05 Anaerotignum lactatifermentans [ ref_mOTU_v3_02190 ] 8.752588e-05 Fusobacterium nucleatum subsp. nucleatum [ ref_mOTU_v3_01003 ] 1.667614e-04 The species with the most significant effect seems to be Dialister pneumosintes , so let us take a look at the distribution of this species: species <- 'Dialister pneumosintes [ref_mOTU_v3_03630]' df.plot <- data.frame ( log_rel_ab = log_rel_ab [ species ,], group = meta_study1 [ colnames ( log_rel_ab ),] $ Group ) ggplot ( df.plot , aes ( x = group , y = log_rel_ab )) + geom_boxplot ( outlier.shape = NA ) + geom_jitter ( width = 0.08 ) + xlab ( '' ) + ylab ( 'D. pneumosintes rel. ab. (log 10)' ) Try to run SIAMCAT to do association testing. Solution We can also use the SIAMCAT R package to test for differential abundance and produce standard visualizations. library ( \"SIAMCAT\" ) Within SIAMCAT, the data are stored in the SIAMCAT object which contains the feature matrix, the metadata, and information about the groups you want to compare. rel_ab = prop.table ( motus_study1 , 2 ) sc.obj <- siamcat ( feat = rel_ab , meta = meta_study1 , label = 'Group' , case = 'CRC' ) We can use SIAMCAT for feature filtering as well. Currently, the matrix of taxonomic profiles contains 33,571 different bacterial species. Of those, not all will be relevant for our question, since some are present only in a handful of samples (low prevalence) or at extremely low abundance. Therefore, it can make sense to filter your taxonomic profiles before you begin the analysis. Here, we could for example use the maximum species abundance as a filtering criterion. All species that have a relative abundance of at least 1e-03 in at least one of the samples will be kept, the rest is filtered out. sc.obj <- filter.features ( sc.obj , filter.method = 'abundance' , cutoff = 1e-03 ) Additionally we can filter based on the prevalence: sc.obj <- filter.features ( sc.obj , filter.method = 'prevalence' , cutoff = 0.05 , feature.type = 'filtered' ) And we can have a look at the object: sc.obj Result: siamcat - class object label () Label object : 60 CTR and 46 CRC samples filt_feat () Filtered features : 1167 features after abundance , prevalence filtering contains phyloseq - class experiment - level object @ phyloseq : phyloseq @ otu_table () OTU Table : [ 33571 taxa and 106 samples ] phyloseq @ sam_data () Sample Data : [ 106 samples by 12 sample variables ] We go from 33,571 taxa to 1,167 after abundance and prevalence filtering. Now, we can test the filtered feature for differential abundance with SIAMCAT: sc.obj <- check.associations ( sc.obj , detect.lim = 1e-05 )","title":"Identify which species show an association to colorectal cancer patients"},{"location":"project3/Step-2-sol/#build-machine-learning-models-to-predict-colorectal-cancer-patients-from-a-metagenomic-sample","text":"Explore the SIAMCAT basic vignette to understand how you can train machine learning models to predict colerectal cancer from metagenomic samples. The SIAMCAT machine learning workflow consists of several steps: Normalization SIAMCAT offers a few normalization approaches that can be useful for subsequent statistical modeling in the sense that they transform features in a way that can increase the accuracy of the resulting models. Importantly, these normalization techniques do not make use of any label information (patient status), and can thus be applied up front to the whole data set (and outside of the following cross validation). sc.obj <- normalize.features ( sc.obj , norm.method = 'log.std' , norm.param = list ( log.n0 = 1e-05 , sd.min.q = 0 )) # Features normalized successfully. sc.obj # siamcat-class object # label() Label object: 60 CTR and 46 CRC samples # filt_feat() Filtered features: 1167 features after abundance, prevalence filtering # associations() Associations: Results from association testing # with 12 significant features at alpha 0.05 # norm_feat() Normalized features: 1167 features normalized using log.std # # contains phyloseq-class experiment-level object @phyloseq: # phyloseq@otu_table() OTU Table: [ 33571 taxa and 106 samples ] # phyloseq@sam_data() Sample Data: [ 106 samples by 12 sample variables ] Cross Validation Split Cross validation is a technique to assess how well an ML model would generalize to external data by partionining the dataset into training and test sets. Here, we split the dataset into 10 parts and then train a model on 9 of these parts and use the left-out part to test the model. The whole process is repeated 10 times. sc.obj <- create.data.split ( sc.obj , num.folds = 10 , num.resample = 10 ) # Features splitted for cross-validation successfully. Model Training Now, we can train a LASSO logistic regression classifier in order to distinguish CRC cases and controls. sc.obj <- train.model ( sc.obj , method = 'lasso' ) # Trained lasso models successfully. Predictions This function will automatically apply the models trained in cross validation to their respective test sets and aggregate the predictions across the whole data set. sc.obj <- make.predictions ( sc.obj ) # Made predictions successfully. Model Evaluation Calling the evaluate.predictions function will result in an assessment of precision and recall as well as in ROC analysis, both of which can be plotted: sc.obj <- evaluate.predictions ( sc.obj ) # Evaluated predictions successfully. model.evaluation.plot ( sc.obj ) ROC: Precision-recall: Model Interpretation Finally, the model.interpretation.plot function will plot characteristics of the models (i.e. model coefficients or feature importance) alongside the input data aiding in understanding how / why the model works (or not). model.interpretation.plot ( sc.obj , consens.thres = 0.7 )","title":"Build machine learning models to predict colorectal cancer patients from a metagenomic sample"},{"location":"project3/Step-2-sol/#prediction-on-external-data","text":"Apply the trained model (from study1_species.motus ) on this new dataset and check the model performance on the external dataset. ( Tip : Check out the help for the make.prediction function in SIAMCAT ) There are few steps: Load the data We load the data load ( url ( \"https://zenodo.org/record/6524317/files/motus_profiles_study2.Rdata\" )) And we create relative abundances: rel_ab2 = prop.table ( motus_study2 , 2 ) Make predictions We can now load the second dataset into a SIAMCAT object: test.obj = siamcat ( feat = rel_ab2 , meta = meta_study2 , label = 'Group' , case = 'CRC' ) Now we can use the model that we built before (in sc.obj ) and we can apply it to the test.obj holdout dataset. First, we will make the predictions on the based on the old dataset: test.obj <- make.predictions ( siamcat = sc.obj , siamcat.holdout = test.obj ) # note that the features are normalized with the same parameters for the old dataset # evaluate the prediction test.obj <- evaluate.predictions ( test.obj ) Now, we can compare the performance of the classifier on the original and the holdout dataset by using the model.evaluation.plot function. Here, we can supply several SIAMCAT objects for which the model evaluation will be plotted in the same plot. Note that we can supply the objects as named objects in order to print the names in the legend. model.evaluation.plot ( 'Original' = sc.obj , 'Test' = test.obj , colours = c ( 'dimgrey' , 'orange' ))","title":"Prediction on External Data"},{"location":"project3/Step-2/","text":"Step 2: Comparative Metagenome Analysis with SIAMCAT General note: this guide has been written assuming you use a R. In R studio load the necessary libraries: library ( \"tidyverse\" ) # for general data wrangling and plotting library ( \"SIAMCAT\" ) # for statistical and machine learning analyses Here you can find a link for the presentations: - association testing - machine learning . Download the taxonomic profiles and metadata From the previous step you learned how to create taxonomic profiles. Within R you can load the files created with mOTUs with the following command: feat.motus <- \"path/to/motus/merged/table\" tax.profiles <- read.table ( feat.motus , sep = '\\t' , quote = '' , comment.char = '' , skip = 2 , stringsAsFactors = FALSE , check.names = FALSE , row.names = 1 , header = TRUE ) Note that we use skip = 2 tp skip the first two headers, and check.names = F when loading the file because some mOTUs names are confusing for R (they are treated as comments). Here we provide 106 human gut taxonomic profiles in the form of a table where columns are samples and rows are species (or clade in general). You can load it directly with: load ( url ( \"https://zenodo.org/record/6524317/files/motus_profiles_study1.Rdata\" )) Look at the metadata, how many controls ( CTR ) and cases ( CRC for colorectal cancer) are there? Identify which species show an association to colorectal cancer patients Colorectal carcinoma (CRC) is among the three most common cancers with more than 1.2 million new cases and about 600,000 deaths per year worldwide. If CRC is diagnosed early, when it is still localized, the 5-year survival rate is > 80%, but decreases to < 10% for late diagnosis of metastasized cancer. With the data that we just dowloaded, we can check if there is any association between specific bacterial species and CRC patients. How would you study and estimate these associations? How would you identify which species are associated to cancer? Which kind of test can you use? Try to apply a t-test or a Wilcoxon test to your data. Explore how SIAMCAT identify associations between clades and phenotypes: link What kind of normalization and filters there are in SIAMCAT? Try to run SIAMCAT to do association testing. The associations metrics computed by SIAMCAT are stored in the SIAMCAT object and can be extracted by using associations(sc.obj) , if you want to have a closer look at the results for yourself. For example, you can use it to plot a volcano plot of the associations between cancer and controls using the output from SIAMCAT. Build machine learning models to predict colorectal cancer patients from a metagenomic sample Population-wide screening and prevention programs for colorectal cancer are recommended in many countries. Fecal occult blood testing (Hemoccult FOBT) is currently the standard noninvasive screening test. However, because FOBT has limited sensitivity and specificity for CRC and does not reliably detect precancerous lesions, there is an urgent demand for more accurate screening tests to identify patients who should undergo colonoscopy, which is considered the most effective diagnostic method. Here, we we can investigate the potential of fecal microbiota for noninvasive detection of colorectal cancer in several patients. We can model the problem using machine learning. Explore the SIAMCAT basic vignette to understand how you can train machine learning models to predict colerectal cancer from metagenomic samples. Explore other profiling methods We profiled the same samples with MAPseq, you can load it with: load ( url ( \"https://zenodo.org/record/6524317/files/mapseq_profiles_study1.Rdata\" )) You can find two profile tables, one from 97% OTUs and one from 99% OTUs. Do you see a similar signal using different taxonomic profiling tools or different taxonomic levels? Prediction on External Data We provide another dataset from a colorectal cancer metagenomic study. The study population was recruited in Germany, you can find the data under: load ( url ( \"https://zenodo.org/record/6524317/files/motus_profiles_study2.Rdata\" )) Note that the features are the same as the mOTUs species in study 1. Apply the trained model (from motus_profiles_study1.Rdata ) on this new dataset and check the model performance on the external dataset. ( Tip : Check out the help for the make.prediction function in SIAMCAT , or the vignette). We profiled this same dataset also with MAPseq, if you want to check it: load ( url ( \"https://zenodo.org/record/6524317/files/mapseq_profiles_study2.Rdata\" ))","title":"Step 2"},{"location":"project3/Step-2/#step-2-comparative-metagenome-analysis-with-siamcat","text":"General note: this guide has been written assuming you use a R. In R studio load the necessary libraries: library ( \"tidyverse\" ) # for general data wrangling and plotting library ( \"SIAMCAT\" ) # for statistical and machine learning analyses Here you can find a link for the presentations: - association testing - machine learning .","title":"Step 2: Comparative Metagenome Analysis with SIAMCAT"},{"location":"project3/Step-2/#download-the-taxonomic-profiles-and-metadata","text":"From the previous step you learned how to create taxonomic profiles. Within R you can load the files created with mOTUs with the following command: feat.motus <- \"path/to/motus/merged/table\" tax.profiles <- read.table ( feat.motus , sep = '\\t' , quote = '' , comment.char = '' , skip = 2 , stringsAsFactors = FALSE , check.names = FALSE , row.names = 1 , header = TRUE ) Note that we use skip = 2 tp skip the first two headers, and check.names = F when loading the file because some mOTUs names are confusing for R (they are treated as comments). Here we provide 106 human gut taxonomic profiles in the form of a table where columns are samples and rows are species (or clade in general). You can load it directly with: load ( url ( \"https://zenodo.org/record/6524317/files/motus_profiles_study1.Rdata\" )) Look at the metadata, how many controls ( CTR ) and cases ( CRC for colorectal cancer) are there?","title":"Download the taxonomic profiles and metadata"},{"location":"project3/Step-2/#identify-which-species-show-an-association-to-colorectal-cancer-patients","text":"Colorectal carcinoma (CRC) is among the three most common cancers with more than 1.2 million new cases and about 600,000 deaths per year worldwide. If CRC is diagnosed early, when it is still localized, the 5-year survival rate is > 80%, but decreases to < 10% for late diagnosis of metastasized cancer. With the data that we just dowloaded, we can check if there is any association between specific bacterial species and CRC patients. How would you study and estimate these associations? How would you identify which species are associated to cancer? Which kind of test can you use? Try to apply a t-test or a Wilcoxon test to your data. Explore how SIAMCAT identify associations between clades and phenotypes: link What kind of normalization and filters there are in SIAMCAT? Try to run SIAMCAT to do association testing. The associations metrics computed by SIAMCAT are stored in the SIAMCAT object and can be extracted by using associations(sc.obj) , if you want to have a closer look at the results for yourself. For example, you can use it to plot a volcano plot of the associations between cancer and controls using the output from SIAMCAT.","title":"Identify which species show an association to colorectal cancer patients"},{"location":"project3/Step-2/#build-machine-learning-models-to-predict-colorectal-cancer-patients-from-a-metagenomic-sample","text":"Population-wide screening and prevention programs for colorectal cancer are recommended in many countries. Fecal occult blood testing (Hemoccult FOBT) is currently the standard noninvasive screening test. However, because FOBT has limited sensitivity and specificity for CRC and does not reliably detect precancerous lesions, there is an urgent demand for more accurate screening tests to identify patients who should undergo colonoscopy, which is considered the most effective diagnostic method. Here, we we can investigate the potential of fecal microbiota for noninvasive detection of colorectal cancer in several patients. We can model the problem using machine learning. Explore the SIAMCAT basic vignette to understand how you can train machine learning models to predict colerectal cancer from metagenomic samples.","title":"Build machine learning models to predict colorectal cancer patients from a metagenomic sample"},{"location":"project3/Step-2/#explore-other-profiling-methods","text":"We profiled the same samples with MAPseq, you can load it with: load ( url ( \"https://zenodo.org/record/6524317/files/mapseq_profiles_study1.Rdata\" )) You can find two profile tables, one from 97% OTUs and one from 99% OTUs. Do you see a similar signal using different taxonomic profiling tools or different taxonomic levels?","title":"Explore other profiling methods"},{"location":"project3/Step-2/#prediction-on-external-data","text":"We provide another dataset from a colorectal cancer metagenomic study. The study population was recruited in Germany, you can find the data under: load ( url ( \"https://zenodo.org/record/6524317/files/motus_profiles_study2.Rdata\" )) Note that the features are the same as the mOTUs species in study 1. Apply the trained model (from motus_profiles_study1.Rdata ) on this new dataset and check the model performance on the external dataset. ( Tip : Check out the help for the make.prediction function in SIAMCAT , or the vignette). We profiled this same dataset also with MAPseq, if you want to check it: load ( url ( \"https://zenodo.org/record/6524317/files/mapseq_profiles_study2.Rdata\" ))","title":"Prediction on External Data"},{"location":"project3/step-1-sol_a/","text":"Solutions Step 1: Taxonomic profiling of metagenomic samples with mOTUs General note: this guide has been written assuming you use a Mac or Linux Command Line. Download example sequencing data Explore the files, in particular you can check: How many reads there are per sample? Solution 1 Knowing that each read takes up four lines in the fastq file, we can simply count the number of lines with wc -l and divide the result by 4 . The following command does it all in one line. echo $( cat sampleA_1.fastq | wc -l ) /4 | bc Solution 2 We can count the number of lines with @read : grep -c \"@read\" sampleA_1.fastq What is the average length of the reads? Is there a difference between the read lengths in the forward and reverse files? Solution 1 We can first extract only the sequences: grep -A 1 \"@read\" sampleA_1.fastq | grep -v \"\\-\\-\" | grep -v \"read\" > sequences_sampleA_1 With -A 1 we select also 1 row after the match. With grep -v we remove what is not needed. We can now check the length with: cat sequences_sampleA_1 | awk '{print length}' This will print a big list, we can count how many times each length appear: cat sequences_sampleA_1 | awk '{print length}' | sort | uniq -c | sort -n | tail Which produces: 238 96 242 92 258 93 341 97 344 94 346 95 428 98 849 20 1194 99 61306 100 So the majority of the reads have length 100 (61,306 out of 67,926, 90%) Solution 2 To quickly check the average length of the reads in the terminal, do: awk 'NR%4==2{sum+=length($0)}END{print sum/(NR/4)}' sampleA_1.fastq The average read length in the reverse reads seem to be slightly lower in all the samples. Do you have the same read IDs in the forward and reverse file? Solution Since these are paired reads, the read ids should be identical and in the same order. You can check this in the terminal like so: #get list of read ids from the forward and reverse files grep '@read' sampleA_1.fastq > sampleA_ids_1.txt grep '@read' sampleA_2.fastq > sampleA_ids_2.txt #check if they are identical diff -s sampleA_ids_1.txt sampleA_ids_2.txt There are many different ways of performing the same task. If you have done something different and accomplished the same thing, awesome! Check the quality of the sequencing data Which part of the reads is of lower quality? Solution The ends of the reads are typically of lower quality. This is to be expected. The quality of calls typically degrades as the run progresses due to problems in the sequencing chemistry. - Is there any difference between the quality of the forward and reverse reads? Solution Reverse reads are usually of lower quality than forward reads, particularly at the read ends. Again this is due to the way paired end sequencing is performed with the forward orientiation is sequenced first followed by the reverse orientation. Filter and trim reads Try to run trimmomatic (you can use different parameters). Solution Here is an example command: trimmomatic PE sampleA_1.fastq sampleA_2.fastq sampleA_filtered_1P.fastq sampleA_filtered_1U.fastq sampleA_filtered_2P.fastq sampleA_filtered_2U.fastq ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36 Here is a description of the parameters used in this specific command. You may have explored other parameters too. ILLUMINACLIP : specifies the file containing the adapter sequences to trim and additional parameters on how to perform the adapter trimming. TruSeq3-PE.fa (provided by trimmomatic ) contains the Illumina adapter sequences used by HiSeq and MiSeq machines. LEADING : Remove low quality bases (lower than 3 ) from the beginning of the reads. TRAILING : Remove low quality bases (lower than 3 ) from the ends of the reads. SLIDINGWINDOW : consider a window of bases (here 4 at once) and trim once the average quality within the window falls below a threshold quality (here 15 ). MINLEN : remove reads lower than the specified min length (here 36 ) How many files did trimmomatic generate? What do they contain? Solution 4 files are produced sampleA_filtered_1P, containing the forward reads that pass the filter and have a mate (in filtered_2P); sampleA_filtered_1U, containing the forward reads that pass the filter and do not have a mate (the paired reverse read didn\u2019t pass the filter) sampleA_filtered_2P, containing the reverse reads that pass the filter and have a mate (in filtered_1P); sampleA_filtered_2U, containing the reverse reads that pass the filter and do not have a mate (the paired forward read didn\u2019t pass the filter) - How many reads have been filtered out? Solution 866 reads (1.27%) of all reads were filtered out from sampleA using the above parameters. Check the quality of the filtered reads. Did the quality improve? Solution You can check the quality of the filterd reads again with fastqc. fastqc sampleA_filtered_1P.fastq fastqc sampleA_filtered_2P.fastq The quality of reads (particularly of the reverse reads) has improved! Taxonomic profiling with mOTUs Use motus (manual: link ) to create a profile from the files created by trimmomatic. Solution Here is the mOTU command to generate a taxonomic profile using default parameters. motus profile -f sampleA_filterd_1P.fastq -r sampleA_filtered_2P.fastq -s sampleA_filtered_1U.fastq,sampleA_filtered_2U.fastq -n sampleA -o sampleA_profile.txt - How many species are detected? How many are reference species and how many are unknown species? Solution You can quickly check how many species were detected with: #this also counts unassigned so subtract 1 from the result grep -c -v '0.0000000000\\|#' sampleA_profile.txt 97 species were dectected. Around 3.4 % were unassigned . You can check how many ref-mOTUs were detected using these command: grep -v '0.0000000000\\|#' sampleA_profile.txt > sampleA_profile_detected.txt grep -c 'ref_mOTU_v3_' sampleA_profile_detected.txt 39 ref-mOTUs were detected in sampleA. Note that this number is also reported as stdout when you run motus profile Can you change some parameters in motus to profile more or less species? (Hint, look here ) Solution Precision is the number of TP out of the total number of detected species. Recall is the number of detected species out of all the species actually present in the sample. To increase precision at the cost of recall you can increase parameters -g (default: 3) and -l (default: 75). motus profile -f sampleA_filterd_1P.fastq -r sampleA_filtered_2P.fastq -s sampleA_filtered_1U.fastq,sampleA_filtered_2U.fastq -n sampleA -o sampleA_profile_high_p.txt -g 8 -l 90 We have detected just 37 species. To increase recall at the cost of having more false positives you can do: motus profile -f sampleA_filterd_1P.fastq -r sampleA_filtered_2P.fastq -s sampleA_filtered_1U.fastq,sampleA_filtered_2U.fastq -n sampleA -o sampleA_profile_high_r.txt -g 1 -l 45 We have detected 331 species. How can you merge different motus profiles into one file? Try to profile and then merge three profiles (Sample A, B and C). Solution After creating the individual tax profiles for all the samples, do: motus merge - i sampleA_profile.txt,sampleB_profile.txt,sampleC_profile.txt -o merged_profiles.txt This results in a tab-separated file containing the tax profiles. Taxonomic profiling with MAPseq Similar as with mOTUs, first create a profile for each sample (A,B, and C) and then merge them into one (Check the github page for the command). Solution In order to create a taxonomic profile using MAPseq for sampleA do: mapseq sampleA_filtered_1P.fasta > sampleA.mseq MAPseq seems to be a bit faster than mOTUs (took ~2 min to run) sampleA.mseq contains the results from mapping reads to the reference database of OTUs provided by MAPseq (alignment score, database hit, etc) and the taxnomic classifications along with associated confidences. After generating the .mseq files for all the samples, you can merge them into one OTU table like so: mapseq -otutable sampleA.mseq sampleB.mseq sampleC.mseq -ti 1 -tl 3 > mapseq_otutable_otu97.tsv This creates an OTU table containing reads mapped to 97% OTUs for sampleA, sampleB and sampleC. Note that depending on whether -ti is 0 or 1, what -tl means changes. If you have -ti 0 , then -tl indicates the taxonomic level (0 (domain), 1 (phylum), 2 (class), 3 (order), 4 (family), 5 (genus), 6 (species)) . So if -ti 0 -tl 3 means that the OTU table will report only read counts mapping to order-level NCBI taxonomies. If you have -ti 1 , then -tl indicates the OTU clustering level (1 (90% OTU), 2 (96% OTU), 3 (97% OTU), 4 (98%), 5 (99%)) . So if -ti 1 -tl 3 means that the OTU table will report only read counts mapping to 97% OTUs. To obtain reads mapping to 99% OTUs : mapseq -otutable sampleA.mseq sampleB.mseq sampleC.mseq -ti 0 -tl 5 > mapseq_otutable_otu99.tsv If we increase the clustering level to 99%, we observe fewer species detected for all the samples. This might be because at a finer resolution, we might not be able to assign taxonomies too well resulting in a smaller number of species being profiled. To obtain reads mapping to 96% OTUs : mapseq -otutable sampleA.mseq sampleB.mseq sampleC.mseq -ti 0 -tl 2 > mapseq_otutable_otu96.tsv If we decrease the clustering level to 96%, we observe more species detected for all the samples. Can you compare mOTUs and MAPseq profiles? Solution Profiler species detected in sampleA mOTUs 97 MAPseq 97% 173 MAPseq 99% 121 MAPseq 96% 179 It looks like mOTUs is a bit more conservative at detecting species. Note that since mOTUs and OTUs are defined differently, it might not be straightforward to make a direct comparison.","title":"Solutions Step 1: Taxonomic profiling of metagenomic samples with mOTUs"},{"location":"project3/step-1-sol_a/#solutions-step-1-taxonomic-profiling-of-metagenomic-samples-with-motus","text":"General note: this guide has been written assuming you use a Mac or Linux Command Line.","title":"Solutions Step 1: Taxonomic profiling of metagenomic samples with mOTUs"},{"location":"project3/step-1-sol_a/#download-example-sequencing-data","text":"Explore the files, in particular you can check: How many reads there are per sample? Solution 1 Knowing that each read takes up four lines in the fastq file, we can simply count the number of lines with wc -l and divide the result by 4 . The following command does it all in one line. echo $( cat sampleA_1.fastq | wc -l ) /4 | bc Solution 2 We can count the number of lines with @read : grep -c \"@read\" sampleA_1.fastq What is the average length of the reads? Is there a difference between the read lengths in the forward and reverse files? Solution 1 We can first extract only the sequences: grep -A 1 \"@read\" sampleA_1.fastq | grep -v \"\\-\\-\" | grep -v \"read\" > sequences_sampleA_1 With -A 1 we select also 1 row after the match. With grep -v we remove what is not needed. We can now check the length with: cat sequences_sampleA_1 | awk '{print length}' This will print a big list, we can count how many times each length appear: cat sequences_sampleA_1 | awk '{print length}' | sort | uniq -c | sort -n | tail Which produces: 238 96 242 92 258 93 341 97 344 94 346 95 428 98 849 20 1194 99 61306 100 So the majority of the reads have length 100 (61,306 out of 67,926, 90%) Solution 2 To quickly check the average length of the reads in the terminal, do: awk 'NR%4==2{sum+=length($0)}END{print sum/(NR/4)}' sampleA_1.fastq The average read length in the reverse reads seem to be slightly lower in all the samples. Do you have the same read IDs in the forward and reverse file? Solution Since these are paired reads, the read ids should be identical and in the same order. You can check this in the terminal like so: #get list of read ids from the forward and reverse files grep '@read' sampleA_1.fastq > sampleA_ids_1.txt grep '@read' sampleA_2.fastq > sampleA_ids_2.txt #check if they are identical diff -s sampleA_ids_1.txt sampleA_ids_2.txt There are many different ways of performing the same task. If you have done something different and accomplished the same thing, awesome!","title":"Download example sequencing data"},{"location":"project3/step-1-sol_a/#check-the-quality-of-the-sequencing-data","text":"Which part of the reads is of lower quality? Solution The ends of the reads are typically of lower quality. This is to be expected. The quality of calls typically degrades as the run progresses due to problems in the sequencing chemistry. - Is there any difference between the quality of the forward and reverse reads? Solution Reverse reads are usually of lower quality than forward reads, particularly at the read ends. Again this is due to the way paired end sequencing is performed with the forward orientiation is sequenced first followed by the reverse orientation.","title":"Check the quality of the sequencing data"},{"location":"project3/step-1-sol_a/#filter-and-trim-reads","text":"Try to run trimmomatic (you can use different parameters). Solution Here is an example command: trimmomatic PE sampleA_1.fastq sampleA_2.fastq sampleA_filtered_1P.fastq sampleA_filtered_1U.fastq sampleA_filtered_2P.fastq sampleA_filtered_2U.fastq ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36 Here is a description of the parameters used in this specific command. You may have explored other parameters too. ILLUMINACLIP : specifies the file containing the adapter sequences to trim and additional parameters on how to perform the adapter trimming. TruSeq3-PE.fa (provided by trimmomatic ) contains the Illumina adapter sequences used by HiSeq and MiSeq machines. LEADING : Remove low quality bases (lower than 3 ) from the beginning of the reads. TRAILING : Remove low quality bases (lower than 3 ) from the ends of the reads. SLIDINGWINDOW : consider a window of bases (here 4 at once) and trim once the average quality within the window falls below a threshold quality (here 15 ). MINLEN : remove reads lower than the specified min length (here 36 ) How many files did trimmomatic generate? What do they contain? Solution 4 files are produced sampleA_filtered_1P, containing the forward reads that pass the filter and have a mate (in filtered_2P); sampleA_filtered_1U, containing the forward reads that pass the filter and do not have a mate (the paired reverse read didn\u2019t pass the filter) sampleA_filtered_2P, containing the reverse reads that pass the filter and have a mate (in filtered_1P); sampleA_filtered_2U, containing the reverse reads that pass the filter and do not have a mate (the paired forward read didn\u2019t pass the filter) - How many reads have been filtered out? Solution 866 reads (1.27%) of all reads were filtered out from sampleA using the above parameters. Check the quality of the filtered reads. Did the quality improve? Solution You can check the quality of the filterd reads again with fastqc. fastqc sampleA_filtered_1P.fastq fastqc sampleA_filtered_2P.fastq The quality of reads (particularly of the reverse reads) has improved!","title":"Filter and trim reads"},{"location":"project3/step-1-sol_a/#taxonomic-profiling-with-motus","text":"Use motus (manual: link ) to create a profile from the files created by trimmomatic. Solution Here is the mOTU command to generate a taxonomic profile using default parameters. motus profile -f sampleA_filterd_1P.fastq -r sampleA_filtered_2P.fastq -s sampleA_filtered_1U.fastq,sampleA_filtered_2U.fastq -n sampleA -o sampleA_profile.txt - How many species are detected? How many are reference species and how many are unknown species? Solution You can quickly check how many species were detected with: #this also counts unassigned so subtract 1 from the result grep -c -v '0.0000000000\\|#' sampleA_profile.txt 97 species were dectected. Around 3.4 % were unassigned . You can check how many ref-mOTUs were detected using these command: grep -v '0.0000000000\\|#' sampleA_profile.txt > sampleA_profile_detected.txt grep -c 'ref_mOTU_v3_' sampleA_profile_detected.txt 39 ref-mOTUs were detected in sampleA. Note that this number is also reported as stdout when you run motus profile Can you change some parameters in motus to profile more or less species? (Hint, look here ) Solution Precision is the number of TP out of the total number of detected species. Recall is the number of detected species out of all the species actually present in the sample. To increase precision at the cost of recall you can increase parameters -g (default: 3) and -l (default: 75). motus profile -f sampleA_filterd_1P.fastq -r sampleA_filtered_2P.fastq -s sampleA_filtered_1U.fastq,sampleA_filtered_2U.fastq -n sampleA -o sampleA_profile_high_p.txt -g 8 -l 90 We have detected just 37 species. To increase recall at the cost of having more false positives you can do: motus profile -f sampleA_filterd_1P.fastq -r sampleA_filtered_2P.fastq -s sampleA_filtered_1U.fastq,sampleA_filtered_2U.fastq -n sampleA -o sampleA_profile_high_r.txt -g 1 -l 45 We have detected 331 species. How can you merge different motus profiles into one file? Try to profile and then merge three profiles (Sample A, B and C). Solution After creating the individual tax profiles for all the samples, do: motus merge - i sampleA_profile.txt,sampleB_profile.txt,sampleC_profile.txt -o merged_profiles.txt This results in a tab-separated file containing the tax profiles.","title":"Taxonomic profiling with mOTUs"},{"location":"project3/step-1-sol_a/#taxonomic-profiling-with-mapseq","text":"Similar as with mOTUs, first create a profile for each sample (A,B, and C) and then merge them into one (Check the github page for the command). Solution In order to create a taxonomic profile using MAPseq for sampleA do: mapseq sampleA_filtered_1P.fasta > sampleA.mseq MAPseq seems to be a bit faster than mOTUs (took ~2 min to run) sampleA.mseq contains the results from mapping reads to the reference database of OTUs provided by MAPseq (alignment score, database hit, etc) and the taxnomic classifications along with associated confidences. After generating the .mseq files for all the samples, you can merge them into one OTU table like so: mapseq -otutable sampleA.mseq sampleB.mseq sampleC.mseq -ti 1 -tl 3 > mapseq_otutable_otu97.tsv This creates an OTU table containing reads mapped to 97% OTUs for sampleA, sampleB and sampleC. Note that depending on whether -ti is 0 or 1, what -tl means changes. If you have -ti 0 , then -tl indicates the taxonomic level (0 (domain), 1 (phylum), 2 (class), 3 (order), 4 (family), 5 (genus), 6 (species)) . So if -ti 0 -tl 3 means that the OTU table will report only read counts mapping to order-level NCBI taxonomies. If you have -ti 1 , then -tl indicates the OTU clustering level (1 (90% OTU), 2 (96% OTU), 3 (97% OTU), 4 (98%), 5 (99%)) . So if -ti 1 -tl 3 means that the OTU table will report only read counts mapping to 97% OTUs. To obtain reads mapping to 99% OTUs : mapseq -otutable sampleA.mseq sampleB.mseq sampleC.mseq -ti 0 -tl 5 > mapseq_otutable_otu99.tsv If we increase the clustering level to 99%, we observe fewer species detected for all the samples. This might be because at a finer resolution, we might not be able to assign taxonomies too well resulting in a smaller number of species being profiled. To obtain reads mapping to 96% OTUs : mapseq -otutable sampleA.mseq sampleB.mseq sampleC.mseq -ti 0 -tl 2 > mapseq_otutable_otu96.tsv If we decrease the clustering level to 96%, we observe more species detected for all the samples. Can you compare mOTUs and MAPseq profiles? Solution Profiler species detected in sampleA mOTUs 97 MAPseq 97% 173 MAPseq 99% 121 MAPseq 96% 179 It looks like mOTUs is a bit more conservative at detecting species. Note that since mOTUs and OTUs are defined differently, it might not be straightforward to make a direct comparison.","title":"Taxonomic profiling with MAPseq"},{"location":"project3/step2-sol-a/","text":"Solution Step 2: Comparative Metagenome Analysis with SIAMCAT General note: this guide has been written assuming you use a R. Download the taxonomic profiles and metadata Look at the metadata, how many controls ( CTR ) and cases ( CRC for colorectal cancer) are there? Solution Load the data: load ( url ( \"https://zenodo.org/record/6524317/files/motus_profiles_study1.Rdata\" )) We can check what there is in the metadata with: head ( meta_study1 ) There are many columns, but the one we are interesed in is \"Group\": table ( meta_study1 $ Group ) Which results in: CRC CTR 46 60 There are 46 profiles from diseased patients (CRC) and 60 profiles from healthy individuals. We can check if there is an overall trend in the profiles looking at a PCA plot: rel_ab = prop.table ( motus_study1 ) log_rel_ab = log10 ( rel_ab + 10 ^ -4 ) # remove zero rows log_rel_ab = log_rel_ab [ rowSums ( rel_ab ) > 0 ,] pc <- prcomp ( t ( log_rel_ab ), center = TRUE , scale. = TRUE ) df = data.frame ( pc1 = pc $ x [, 1 ], pc2 = pc $ x [, 2 ], Group = as.factor ( meta_study1 [ rownames ( pc $ x ), \"Group\" ]) ) ggplot ( df , aes ( x = pc1 , y = pc2 , col = Group )) + geom_point () Overall there is not a big shift visible from the PCA. Identify which species show an association to colorectal cancer patients Try to apply a t-test or a Wilcoxon test to your data. Solution Since we observed before that the data is not normally distributed, we can use a Wilcoxon test instead of a t-test. We can test all microbial species for statistically significant differences. In order to do so, we perform a Wilcoxon test on each individual bacterial species. # use the same log transformed data as before rel_ab = prop.table ( motus_study1 , 2 ) log_rel_ab = log10 ( rel_ab + 10 ^ -4 ) # remove zero rows log_rel_ab = log_rel_ab [ rowSums ( rel_ab ) > 0 ,] # we go through each measured species p.vals <- rep_len ( 1 , nrow ( log_rel_ab )) names ( p.vals ) <- rownames ( log_rel_ab ) for ( i in rownames ( log_rel_ab )){ x <- log_rel_ab [ i ,] y <- meta_study1 [ colnames ( log_rel_ab ),] $ Group t <- wilcox.test ( x ~ y ) p.vals [ i ] <- t $ p.value } head ( sort ( p.vals )) Result: Dialister pneumosintes [ ref_mOTU_v3_03630 ] 1.277337e-07 Fusobacterium nucleatum subsp. animalis [ ref_mOTU_v3_01001 ] 1.137605e-06 Olsenella sp. Marseille - P2300 [ ref_mOTU_v3_10001 ] 2.184340e-05 Fusobacterium nucleatum subsp. vincentii [ ref_mOTU_v3_01002 ] 5.576030e-05 Anaerotignum lactatifermentans [ ref_mOTU_v3_02190 ] 8.752588e-05 Fusobacterium nucleatum subsp. nucleatum [ ref_mOTU_v3_01003 ] 1.667614e-04 The species with the most significant effect seems to be Dialister pneumosintes , so let us take a look at the distribution of this species: species <- 'Dialister pneumosintes [ref_mOTU_v3_03630]' df.plot <- data.frame ( log_rel_ab = log_rel_ab [ species ,], group = meta_study1 [ colnames ( log_rel_ab ),] $ Group ) ggplot ( df.plot , aes ( x = group , y = log_rel_ab )) + geom_boxplot ( outlier.shape = NA ) + geom_jitter ( width = 0.08 ) + xlab ( '' ) + ylab ( 'D. pneumosintes rel. ab. (log 10)' ) Try to run SIAMCAT to do association testing. Solution We can also use the SIAMCAT R package to test for differential abundance and produce standard visualizations. library ( \"SIAMCAT\" ) Within SIAMCAT, the data are stored in the SIAMCAT object which contains the feature matrix, the metadata, and information about the groups you want to compare. rel_ab = prop.table ( motus_study1 , 2 ) sc.obj <- siamcat ( feat = rel_ab , meta = meta_study1 , label = 'Group' , case = 'CRC' ) We can use SIAMCAT for feature filtering as well. Currently, the matrix of taxonomic profiles contains 33,571 different bacterial species. Of those, not all will be relevant for our question, since some are present only in a handful of samples (low prevalence) or at extremely low abundance. Therefore, it can make sense to filter your taxonomic profiles before you begin the analysis. Here, we could for example use the maximum species abundance as a filtering criterion. All species that have a relative abundance of at least 1e-03 in at least one of the samples will be kept, the rest is filtered out. sc.obj <- filter.features ( sc.obj , filter.method = 'abundance' , cutoff = 1e-03 ) Additionally we can filter based on the prevalence: sc.obj <- filter.features ( sc.obj , filter.method = 'prevalence' , cutoff = 0.05 , feature.type = 'filtered' ) And we can have a look at the object: sc.obj Result: siamcat - class object label () Label object : 60 CTR and 46 CRC samples filt_feat () Filtered features : 1167 features after abundance , prevalence filtering contains phyloseq - class experiment - level object @ phyloseq : phyloseq @ otu_table () OTU Table : [ 33571 taxa and 106 samples ] phyloseq @ sam_data () Sample Data : [ 106 samples by 12 sample variables ] We go from 33,571 taxa to 1,167 after abundance and prevalence filtering. Now, we can test the filtered feature for differential abundance with SIAMCAT: sc.obj <- check.associations ( sc.obj , detect.lim = 1e-05 )","title":"Solution Step 2: Comparative Metagenome Analysis with SIAMCAT"},{"location":"project3/step2-sol-a/#solution-step-2-comparative-metagenome-analysis-with-siamcat","text":"General note: this guide has been written assuming you use a R.","title":"Solution Step 2: Comparative Metagenome Analysis with SIAMCAT"},{"location":"project3/step2-sol-a/#download-the-taxonomic-profiles-and-metadata","text":"Look at the metadata, how many controls ( CTR ) and cases ( CRC for colorectal cancer) are there? Solution Load the data: load ( url ( \"https://zenodo.org/record/6524317/files/motus_profiles_study1.Rdata\" )) We can check what there is in the metadata with: head ( meta_study1 ) There are many columns, but the one we are interesed in is \"Group\": table ( meta_study1 $ Group ) Which results in: CRC CTR 46 60 There are 46 profiles from diseased patients (CRC) and 60 profiles from healthy individuals. We can check if there is an overall trend in the profiles looking at a PCA plot: rel_ab = prop.table ( motus_study1 ) log_rel_ab = log10 ( rel_ab + 10 ^ -4 ) # remove zero rows log_rel_ab = log_rel_ab [ rowSums ( rel_ab ) > 0 ,] pc <- prcomp ( t ( log_rel_ab ), center = TRUE , scale. = TRUE ) df = data.frame ( pc1 = pc $ x [, 1 ], pc2 = pc $ x [, 2 ], Group = as.factor ( meta_study1 [ rownames ( pc $ x ), \"Group\" ]) ) ggplot ( df , aes ( x = pc1 , y = pc2 , col = Group )) + geom_point () Overall there is not a big shift visible from the PCA.","title":"Download the taxonomic profiles and metadata"},{"location":"project3/step2-sol-a/#identify-which-species-show-an-association-to-colorectal-cancer-patients","text":"Try to apply a t-test or a Wilcoxon test to your data. Solution Since we observed before that the data is not normally distributed, we can use a Wilcoxon test instead of a t-test. We can test all microbial species for statistically significant differences. In order to do so, we perform a Wilcoxon test on each individual bacterial species. # use the same log transformed data as before rel_ab = prop.table ( motus_study1 , 2 ) log_rel_ab = log10 ( rel_ab + 10 ^ -4 ) # remove zero rows log_rel_ab = log_rel_ab [ rowSums ( rel_ab ) > 0 ,] # we go through each measured species p.vals <- rep_len ( 1 , nrow ( log_rel_ab )) names ( p.vals ) <- rownames ( log_rel_ab ) for ( i in rownames ( log_rel_ab )){ x <- log_rel_ab [ i ,] y <- meta_study1 [ colnames ( log_rel_ab ),] $ Group t <- wilcox.test ( x ~ y ) p.vals [ i ] <- t $ p.value } head ( sort ( p.vals )) Result: Dialister pneumosintes [ ref_mOTU_v3_03630 ] 1.277337e-07 Fusobacterium nucleatum subsp. animalis [ ref_mOTU_v3_01001 ] 1.137605e-06 Olsenella sp. Marseille - P2300 [ ref_mOTU_v3_10001 ] 2.184340e-05 Fusobacterium nucleatum subsp. vincentii [ ref_mOTU_v3_01002 ] 5.576030e-05 Anaerotignum lactatifermentans [ ref_mOTU_v3_02190 ] 8.752588e-05 Fusobacterium nucleatum subsp. nucleatum [ ref_mOTU_v3_01003 ] 1.667614e-04 The species with the most significant effect seems to be Dialister pneumosintes , so let us take a look at the distribution of this species: species <- 'Dialister pneumosintes [ref_mOTU_v3_03630]' df.plot <- data.frame ( log_rel_ab = log_rel_ab [ species ,], group = meta_study1 [ colnames ( log_rel_ab ),] $ Group ) ggplot ( df.plot , aes ( x = group , y = log_rel_ab )) + geom_boxplot ( outlier.shape = NA ) + geom_jitter ( width = 0.08 ) + xlab ( '' ) + ylab ( 'D. pneumosintes rel. ab. (log 10)' ) Try to run SIAMCAT to do association testing. Solution We can also use the SIAMCAT R package to test for differential abundance and produce standard visualizations. library ( \"SIAMCAT\" ) Within SIAMCAT, the data are stored in the SIAMCAT object which contains the feature matrix, the metadata, and information about the groups you want to compare. rel_ab = prop.table ( motus_study1 , 2 ) sc.obj <- siamcat ( feat = rel_ab , meta = meta_study1 , label = 'Group' , case = 'CRC' ) We can use SIAMCAT for feature filtering as well. Currently, the matrix of taxonomic profiles contains 33,571 different bacterial species. Of those, not all will be relevant for our question, since some are present only in a handful of samples (low prevalence) or at extremely low abundance. Therefore, it can make sense to filter your taxonomic profiles before you begin the analysis. Here, we could for example use the maximum species abundance as a filtering criterion. All species that have a relative abundance of at least 1e-03 in at least one of the samples will be kept, the rest is filtered out. sc.obj <- filter.features ( sc.obj , filter.method = 'abundance' , cutoff = 1e-03 ) Additionally we can filter based on the prevalence: sc.obj <- filter.features ( sc.obj , filter.method = 'prevalence' , cutoff = 0.05 , feature.type = 'filtered' ) And we can have a look at the object: sc.obj Result: siamcat - class object label () Label object : 60 CTR and 46 CRC samples filt_feat () Filtered features : 1167 features after abundance , prevalence filtering contains phyloseq - class experiment - level object @ phyloseq : phyloseq @ otu_table () OTU Table : [ 33571 taxa and 106 samples ] phyloseq @ sam_data () Sample Data : [ 106 samples by 12 sample variables ] We go from 33,571 taxa to 1,167 after abundance and prevalence filtering. Now, we can test the filtered feature for differential abundance with SIAMCAT: sc.obj <- check.associations ( sc.obj , detect.lim = 1e-05 )","title":"Identify which species show an association to colorectal cancer patients"},{"location":"project4/project4a/","text":"Project 4A: Diauxic growth in bacteria Project assignment Develop a coarse-grained model that (i) includes a representation of known regulatory mechanisms underlying carbon catabolite repression (CCR) and (ii) reproduces quantitative data and qualitative observations on diauxic growth. Biological question to be answered Which mechanisms are necessary for bringing about diauxic growth? Known regulatory mechanisms to be included: enzyme induction, inducer exclusion, and global regulation of gene expression by metabolic signal (cAMP). Quantitative data and qualitative observations to account for: Curves of glucose-lactose diauxie in standard batch conditions with glucose and lactose; Curves of glucose-lactose diauxie in a batch experiment with a glucose pulse and a batch experiment with a high starting concentration of lactose enzymes; No growth in the absence of glucose and lactose. Model structure Environment described by concentrations of biomass ( B , gDW/L), preferred substrate ( G , mmol/L) and secondary substrate ( L , mmol/L) in the bioreactor of constant volume. \\[\\frac{dG}{dt} = -v_G B,\\] \\[\\frac{dL}{dt} = -v_L B,\\] \\[\\frac{dB}{dt} = \\mu B,\\] The growth rate is denoted by \\(\\mu\\) (1/h), while \\(v_G\\) and \\(v_L\\) (mmol/(gDW L)) correspond to the uptake rates of glucose and lactose, respectively. In the project, we are going to build a model of the growing bacterial population in this environment. Available data Csv files of the three experiments described above. The data originates from K. Bettenbrock et al. A quantitative approach to catabolite repression in Escherichia coli. J Biol Chem. 2006;281:2578\u201384. Additional comments: In the pulse experiment, the glucose pulse occurs at 5 h and LacZ activity is a proxy for the LacZ concentration; In the batch experiment with a high starting concentration of lactose enzymes, glucose is depleted at 3 h and LacZ activity is a proxy for the LacZ concentration.","title":"Project 4A"},{"location":"project4/project4a/#project-4a-diauxic-growth-in-bacteria","text":"","title":"Project 4A: Diauxic growth in bacteria"},{"location":"project4/project4a/#project-assignment","text":"Develop a coarse-grained model that (i) includes a representation of known regulatory mechanisms underlying carbon catabolite repression (CCR) and (ii) reproduces quantitative data and qualitative observations on diauxic growth.","title":"Project assignment"},{"location":"project4/project4a/#biological-question-to-be-answered","text":"Which mechanisms are necessary for bringing about diauxic growth? Known regulatory mechanisms to be included: enzyme induction, inducer exclusion, and global regulation of gene expression by metabolic signal (cAMP). Quantitative data and qualitative observations to account for: Curves of glucose-lactose diauxie in standard batch conditions with glucose and lactose; Curves of glucose-lactose diauxie in a batch experiment with a glucose pulse and a batch experiment with a high starting concentration of lactose enzymes; No growth in the absence of glucose and lactose.","title":"Biological question to be answered"},{"location":"project4/project4a/#model-structure","text":"Environment described by concentrations of biomass ( B , gDW/L), preferred substrate ( G , mmol/L) and secondary substrate ( L , mmol/L) in the bioreactor of constant volume. \\[\\frac{dG}{dt} = -v_G B,\\] \\[\\frac{dL}{dt} = -v_L B,\\] \\[\\frac{dB}{dt} = \\mu B,\\] The growth rate is denoted by \\(\\mu\\) (1/h), while \\(v_G\\) and \\(v_L\\) (mmol/(gDW L)) correspond to the uptake rates of glucose and lactose, respectively. In the project, we are going to build a model of the growing bacterial population in this environment.","title":"Model structure"},{"location":"project4/project4a/#available-data","text":"Csv files of the three experiments described above. The data originates from K. Bettenbrock et al. A quantitative approach to catabolite repression in Escherichia coli. J Biol Chem. 2006;281:2578\u201384. Additional comments: In the pulse experiment, the glucose pulse occurs at 5 h and LacZ activity is a proxy for the LacZ concentration; In the batch experiment with a high starting concentration of lactose enzymes, glucose is depleted at 3 h and LacZ activity is a proxy for the LacZ concentration.","title":"Available data"},{"location":"project4/project4b/","text":"Project 4B: A syntrophic bacterial consortium Project assignment Develop a coarse-grained model that Quantitatively reproduces batch growth on glucose of the glucose specialist, Qualitatively reproduces batch growth on acetate of the acetate specialist, Qualitatively reproduces the behavior of a consortium of glucose and acetate specialists growing on glucose in a chemostat. Biological question to be answered What are the conditions for (i) the co-existence of the two strains and (ii) the occurrence of synthrophy, when the consortium is grown in a chemostat on glucose? Quantitative data and qualitative observations to account for Growth curves of the glucose specialist in standard batch conditions on glucose, Overflow of acetate by the glucose specialist in standard batch conditions on glucose, Toxic effect of acetate on growth rate. Optional: dependence of acetate overflow on the glucose uptake rate Model structure Environment described by concentrations of biomass (B, gdW/L), glucose (G, g/L) and acetate (A, g/L) in a continuous culture of constant volume. \\[\\frac{dB_{C}}{dt} = \\mu_C B_C - DB_C \\] \\[\\frac{dB_{P}}{dt} = \\mu_P B_P - DB_P \\] \\[\\frac{dA}{dt} = (r^a_{overP} - r^a_{upP}) B_P + (r^a_{overC} - r^a_{upC}) B_C - DA\\] \\[\\frac{dG}{dt} = -r^g_{upP} B_P - r^g_{upC} B_C + D (G_{in} - G)\\] where \\(\\mu\\) is the growth rate (h\u207b\u00b9), D is the dilution rate (h\u207b\u00b9), \\(r_{over}\\) is the acetate overflow rate (g/gDW/h) and \\(r_{up}\\) (g/gDW/h) is the uptake rate. Available data The following data are available as csv files : Growth of the glucose specialist on glucose in batch, The effect of acetate on the growth rate of the glucose specialist. The units are the same as described in the Model structure . Time is in hours.","title":"Project 4B"},{"location":"project4/project4b/#project-4b-a-syntrophic-bacterial-consortium","text":"","title":"Project 4B: A syntrophic bacterial consortium"},{"location":"project4/project4b/#project-assignment","text":"Develop a coarse-grained model that Quantitatively reproduces batch growth on glucose of the glucose specialist, Qualitatively reproduces batch growth on acetate of the acetate specialist, Qualitatively reproduces the behavior of a consortium of glucose and acetate specialists growing on glucose in a chemostat.","title":"Project assignment"},{"location":"project4/project4b/#biological-question-to-be-answered","text":"What are the conditions for (i) the co-existence of the two strains and (ii) the occurrence of synthrophy, when the consortium is grown in a chemostat on glucose?","title":"Biological question to be answered"},{"location":"project4/project4b/#quantitative-data-and-qualitative-observations-to-account-for","text":"Growth curves of the glucose specialist in standard batch conditions on glucose, Overflow of acetate by the glucose specialist in standard batch conditions on glucose, Toxic effect of acetate on growth rate. Optional: dependence of acetate overflow on the glucose uptake rate","title":"Quantitative data and qualitative observations to account for"},{"location":"project4/project4b/#model-structure","text":"Environment described by concentrations of biomass (B, gdW/L), glucose (G, g/L) and acetate (A, g/L) in a continuous culture of constant volume. \\[\\frac{dB_{C}}{dt} = \\mu_C B_C - DB_C \\] \\[\\frac{dB_{P}}{dt} = \\mu_P B_P - DB_P \\] \\[\\frac{dA}{dt} = (r^a_{overP} - r^a_{upP}) B_P + (r^a_{overC} - r^a_{upC}) B_C - DA\\] \\[\\frac{dG}{dt} = -r^g_{upP} B_P - r^g_{upC} B_C + D (G_{in} - G)\\] where \\(\\mu\\) is the growth rate (h\u207b\u00b9), D is the dilution rate (h\u207b\u00b9), \\(r_{over}\\) is the acetate overflow rate (g/gDW/h) and \\(r_{up}\\) (g/gDW/h) is the uptake rate.","title":"Model structure"},{"location":"project4/project4b/#available-data","text":"The following data are available as csv files : Growth of the glucose specialist on glucose in batch, The effect of acetate on the growth rate of the glucose specialist. The units are the same as described in the Model structure . Time is in hours.","title":"Available data"},{"location":"project4/software/","text":"Software To do the modelling, you need to use Python. To solve the system of ODEs you can use scipy.integrate.solve_ivp . To fit the model with the data, you can use lmfit .","title":"Software"},{"location":"project4/software/#software","text":"To do the modelling, you need to use Python. To solve the system of ODEs you can use scipy.integrate.solve_ivp . To fit the model with the data, you can use lmfit .","title":"Software"},{"location":"project4/timeline/","text":"Project timeline: Monday afternoon: project introduction By Tuesday evening: model definition (conceptual and equations) By Wednesday evening: model calibration, Python implementation, and first tests By Thursday noon: final model tests By Thursday afternoon: preparation of presentation Friday: project presentation","title":"Project timeline"},{"location":"project4/timeline/#project-timeline","text":"Monday afternoon: project introduction By Tuesday evening: model definition (conceptual and equations) By Wednesday evening: model calibration, Python implementation, and first tests By Thursday noon: final model tests By Thursday afternoon: preparation of presentation Friday: project presentation","title":"Project timeline:"}]}