{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing Ilastik Segmentation\n",
    "Ilastik outputs a probability mask that assigns a probability for each pixel that it either belongs to foreground (cell) or background.\n",
    "We now need to convert this to a proper segmentation, where each cell is assigned a unique number (i.e. a label).\n",
    "\n",
    "> **Exercise**  \n",
    ">  \n",
    "> Think about the steps needed to convert a probability map to a segmentation.  \n",
    "> You can discuss with your colleagues and tutors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Import packages\n",
    "\n",
    "Before starting the code we need to import all the required packages.\n",
    "\n",
    "We use a number of important Python packages:\n",
    "- [Numpy](https://numpy.org): Goto package for vector/matrix based calculations (heavily inspired by Matlab)\n",
    "- [Pandas](https://pandas.pydata.org): Goto package for handling data tables (heavily inspired by R) \n",
    "- [Scipy](https://scipy.org): Numpy extensions for statistics, image analysis, and more\n",
    "- [Scikit-image (skimage)](https://scikit-image.org): Goto package for image analysis\n",
    "- [Matplotlib](https://matplotlib.org): Goto package for plotting data\n",
    "- [Seaborn](https://seaborn.pydata.org): Fancy plots made easy (Similar to ggplot in R)\n",
    "- [Napari](https://napari.org): GUI based interactive image viewer\n",
    "- [Dask-Image](https://image.dask.org/en/latest/): Out-of-memory computation made easy\n",
    "- [pathlib](https://docs.python.org/3/library/pathlib.html): Path handling made easy\n",
    "- [h5py](https://www.h5py.org): Read HDF5 file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#next two lines make sure that Matplotlib plots are shown properly in Jupyter Notebook\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "#next line is required for Napari\n",
    "%gui qt\n",
    "\n",
    "#main data analysis packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#image processing packages\n",
    "from scipy import ndimage as ndi\n",
    "import skimage.segmentation as segmentation \n",
    "import skimage.filters as filters\n",
    "from skimage.measure import label, regionprops, regionprops_table\n",
    "from skimage import morphology\n",
    "\n",
    "#data plotting packages\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#set default figure size\n",
    "matplotlib.rc(\"figure\", figsize=(10,5))\n",
    "import seaborn as sns\n",
    "\n",
    "#image viewer\n",
    "import napari\n",
    "from napari.utils.notebook_display import nbscreenshot\n",
    "\n",
    "#out of memory computation\n",
    "from dask_image.imread import imread\n",
    "import dask.array as da\n",
    "\n",
    "#path handling\n",
    "import pathlib\n",
    "\n",
    "#file handling\n",
    "import h5py\n",
    "\n",
    "#Instead of dask_image.imread.imread() you can also use tifffile.imread() to directly read images into memory\n",
    "#import tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we initiate a cashe for Dask to speed up repeated computation (important for working with Napari)\n",
    "from dask.cache import Cache\n",
    "cache = Cache(2e9)  # Leverage two gigabytes of memory\n",
    "cache.register()    # Turn cache on globally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Import and visualize Ilastik output\n",
    "### Load images\n",
    "First we need to specify the folder and file names and some image properties.  \n",
    "If you followed our filename convention you don't have to make any changes here, but if you get an error later on check your path and filenames and adept them in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the path to the folder that contains project data\n",
    "#pathlib.Path.home() return the location of your home folder in a platform independent way. \n",
    "root = pathlib.Path(pathlib.Path.home(), \n",
    "                    'I2ICourse/Project2A/ProcessedData/')\n",
    "\n",
    "image_name = 'pos0_preproc-rg.tif' #set name of image\n",
    "segment_name = 'pos0_preproc-rg_Probabilities.h5' #set name of segmented data\n",
    "n_channel = 2 #set number of color channels in image\n",
    "fg_idx = 0 #set index of foreground label used in Ilastik\n",
    "\n",
    "im_path = root / image_name \n",
    "print('path to image: ', im_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time-lapse images can be big, and can easily overload your memory. To make sure this does not happen, you can use \"out of memory\" computation, where only the data that is directly needed is loaded into memory (e.g. the current time point). This is slower then loading the full dataset into memory, but it scales well when data gets too big to do that. Out-of-memory computation is quite easy with the use of [Dask image](https://image.dask.org/en/latest/). We will use this below.\n",
    "\n",
    "Don't worry about the technical details, though if you are curious we provide some optional 'Technical Notes' below.\n",
    "\n",
    "We now load the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use tifffile.imread to read complete image stack into memory.\n",
    "#im_stack = tifffile.imread(im_path) #load image into memory\n",
    "\n",
    "# or dask_image imread to use out of memory processing\n",
    "im_stack = imread(im_path) #load image with dask-image for out of memory processing \n",
    "\n",
    "# dask_image imread creates a 3D stack, where both color channels are interweaved\n",
    "# to separate them we need to reshape to 4D stack\n",
    "if n_channel>1: \n",
    "    newshape = (int(im_stack.shape[0]/n_channel), n_channel, *im_stack.shape[1:])\n",
    "    im_stack = im_stack.reshape(newshape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the data we will use [Napari](https://napari.org), that allows for interactive image visualization. \n",
    "You can use the slider at the bottom of the window to scroll trough time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Technical note: Napari and Dask work very well together, every time you move the time slider, Dask loads the corresponding image into memory.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup napari viewer, with 2 channel image\n",
    "viewer = napari.view_image(im_stack,\n",
    "            channel_axis=1,\n",
    "            name=[\"red\", \"green\"],\n",
    "            colormap=[\"yellow\", \"blue\"],)\n",
    "napari.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and visualize Ilastik output\n",
    "Next we load the data exported by Ilastik.\n",
    "\n",
    "`h5py.File=()` loads a dictionary, which contains a single entry called `exported_data`.   \n",
    "This contains the Ilastik probability output as an array of shape (t,label,y,x).  \n",
    "We only need the probability that a pixel belongs to the foreground label (= cells), so we extract the corresponding layer.\n",
    "\n",
    "Then we add the probability map to the Napari Viewer.\n",
    "You can change the opacity and layer order to improve visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_path = root / segment_name #path to Ilastik output\n",
    "seg_data = h5py.File(seg_path, 'r') #open \n",
    "print('keys = ', list(seg_data.keys()))\n",
    "\n",
    "#we again use Dask to load the data out of memory\n",
    "#technical comment: we set chunk size to be equal to the size of single frame. \n",
    "chuck_size = (1, *im_stack.shape[-2:]) if n_channel==1 else (1, n_channel, *im_stack.shape[-2:])\n",
    "seg_cell = da.from_array(seg_data['exported_data'], chunks=chuck_size)\n",
    "\n",
    "#we get a segmentation probability for both labels (cells and BG) but only need the one for the cells, so we extract the correct dimension:\n",
    "seg_cell = seg_cell[:,fg_idx,:,:]\n",
    "\n",
    "#add probability layer to Napari Viewer\n",
    "prop_layer = viewer.add_image(seg_cell, name='probability',colormap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise** \n",
    ">   \n",
    "> Check the segmentation quality. How well did it work? Are there frames with big problems, if so discuss with you Tutor.  \n",
    "> Also compare your results to those of your colleagues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Convert Ilastik probability map into segmentation\n",
    "Ilastik assigns to each pixel the probability that it belongs to a cell. We need to convert this to in instance based segmentation, where each cell is assigned a unique label. This requires a number of steps:\n",
    "1. Pre-process probability map using filters\n",
    "2. Convert probability map to semantic segmentation (binary image where cells=1 and background=0) using thresholding\n",
    "3. Clean up semantic segmentation using morphological operations\n",
    "4. Convert semantic segmentation into instance segmentation (integer images where each cells has own label)\n",
    "5. Post-process instance segmentation by separating merged cells using watershed algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Pre-process probability map using filters\n",
    "\n",
    "As a first step probability maps are often processed using a Gaussian blur filter using a small (~1 pixel) size, to ensure that the probability maps are locally smooth. We will use scikit image [`filters.gaussion`](https://scikit-image.org/docs/stable/api/skimage.filters.html) to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Technical note: scikit images does not automatically play nice with Dask's out of memory processing, instead we need to manually specify that we want to run `filters.gaussion` on each chunk (i.e. each frame) of our `seg_cell` image stack. This we can do using the Dask Image [`map_blocks()` function](https://docs.dask.org/en/latest/generated/dask.array.map_blocks.html))*.\n",
    "\n",
    "*The usage is pretty straightforward, if normally you would use: `name_of_function_to_apply(normal_array, other_function_arguments)` you can now use: `da.map_blocks(name_of_function_to_apply, dask_array, other_function_arguments)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1 #size of Gaussion kernel to use \n",
    "\n",
    "#this is how you normally use scikit filters.gaussian\n",
    "#seg_cell_sm = filters.gaussian(seg_cell, sigma, channel_axis=0) \n",
    "\n",
    "#this is how we do it using Dask:\n",
    "seg_cell_sm = da.map_blocks(filters.gaussian, seg_cell, sigma, channel_axis=0)\n",
    "\n",
    "#add to viewer\n",
    "prop_layer_sm = viewer.add_image(seg_cell_sm, name='probability smoothed',colormap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### 2. Convert probability map to semantic segmentation using thresholding\n",
    "\n",
    "[Thresholding](https://en.wikipedia.org/wiki/Thresholding_%28image_processing%29) is used to convert continues probability maps into binary label images. It is a first step in segmenting objects. A threshold value determines the intensity value separating foreground pixels from background pixels. Foregound pixels are pixels brighter than the threshold value, background pixels are darker.\n",
    "\n",
    "There are automatic methods to calculate a threshold, e.g. [Otsu's method](https://en.wikipedia.org/wiki/Otsu%27s_method) and [Li's minimum cross entropy threshold](https://scikit-image.org/docs/dev/auto_examples/developers/plot_threshold_li.html) are two common algorithms. \n",
    "\n",
    "However, here we will pick the threshold manually. \n",
    "\n",
    "Note: to prevent subjectivity to affect your analysis it is important to keep a manually chosen threshold (or the chosen automatic method) constant between replicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create array with all threshold values to rry, here we use 0,0.01,0.02,...,1\n",
    "thresholds_to_try = np.linspace(0,1,101)\n",
    "\n",
    "#apply treshold, loop over all threshold values and store output in list\n",
    "all_thresholds = [seg_cell_sm>t for t in thresholds_to_try]\n",
    "\n",
    "#convert the list of 3D stacks to a single 4D stack\n",
    "threshold_stack = da.stack(all_thresholds, axis=0)\n",
    "\n",
    "#add to viewer\n",
    "mask_layer_int = viewer.add_image(threshold_stack, name='binary mask int',colormap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Napari viewer you will now have a new layer `binary mask int`, which shows you the result of the interactive thresholding, you can change the threshold value by moving the new slider (channel 0).\n",
    "\n",
    "> **Exercise** \n",
    ">  \n",
    "> Move the slider to find a good threshold value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Technical aside: if at any time you want to store the current view shown in the Napari viewer, then you can use `nbscreenshot(viewer)`.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbscreenshot(viewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise** \n",
    "> \n",
    "> Now it is time to choose a final threshold value. Enter the value you have chosen below.  \n",
    "> Hint: the slider position shows the index in the `thresholds_to_try` array we defined above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose your favorite method or enter a manually chosen value\n",
    "final_threshold = 0.66\n",
    "\n",
    "#threshold and add to viewer\n",
    "bin_mask = seg_cell_sm > final_threshold\n",
    "mask_layer_final = viewer.add_image(bin_mask, name='final binary mask',colormap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now remove the interactive threshold layer from the viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.layers.remove(\"binary mask int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### 3. Clean up semantic segmentation using morphological operations\n",
    "\n",
    "[Mathematical morphology](https://en.wikipedia.org/wiki/Mathematical_morphology) operations and structuring elements are defined in `skimage.morphology`.\n",
    "\n",
    "Functions operating on [connected components](https://en.wikipedia.org/wiki/Connected_space) can remove small undesired elements while preserving larger shapes.\n",
    "\n",
    "`skimage.morphology.remove_small_holes` fills holes and `skimage.morphology.remove_small_objects` removes bright regions. Both functions accept a size parameter, which is the minimum size (in pixels) of accepted holes or objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_hole_size = 40 # maximum area of holes that will be filled (in pixels)\n",
    "min_cell_size = 150 # minimum area of objects to keep (in pixels)\n",
    "\n",
    "holes_removed = da.map_blocks(morphology.remove_small_holes, bin_mask, max_hole_size)\n",
    "bin_mask_clean = da.map_blocks(morphology.remove_small_objects, holes_removed, min_cell_size)\n",
    "\n",
    "mask_layer_clean = viewer.add_image(bin_mask_clean, name='binary mask cleaned',colormap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**\n",
    ">\n",
    "> Try finding good values for `max_hole_size` and `min_cell_size`, by changing the parameters above.  \n",
    "> Spend max 5 min on this step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Technical note `skimage.morphology` only works on 2D/3D image data, not on time stacks, we thus have to apply it to each frame separately, luckily `da.map_blocks()` can take care of that for us)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Convert semantic segmentation into instance segmentation\n",
    "\n",
    "Now we are ready to label the connected components of this image. This means that each object will be assigned a unique number. For this we can use the [`skimage.measure.labels()`](https://scikit-image.org/docs/dev/api/skimage.measure.html) function.\n",
    "To add labels to Napari, we can use the `add_label()` function.\n",
    "\n",
    "You can change the colors assigned to cells, using the shuffle button in upper left corner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again we will need to loop over the time using map_blocks\n",
    "label_im = da.map_blocks(label, bin_mask_clean)\n",
    "label_layer = viewer.add_labels(label_im, opacity=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**\n",
    "> \n",
    "> Inspect the segmentation. Do you find any problems? Think about what we need to do next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Post-process instance segmentation by separating merged cells using watershed algorithm\n",
    "\n",
    "We can see that tightly packed cells are sometimes assigned the same label.\n",
    "\n",
    "A better segmentation would assign different labels to different cells. \n",
    "\n",
    "Typically we use [watershed segmentation](https://en.wikipedia.org/wiki/Watershed_%28image_processing%29) for this purpose. This requires two steps: 1) using a so-called [distance transform](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.morphology.distance_transform_edt.html) we  calculate the distance of each pixel to the object boundary 2) we need to find center points (called *markers*) for each object. We place *markers* at the centre of each object, and these labels are expanded until they meet an edge or an adjacent marker.\n",
    "\n",
    "The trick, then, is how to find these markers. For spherical cells, there is a commonly used strategy that works quite well: we can simply find the local maxima of the distance transform (we provide the code for this below for reference).\n",
    "\n",
    "For rod shape cells, this works quite badly, though. Another way to find cell centers is by using a very high threshold for the probability map, as probabilities are typically  highest in the cell centre. Then we can use these regions as markers for the watershed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set threshold to use to find cell centers\n",
    "center_threshold = 0.95\n",
    "\n",
    "center_seg = seg_cell_sm > center_threshold\n",
    "center_markers = da.map_blocks(label, center_seg)\n",
    "    \n",
    "centre_layer = viewer.add_labels(center_markers, name='cell centre areas')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**\n",
    "> \n",
    "> Find a good value to use for `center_threshold` such that nearby cells have different center markers (without chopping cells into parts).  \n",
    "> Spend max 5 min on this step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the center points we can further segment the image by using the watershed: we calculate the distance using [scipy.ndimage.morphology.distance_transform_edt](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.morphology.distance_transform_edt.html) and then apply the watershed using [`skimage.segmentation.watershed`](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.watershed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need a wrapper function to tranform named arguments into positional arguments to make things work with Dask\n",
    "def watershed(dist, markers, mask):\n",
    "    return segmentation.watershed(-dist, markers=markers, mask=mask)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate distance to edge of mask\n",
    "dist_transform = da.map_blocks(ndi.distance_transform_edt, bin_mask_clean)   \n",
    "\n",
    "#run watershed\n",
    "labels_final = da.map_blocks(watershed, -dist_transform, center_markers, bin_mask_clean, chunks=(1,*bin_mask_clean.shape[-2:]))\n",
    "\n",
    "#add to Napari\n",
    "watershed_layer = viewer.add_labels(labels_final, name='after watershed')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Concluding comments on segmentation\n",
    "\n",
    "The watershed should have cut clusters of cells apart, improving the segmentation by separating nearby cells.\n",
    "\n",
    "However, you might also have seen some cells that were cut but should not have, there is often a bit of a trade-off here.\n",
    "The best solution is to improve the quality of the probability map exported by Ilastik.   \n",
    "\n",
    "There are several things you could do here to improve things:\n",
    "\n",
    "- Include fluorescent marker if possible: segmenting on fluorescent channel is almost always better than on phase contrast\n",
    "- Optimize image acquisition: the better quality the input data has, the easier and better the segmentation will be\n",
    "  - Optimize signal-to-noise ratio\n",
    "  - Optimize focus \n",
    "  - Throw out data with bad focus / exposure \n",
    "- Optimize Ilastik classification\n",
    "  - Add extra training points in problem areas\n",
    "  - Make sure your training data is diverse: include images from all your replicates\n",
    "  - Make sure you include problem frames (mediocre focus, dense cell clusters) in your training data\n",
    "  - Try segmenting on different color channels or combinations of them (e.g. phase + fluorescence)\n",
    "    - Important note: never segment on a channel you want to quantify later, i.e. do not segment on reporter gene signal, this will create a bias in  your data.\n",
    "  - Try doing deconvolution before segmentation, this will reduce the blur caused by diffraction making dense cell clusters easier to separate \n",
    "\n",
    "The fastest and easiest is often to simply go back to Ilastik and add some more training points. For time reasons we will skip this now, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data storage and retrieval \n",
    "### Storing segmentation data\n",
    "\n",
    "Segmentation can be a time-consuming process so you might want to store the output on your HD. \n",
    "Here we store it as HDF5 for fast access from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can first convert to 16bit to save some space\n",
    "labels_final.astype('int16')\n",
    "\n",
    "#store as hdf5\n",
    "seg_name_hdf5 = root /  image_name.replace('.tif','_label_im.hdf5')\n",
    "labels_final.to_hdf5(seg_name_hdf5, '/labels_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading segmentation data\n",
    "To load the segmentation again we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = h5py.File(seg_name_hdf5, 'r') #open \n",
    "labels_final_loaded = da.from_array(label_data['labels_final'], chunks=(1,-1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Next step: Extracting cell properties\n",
    "\n",
    "We will continue in the next notebook: `1_extracting_cell_properties`, to extract the properties of cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extra Material: Code to find markers for spherical cells with watershed algorithm\n",
    "\n",
    "**Do not run this section now** we provide this code for reference only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import peak_local_max\n",
    "\n",
    "#calculate distance to edge of mask\n",
    "dist_transform = da.map_blocks(ndi.distance_transform_edt, bin_mask_clean)   \n",
    "\n",
    "#add to Napari \n",
    "dist_layer = viewer.add_image(dist_transform, name='distance transform',colormap='viridis')  \n",
    "\n",
    "#min distance between maxima, adjust this value to slightly less than 2x the radius of the smallest cells.\n",
    "min_distance = 10\n",
    "\n",
    "\n",
    "#initialize empty array of right size\n",
    "markers_list = np.empty((3,0))\n",
    "\n",
    "#find maxima in distance transform\n",
    "for t, dist_map in enumerate(dist_transform):\n",
    "    #dist_map = filters.gaussian(dist_map, 2) \n",
    "    peak_idx = peak_local_max(dist_map.compute(), min_distance=min_distance)\n",
    "    peak_idx_3d = np.insert(peak_idx.T,0,t,axis=0)\n",
    "    markers_list = np.concatenate((markers_list,peak_idx_3d),axis=1)\n",
    "    \n",
    "#add to Napari Viewer\n",
    "point_layer = viewer.add_points(\n",
    "    np.transpose(markers_list),\n",
    "    name='center points',\n",
    "    size=4,\n",
    "    n_dimensional=False)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
